---
title: "Script 4 - FinalModels"
author: 
  - name: "MSc. Friederike Johanna Rosa WÃ¶lke"
    orcid: "0000-0001-9034-4883"
    url: "https://friedarosa.github.io"
    email: "wolke@fzp.czu.cz"
    corresponding: true
date: "2023-05-29"
---

::: panel-tabset
## Source custom functions

```{r}
#| label: load-functions
#| message: FALSE
#| warning: FALSE
rm(list = ls())
source("src/functions.R")

```

## MachineLearning packages

```{r}
#| label: load-ML-packages
#| message: FALSE
#| error: FALSE

pckgs <- c("dplyr", "ggplot2", "reshape2", 
           "ggcorrplot", 
           "caret",  "recipes",   "caretEnsemble", 
           "randomForest", "ranger", "gbm", "xgboost", 
           "vegan", "pdp", 
           "gridExtra", "kableExtra")

install_and_load(pckgs)
```

## Load RData to reduce computing time

```{r}
#| label: load-RData
#| message: FALSE
#| error: FALSE

# Load final workspace to save computing time:
load("data/RData/04_FinalModels.RData")
```
:::

### Train all models together

# Jaccard 1

::: panel-tabset
## Train model

```{r}
#| label: set-vars-J1
#| warning: false

tictoc::tic("J1")
# Define model variables  (response, indices and predictors)
response <- "Jaccard" # Replace with your actual response variable name
predictors <- reduced_predictors[[1]] # Replace with your actual predictors
index <- indices_J1
dd <- dat_train_J1 %>% select(all_of(c(response, predictors)))
dd_test <- dat_test_J1 %>% select(all_of(c(response, predictors)))

# Define training control
trained_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final",
  returnResamp = "final",
  verboseIter = FALSE,
  index = index # Ensure indices_J1 is defined
)

```

::: panel-tabset
## Train model

```{r}
#| label: train-modelsList-J1
#| warning: false
#| eval: false

tictoc::tic("J1")

# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd
set.seed(42)
modelsList_J1 <- caretList(
  as.formula(paste(response, "~ .")),
  data = dd,
  trControl = trained_control,
  tuneList = list(
    ranger = caretModelSpec(
      method = "ranger",
      tuneGrid = expand.grid(
        mtry = 12,
        splitrule = "variance",
        min.node.size = 5
      ),
      importance = "permutation",
      num.trees = 5000
    ),
    gbm = caretModelSpec(
      method = "gbm",
      tuneGrid = expand.grid(
        n.trees = 300,
        interaction.depth = 10,
        shrinkage = 0.1,
        n.minobsinnode = 10
      ),
      verbose = FALSE
    ),
    xgbTree = caretModelSpec(
      method = "xgbTree",
      tuneGrid = expand.grid(
        nrounds = 1000,
        eta = 0.1,
        max_depth = 5,
        gamma = 0,
        colsample_bytree = 0.6,
        min_child_weight = 1,
        subsample = 1
      )
    )
  )
)
```

## Summarize individual models

```{r}
## Summarzize across individual models ========================================#

# Create resamples from the list of models
resamps_J1 <- resamples(modelsList_J1)

# Plot the resampled error rates for each model
dotplot_resamps_J1 <- dotplot(resamps_J1)

# Summarize the resamples
summary_resamps_J1 <- summary(resamps_J1)

# Combine everything into a list
resamples_all_J1 <- list(
  Jaccard1 = list(
    resamps_J1 = resamps_J1,
    Dotplot = dotplot_resamps_J1, # store the dotplot object
    Summary = summary_resamps_J1  # store the summary object
  )
)

resamples_all_J1
```

## Predictive performances

```{r}
## Predictive Performance analysis ================================================
p_J1 <- as.data.frame(
  predict(modelsList_J1, newdata = dd_test)) %>% 
  cbind(dd_test$Jaccard) %>%
  mutate(
    error_ranger = dd_test$Jaccard-ranger,
    error_gbm = dd_test$Jaccard-gbm,
    error_xgb = dd_test$Jaccard-xgbTree) 

p_J1 %>%
  summarise(mean_ranger = mean(error_ranger),
            mean_gbm = mean(error_gbm),
            mean_xgb = mean(error_xgb)) %>% 
  kableExtra::kable() # ranger performs best
```

## Ensemble model

The ensemble model is not better than the ranger model alone. In fact, it's a bit worse. We will discard the ensembleModel approach therefore.

```{r}
# Create the ensemble model ================================================

## Are they correlated? 
# yes. not the best foundation for ensembleModels...
modelCor(resamples(modelsList_J1))

ensembleModel_J1 <- caretEnsemble(
    modelsList_J1,
    metric = "Rsquared",
    trControl = trained_control)
summary(ensembleModel_J1)

```

## Summarize predictor importances

```{r}
# Summarize predictor importances ============================================

varImp(ensembleModel_J1) %>% arrange(desc(overall))
# ranger
imp_ranger <- varImp(modelsList_J1[[1]])$importance %>% 
  as.data.frame() %>% 
  rename("imp_ranger" = "Overall")
imp_ranger$var <- row.names(imp_ranger)

# gbm
imp_gbm <- varImp(modelsList_J1[[2]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_gbm" = "Overall")
imp_gbm$var <- row.names(imp_gbm)
imp_merged <- merge(imp_ranger, imp_gbm)

# xgb
imp_xgb <- varImp(modelsList_J1[[3]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_xgb" = "Overall")
imp_xgb$var <- row.names(imp_xgb)

imp_merged_all_J1 <- merge(imp_merged, imp_xgb) %>% 
  arrange(desc(imp_ranger))

# Print results
imp_merged_all_J1 %>% 
  kableExtra::kable()
varImp(ensembleModel_J1) %>% 
  arrange(desc(overall)) %>% 
  kableExtra::kable()




rm(response, predictors, index, dd, dd_test, trained_control)

tictoc::toc()
```
:::

# Jaccard 2

::: panel-tabset
## Train model

```{r}
#| label: set-vars-J2
#| warning: false
tictoc::tic("J2")

# Define model variables  (response, indices and predictors)
response <- "Jaccard" # Replace with your actual response variable name
predictors <- reduced_predictors[[2]] # Replace with your actual predictors
index <- indices_J2
dd <- dat_train_J2 %>% select(all_of(c(response, predictors)))
dd_test <- dat_test_J2 %>% select(all_of(c(response, predictors)))

# Define training control
trained_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final",
  returnResamp = "final",
  verboseIter = FALSE,
  index = index # Ensure indices_J2 is defined
)

```


::: panel-tabset
## Train model

```{r}
#| label: train-modelsList-J2
#| warning: false
#| eval: false
tictoc::tic("J2")

# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd
set.seed(42)
modelsList_J2 <- caretList(
  as.formula(paste(response, "~ .")),
  data = dd,
  trControl = trained_control,
  tuneList = list(
    ranger = caretModelSpec(
      method = "ranger",
      tuneGrid = expand.grid(
        mtry = 5,
        splitrule = "variance",
        min.node.size = 5
      ),
      importance = "permutation",
      num.trees = 5000
    ),
    gbm = caretModelSpec(
      method = "gbm",
      tuneGrid = expand.grid(
        n.trees = 50,
        interaction.depth = 9,
        shrinkage = 0.1,
        n.minobsinnode = 10
      ),
      verbose = FALSE
    ),
    xgbTree = caretModelSpec(
      method = "xgbTree",
      tuneGrid = expand.grid(
        nrounds = 1000,
        eta = 0.1,
        max_depth = 5,
        gamma = 0.01,
        colsample_bytree = 0.6,
        min_child_weight = 1,
        subsample = 1
      )
    )
  )
)
```

## Summarize individual models

```{r}
## Summarzize across individual models ========================================#

# Create resamples from the list of models
resamps_J2 <- resamples(modelsList_J2)

# Plot the resampled error rates for each model
dotplot_resamps_J2 <- dotplot(resamps_J2)

# Summarize the resamples
summary_resamps_J2 <- summary(resamps_J2)

# Combine everything into a list
resamples_all_J2 <- list(
  Jaccard1 = list(
    resamps_J2 = resamps_J2,
    Dotplot = dotplot_resamps_J2, # store the dotplot object
    Summary = summary_resamps_J2  # store the summary object
  )
)

resamples_all_J2
```

## Predictive performances

```{r}
## Predictive Performance analysis ================================================
p_J2 <- as.data.frame(
  predict(modelsList_J2, newdata = dd_test)) %>% 
  cbind(dd_test$Jaccard) %>%
  mutate(
    error_ranger = dd_test$Jaccard-ranger,
    error_gbm = dd_test$Jaccard-gbm,
    error_xgb = dd_test$Jaccard-xgbTree) 

p_J2 %>%
  summarise(mean_ranger = mean(error_ranger),
            mean_gbm = mean(error_gbm),
            mean_xgb = mean(error_xgb)) %>% 
  kableExtra::kable() # ranger performs best
```

## Ensemble model

The ensemble model is not better than the ranger model alone. In fact, it's a bit worse. We will discard the ensembleModel approach therefore.

```{r}
# Create the ensemble model ================================================

## Are they correlated? 
# yes. not the best foundation for ensembleModels...
modelCor(resamples(modelsList_J2))

ensembleModel_J2 <- caretEnsemble(
    modelsList_J2,
    metric = "Rsquared",
    trControl = trained_control)
summary(ensembleModel_J2)

```

## Summarize predictor importances

```{r}
# Summarize predictor importances ============================================

varImp(ensembleModel_J2) %>% arrange(desc(overall))
# ranger
imp_ranger <- varImp(modelsList_J2[[1]])$importance %>% 
  as.data.frame() %>% 
  rename("imp_ranger" = "Overall")
imp_ranger$var <- row.names(imp_ranger)

# gbm
imp_gbm <- varImp(modelsList_J2[[2]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_gbm" = "Overall")
imp_gbm$var <- row.names(imp_gbm)
imp_merged <- merge(imp_ranger, imp_gbm)

# xgb
imp_xgb <- varImp(modelsList_J2[[3]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_xgb" = "Overall")
imp_xgb$var <- row.names(imp_xgb)

imp_merged_all_J2 <- merge(imp_merged, imp_xgb) %>% 
  arrange(desc(imp_ranger))

# Print results
imp_merged_all_J2 %>% 
  kableExtra::kable()
varImp(ensembleModel_J2) %>% 
  arrange(desc(overall)) %>% 
  kableExtra::kable()

rm(response, predictors, index, dd, dd_test, trained_control)

tictoc::toc()
```
:::



# Log Ratio 1

::: panel-tabset
## Train model

```{r}
#| label: set-vars-LR1
#| warning: false

tictoc::tic("LR1")
# Define model variables  (response, indices and predictors)
response <- "log_R2_1" 
predictors <- reduced_predictors[[3]] # Replace with your actual predictors
index <- indices_LR1
dd <- dat_train_LR1 %>% select(all_of(c(response, predictors)))
dd_test <- dat_test_LR1 %>% select(all_of(c(response, predictors)))

# Define training control
trained_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final",
  returnResamp = "final",
  verboseIter = FALSE,
  index = index # Ensure indices_LR1 is defined
)
```


::: panel-tabset
## Train model

```{r}
#| label: train-modelsList-LR1
#| warning: false
#| eval: false

tictoc::tic("LR1")


# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd
set.seed(42)
modelsList_LR1 <- caretList(
  as.formula(paste(response, "~ .")),
  data = dd,
  trControl = trained_control,
  tuneList = list(
    ranger = caretModelSpec(
      method = "ranger",
      tuneGrid = expand.grid(
        mtry = 28,
        splitrule = "extratrees",
        min.node.size = 5
      ),
      importance = "permutation",
      num.trees = 5000
    ),
    gbm = caretModelSpec(
      method = "gbm",
      tuneGrid = expand.grid(
        n.trees = 50,
        interaction.depth = 3,
        shrinkage = 0.1,
        n.minobsinnode = 10
      ),
      verbose = FALSE
    ),
    xgbTree = caretModelSpec(
      method = "xgbTree",
      tuneGrid = expand.grid(
        nrounds = 1000,
        eta = 0.1,
        max_depth = 5,
        gamma = 0.1,
        colsample_bytree = 0.6,
        min_child_weight = 1,
        subsample = 1
      )
    )
  )
)
```

## Summarize individual models

```{r}
## Summarzize across individual models ========================================#

# Create resamples from the list of models
resamps_LR1 <- resamples(modelsList_LR1)

# Plot the resampled error rates for each model
dotplot_resamps_LR1 <- dotplot(resamps_LR1)

# Summarize the resamples
summary_resamps_LR1 <- summary(resamps_LR1)

# Combine everything into a list
resamples_all_LR1 <- list(
  Jaccard1 = list(
    resamps_LR1 = resamps_LR1,
    Dotplot = dotplot_resamps_LR1, # store the dotplot object
    Summary = summary_resamps_LR1  # store the summary object
  )
)

resamples_all_LR1
```

## Predictive performances

```{r}
## Predictive Performance analysis ================================================
p_LR1 <- as.data.frame(
  predict(modelsList_LR1, newdata = dd_test)) %>% 
  cbind(dd_test$log_R2_1) %>%
  mutate(
    error_ranger = dd_test$log_R2_1-ranger,
    error_gbm = dd_test$log_R2_1-gbm,
    error_xgb = dd_test$log_R2_1-xgbTree) 

p_LR1 %>%
  summarise(mean_ranger = mean(error_ranger),
            mean_gbm = mean(error_gbm),
            mean_xgb = mean(error_xgb)) %>% 
  kableExtra::kable() # ranger performs best
```

## Ensemble model

The ensemble model is not better than the ranger model alone. In fact, it's a bit worse. We will discard the ensembleModel approach therefore.

```{r}
# Create the ensemble model ================================================

## Are they correlated? 
# yes. not the best foundation for ensembleModels...
modelCor(resamples(modelsList_LR1))

ensembleModel_LR1 <- caretEnsemble(
    modelsList_LR1,
    metric = "Rsquared",
    trControl = trained_control)
summary(ensembleModel_LR1)

```

## Summarize predictor importances

```{r}
# Summarize predictor importances ============================================

varImp(ensembleModel_LR1) %>% arrange(desc(overall))
# ranger
imp_ranger <- varImp(modelsList_LR1[[1]])$importance %>% 
  as.data.frame() %>% 
  rename("imp_ranger" = "Overall")
imp_ranger$var <- row.names(imp_ranger)

# gbm
imp_gbm <- varImp(modelsList_LR1[[2]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_gbm" = "Overall")
imp_gbm$var <- row.names(imp_gbm)
imp_merged <- merge(imp_ranger, imp_gbm)

# xgb
imp_xgb <- varImp(modelsList_LR1[[3]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_xgb" = "Overall")
imp_xgb$var <- row.names(imp_xgb)

imp_merged_all_LR1 <- merge(imp_merged, imp_xgb) %>% 
  arrange(desc(imp_ranger))

# Print results
imp_merged_all_LR1 %>% 
  kableExtra::kable()
varImp(ensembleModel_LR1) %>% 
  arrange(desc(overall)) %>% 
  kableExtra::kable()
rm(response, predictors, index, dd, dd_test, trained_control)

tictoc::toc()
```
:::



# Log Ratio 2

::: panel-tabset
## Train model

```{r}
#| label: set-vars-LR2
#| warning: false
tictoc::tic("LR2")

# Define model variables  (response, indices and predictors)
response <- "log_R2_1" 
predictors <- reduced_predictors[[4]] # Replace with your actual predictors
index <- indices_LR2
dd <- dat_train_LR2 %>% select(all_of(c(response, predictors)))
dd_test <- dat_test_LR2 %>% select(all_of(c(response, predictors)))

# Define training control
trained_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final",
  returnResamp = "final",
  verboseIter = FALSE,
  index = index # Ensure indices_LR2 is defined
)

```

::: panel-tabset
## Train model

```{r}
#| label: train-modelsList-LR2
#| warning: false
#| eval: false

tictoc::tic("LR2")
# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd
set.seed(42)
modelsList_LR2 <- caretList(
  as.formula(paste(response, "~ .")),
  data = dd,
  trControl = trained_control,
  tuneList = list(
    ranger = caretModelSpec(
      method = "ranger",
      tuneGrid = expand.grid(
        mtry = 52,
        splitrule = "extratrees",
        min.node.size = 5
      ),
      importance = "permutation",
      num.trees = 5000
    ),
    gbm = caretModelSpec(
      method = "gbm",
      tuneGrid = expand.grid(
        n.trees = 50,
        interaction.depth = 7,
        shrinkage = 0.1,
        n.minobsinnode = 10
      ),
      verbose = FALSE
    ),
    xgbTree = caretModelSpec(
      method = "xgbTree",
      tuneGrid = expand.grid(
        nrounds = 1000,
        eta = 0.1,
        max_depth = 5,
        gamma = 0.1,
        colsample_bytree = 0.6,
        min_child_weight = 1,
        subsample = 1
      )
    )
  )
)
```

## Summarize individual models

```{r}
## Summarzize across individual models ========================================#

# Create resamples from the list of models
resamps_LR2 <- resamples(modelsList_LR2)

# Plot the resampled error rates for each model
dotplot_resamps_LR2 <- dotplot(resamps_LR2)

# Summarize the resamples
summary_resamps_LR2 <- summary(resamps_LR2)

# Combine everything into a list
resamples_all_LR2 <- list(
  Jaccard1 = list(
    resamps_LR2 = resamps_LR2,
    Dotplot = dotplot_resamps_LR2, # store the dotplot object
    Summary = summary_resamps_LR2  # store the summary object
  )
)

resamples_all_LR2
```

## Predictive performances

```{r}
## Predictive Performance analysis ================================================
p_LR2 <- as.data.frame(
  predict(modelsList_LR2, newdata = dd_test)) %>% 
  cbind(dd_test$log_R2_1) %>%
  mutate(
    error_ranger = dd_test$log_R2_1-ranger,
    error_gbm = dd_test$log_R2_1-gbm,
    error_xgb = dd_test$log_R2_1-xgbTree) 

p_LR2 %>%
  summarise(mean_ranger = mean(error_ranger),
            mean_gbm = mean(error_gbm),
            mean_xgb = mean(error_xgb)) %>% 
  kableExtra::kable() # ranger performs best
```

## Ensemble model

The ensemble model is not better than the ranger model alone. In fact, it's a bit worse. We will discard the ensembleModel approach therefore.

```{r}
# Create the ensemble model ================================================

## Are they correlated? 
# yes. not the best foundation for ensembleModels...
modelCor(resamples(modelsList_LR2))

ensembleModel_LR2 <- caretEnsemble(
    modelsList_LR2,
    metric = "Rsquared",
    trControl = trained_control)
summary(ensembleModel_LR2)

```

## Summarize predictor importances

```{r}
# Summarize predictor importances ============================================

varImp(ensembleModel_LR2) %>% arrange(desc(overall))
# ranger
imp_ranger <- varImp(modelsList_LR2[[1]])$importance %>% 
  as.data.frame() %>% 
  rename("imp_ranger" = "Overall")
imp_ranger$var <- row.names(imp_ranger)

# gbm
imp_gbm <- varImp(modelsList_LR2[[2]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_gbm" = "Overall")
imp_gbm$var <- row.names(imp_gbm)
imp_merged <- merge(imp_ranger, imp_gbm)

# xgb
imp_xgb <- varImp(modelsList_LR2[[3]])$importance %>% 
  as.data.frame()%>% 
  rename("imp_xgb" = "Overall")
imp_xgb$var <- row.names(imp_xgb)

imp_merged_all_LR2 <- merge(imp_merged, imp_xgb) %>% 
  arrange(desc(imp_ranger))

# Print results
imp_merged_all_LR2 %>% 
  kableExtra::kable()
varImp(ensembleModel_LR2) %>% 
  arrange(desc(overall)) %>% 
  kableExtra::kable()

rm(response, predictors, index, dd, dd_test, trained_control)

tictoc::toc()

```
:::

# Save the models
```{r}
# save.image("data/RData/04_FinalModels.RData")
```

