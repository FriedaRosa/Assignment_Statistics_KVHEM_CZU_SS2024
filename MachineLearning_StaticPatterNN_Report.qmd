---
title: "MachineLearning - StaticPatterNN"
author: 
  - name: "MSc. Friederike Johanna Rosa Wölke"
    orcid: "0000-0001-9034-4883"
    url: "https://friedarosa.github.io"
    email: "wolke@fzp.czu.cz"
    corresponding: true
date: "2023-05-29"
format: 
  html:
    toc: true
    code-fold: true
    code-overflow: wrap
    bibliography: references.bib
keep-tex: true
---

# Assignment for PhD Statistics class 2024

------------------------------------------------------------------------

## **Topic of Credit Report -- Statistical Methods** - Prague, 07.05.2024

[Title of report:]{.underline} *Towards predicting temporal biodiversity change from static patterns*

[Name:]{.underline} MSc. Friederike Wölke

[Supervisor:]{.underline} Dr. Petr Keil

[Thesis title:]{.underline} Universal imprints of temporal change in static spatial patterns of biodiversity

[Expected methods applied:]{.underline}

-   Machine learning [(random forest regression from ranger package; using the caret suite of tools for hyperparameter tuning)]{style="color:red;"}
-   visualization of results (ggplot2, partial plots, BAM chart).

[Abstract:]{.underline}

The world is undergoing significant environmental transformations, impacting biodiversity and ecosystem functions. Since obtaining temporal replication of biodiversity data is challenging due to cost and monitoring limitations, I aim at predicting temporal trends in species occupancy without requiring temporally replicated data.

Biodiversity kinetics leave characteristics imprint in the spatial patterns that we can see from geo-referenced presence/absence data because the underlying processes such as extinction and colonization happen across space.

I aim at predicting the log ratio of temporal change in occupancy (i.e., the sum of area occupied by a species) between two sampling periods from a set of predictor variables that are either related to **H1) species traits and ecology**, **H2)  geometric features of the species range from a single sampling period**, **H3) biodiversity equilibrium dynamics via spatial diversity patterns**,  or **H4) to the characteristics of the study region** -- all of which may equally contribute and act in concert to explaining the temporal process that is underlying the spatial pattern.

For this I use high-quality, spatially continuous atlas data from four breeding bird atlases from temperate zones across the globe. Atlas data comes with spatial grids that enable easy up- and down scaling of the data.

Here, I assess universal imprints of temporal change in breeding birds in Czech Republic, Japan, New York State and the whole of Europe across two aggregated sampling periods that took place pre-2000 and post-2000. Since data for two sampling periods are available, I will additionally test whether imprints in the spatial aggregation of species can better predict past or future biodiversity change.

For this, I collated 60 predictor variables that vary across sampling periods - each belonging to one of the hypotheses mentioned above. I will use random forest regression to determine the capability of static patterns to predict temporal change, identify the most important predictor variables and compare observed versus predicted results.

If my model can predict temporal change from static patterns, this method will be a useful tool for estimating temporal change in areas and for species where repeated monitoring might not be feasible. If the models are only partially able to predict temporal trends, the important predictors may still yield insights into how temporal processes are acting across space. Additionally disentangling whether imprints of biodiversity kinetics are better at explaining past versus future change may help to understand the temporal dimensions of the imprints.

------------------------------------------------------------------------

## Adjustments to the topic

In June 2024, I attended the annual Macroecology & Biogeography meeting of the GfÖ society in Marburg, Germany, which was under the theme **Artificial Intelligence in Biogeography and Macroecology**. Talks and posters at the conference mostly concerned machine learning techniques for biodiversity research. Following a talk by Dr. Florian Hartig on perks and drawbacks of different machine learning methods for inference models, I decided to switch from the application of random forest models to neural networks since these are said to be more accurate for complex situations with many predictors. In his talk, Dr. Hartig elaborated about random forest being unable to predict with uncorrelated data since predictor variables included in the building of a certain tree tend to borrow strength from other predictors (which are not included) and therefore lead to false variable importance when extracted across the full forest.

Moreover, inferring log ratio of AOO change may have been the wrong response variable to begin with for this project, since the distribution of this variable was predominantly centered around zero with few extreme cases of extreme increase or decrease, while predictor variables varied heavily, leading to the conclusion that there is just not enough signal in the log ratio for predictions.

While I was investigating the issue of very low predictive power, I detected that, although the change in occupied area is most often zero, spatial patterns of species distributions do change, suggesting that a measure of turnover may be better suited to characterize the temporal change in species distributions. Hence, I calculated *Jaccard index of similarity* on species level across sites (i.e., the site-similarity for a species from one sampling period to the next), to capture the spatial change signal in the occupancy data.

------------------------------------------------------------------------

# The Analysis

I will start by sourcing some functions that I have wrote to simplify my code. The first is `load_and_install()` and it's supposed to suppress the start messages from the packages while installing and loading all packages that are not already installed and loaded.

::: panel-tabset
## Source custom functions

```{r}
#| label: load-packages
#| message: FALSE
#| warning: FALSE
rm(list = ls())
source("src/functions.R")

```

## MachineLearning packages

```{r}
#| label: load-ML-packages
pckgs <- c("dplyr", "ggplot2", "reshape2", 
           "ggcorrplot", 
           "caret",  "recipes",   "caretEnsemble", 
           "randomForest", "ranger", "gbm", "xgboost", 
           "vegan", "pdp", 
           "gridExtra", "kableExtra")

install_and_load(pckgs)
```

## NeuralNetworks packages

```{r}
#| label: load-NN-packages
NN_pckgs <- install_and_load(c("cito",  "plotly", "igraph", "ggraph" ))
# Neural Networks requirements:
if (!require("torch", quietly = TRUE)) install.packages("torch")
library("torch")

# install torch
if (!torch_is_installed()) install_torch()
```

## Load RData to reduce computing time

```{r}
#| label: load-RData
# Load workspace to save computing time:
## it has: varPart from ranger models
## recursive feature selection results

# load("data/varPart_rfe.RData")
# load("data/models.RData")
```
:::

## The data

Since the raw data is not open, I am providing the (reduced) predictor table that I calculated from it and other external data.

The data has bird species in rows (`verbatim_name`) and their predictor data across different datasets (`dataset`) in columns.

The column `log_R2_1` is the log ratio of AOO (area of occupancy) between two sampling periods (indicated with `tp = 1` or `tp = 2`) and was the inital response for my temporal change models.

`Telfer_1_2` is another measure of temporal change, though it is relative for each species in a dataset in comparison to the average other species. Telfer will not be further investigated in this assignment.

`Jaccard` is the Jaccard index of similarity and indicates how similar ( 0 - 1) two species ranges are across different sampling periods.

# Preparations

## Data handling

Here I'm excluding some predictors which are overlapping with some others.

Next, I'm splitting the data into their time periods. In the following I will only continue to investigate the change from period 1 to period 2 (i.e, future change).

Some predictor columns have NAs that result from either very rare species or highly cosmopolitan species (thus resulting in division by 0 during computation). With knowledge of how I computed the predictors, I manually set some rows with NAs to 0 or 1.

Spatial autocorrelation (Moran's I) cannot be calculated for species occupying 100% of an area, thus resulting in NA. These species are removed completely from the model as there is no way to impute this value.

# Hypotheses

::: panel-tabset
## Hypothesis 1: Species Traits

```{r}
#| label: set-up-variables-H1
#| message: FALSE

H1_vars <- c(
    "sd_PC1", "sd_PC2",
    "GlobRangeSize_m2", "IUCN", "Mass", "Habitat", "Habitat.Density",
    "Migration", "Trophic.Level", "Trophic.Niche", "Primary.Lifestyle",
    "FP", "Hand.Wing.Index")
```

## Hypothesis 2: Species range geometry

```{r}
#| label: set-up-variables-H2
#| message: FALSE

H2_vars <- c(
    "AOO", "rel_occ_Ncells", "mean_prob_cooccur", "D_AOO_a", 
    "moran", "x_intercept", "sp_centr_lon", "sp_centr_lat",
    "lengthMinRect", "widthMinRect", "elonMinRect", "bearingMinRect",
    "circ", "bearing", "Southernness", "Westernness",
    "rel_maxDist", "rel_ewDist", "rel_nsDist", "rel_elonRatio",
    "rel_relCirc", "rel_circNorm", "rel_lin", "Dist_centroid_to_COG",
    "maxDist_toBorder_border", "maxDist_toBorder_centr",
    "minDist_toBorder_centr")
```

## Hypothesis 3: Diversity Metrics

```{r}
#| label: set-up-variables-H3
#| message: FALSE

H3_vars <- c("GammaSR", "AlphaSR_sp", "BetaSR_sp")
```

## Hypothesis 4: Atlas geometry

```{r}
#| label: set-up-variables-H4
#| message: FALSE

H4_vars <- c(
    "dataset", "mean_area", "Total_area_samp", "Total_Ncells_samp",
    "mean_cell_length", "atlas_lengthMinRect", "atlas_widthMinRect",
    "atlas_elonMinRect", "atlas_circ", "atlas_bearingMinRect",
    "atlas_bearing", "AtlasCOG_long", "AtlasCOG_lat")

```
:::

# Create data subsets

In the following part we will create 4 different datasets from our data table. We will be assessing whether change in occupied area (`log Ratio AOO`) or change in sites (`Jaccard`) can be better predicted per species, and whether past (`tp = 1`) or future (`tp = 2`) change can be better predicted.

## 1. Site Turnover:

::: panel-tabset
### Jaccard 1. Sampling period

```{r}
#| label: Jaccard-1-data-prep
response <- "Jaccard"
vars <- c(H1_vars, H2_vars, H3_vars, H4_vars)
tp_value <- 1
file_path <- "data/AllPredictors.rds"

# Function to process the data
dat_J1 <- process_data(file_path, tp_value, response, vars)

# Check NAs
summarize_NA(dat_J1)

```

### Jaccard 2. Sampling period

```{r}
#| label: Jaccard-2-data-prep
response <- "Jaccard"
vars <- c(H1_vars, H2_vars, H3_vars, H4_vars)
tp_value <- 2
file_path <- "data/AllPredictors.rds"

# Function to process the data
dat_J2 <- process_data(file_path, tp_value, response, vars)

# Check NAs
summarize_NA(dat_J2)


```

### Distribution

The distribution shows that our data spans the full range of Jaccard 0 to 1 and (besides Europe), the variable seems to be distributed uniformly. This can be advantageous for prediction modeling as most events are equally likely and data partitioning will most probably not bias the training data towards a certain pattern.

```{r}
# Plot response distribution
dat_J1 %>%
    select(Jaccard, dataset) %>%
    melt(id.vars = "dataset") %>%
    ggplot(aes(x = value, fill = dataset)) +
    geom_histogram(bins = 30, color = "black") +
    facet_wrap(~variable, scales = "free_x") +
    scale_fill_manual(values = c("#0072B2", "#E69F00", "#009E73", "#D55E00")) +
    theme_bw() +
    labs(
        title = "Species-level Jaccard index of site-similarity",
        x = "Jaccard",
        y = "Frequency"
    ) +
    facet_wrap(dataset ~ .)
```
:::

## 2. Area change

::: panel-tabset
### Log Ratio 1. Sampling period

```{r}
#| label: logRatio-1-data-prep
response <- "log_R2_1"
vars <- c(H1_vars, H2_vars, H3_vars, H4_vars)
tp_value <- 1
file_path <- "data/AllPredictors.rds"

# Function to process the data
dat_LR1 <- process_data(file_path, tp_value, response, vars)

# Check NAs
summarize_NA(dat_LR1)
```

### Log Ratio 2. Sampling period

```{r}
#| label: logRatio-2-data-prep
response <- "log_R2_1"
vars <- c(H1_vars, H2_vars, H3_vars, H4_vars)
tp_value <- 1
file_path <- "data/AllPredictors.rds"

# Function to process the data
dat_LR2 <- process_data(file_path, tp_value, response, vars)

# Check NAs
summarize_NA(dat_LR2)
```

### Distribution

We can see that the response variable log Ratio has little variation and is mainly distributed around 0. This may be a sign of weak signal of temporal change in this variable, and thus lead to low predictive performance.

```{r}
# Plot response distribution
dat_LR1 %>%
    select(log_R2_1, dataset) %>%
    melt(id.vars = "dataset") %>%
    ggplot(aes(x = value, fill = dataset)) +
    geom_histogram(bins = 30, color = "black") +
    facet_wrap(~variable, scales = "free_x") +
    scale_fill_manual(values = c("#0072B2", "#E69F00", "#009E73", "#D55E00")) +
    theme_bw() +
    labs(
        title = "Species-level change in AOO",
        x = "Log Ratio",
        y = "Frequency"
    ) +
    facet_wrap(dataset ~ .)
```
:::

## Relationships between variables

The feature plots show how the variables for each hypothesis are related to each other and to the response variable. We can see that some relationships follow distinct patterns which suggests a correlation between variables. We will check this more specifically below and remove any variables that are correlated more than a certain threshold.

::: panel-tabset
## Feature plot: H1

```{r}
#| label: feature-plot-H1
#| fig.width: 8
#| fig.height: 12

featurePlot(x = dat_J1 %>% select(dataset, all_of(H1_vars)),
    y = dat_J1$Jaccard,
    group = dat_J1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species traits (H1) - Jaccard 1",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_J2 %>% select(dataset, all_of(H1_vars)),
    y = dat_J2$Jaccard,
    group = dat_J2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species traits (H1) - Jaccard 2",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_LR1 %>% select(dataset, all_of(H1_vars)),
    y = dat_LR1$log_R2_1,
    group = dat_LR1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species traits (H1) - Log ratio 1",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_LR2 %>% select(dataset, all_of(H1_vars)),
    y = dat_LR2$log_R2_1,
    group = dat_LR2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species traits (H1) - log Ratio 2",
    par.settings =
        list(fontsize = list(text = 4)))
```

## Feature plot: H2

```{r}
#| label: feature-plot-H2
#| fig.width: 8
#| fig.height: 12


featurePlot(x = dat_J1 %>% select(dataset, all_of(H2_vars)),
    y = dat_J1$Jaccard,
    group = dat_J1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species geometry (H2) - Jaccard 1",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_J2 %>% select(dataset, all_of(H2_vars)),
    y = dat_J2$Jaccard,
    group = dat_J2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species geometry (H2) - Jaccard 2",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_LR1 %>% select(dataset, all_of(H2_vars)),
    y = dat_LR1$log_R2_1,
    group = dat_LR1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species geometry (H2) - log Ratio 1",
    par.settings =
        list(fontsize = list(text = 4)))

featurePlot(x = dat_LR2 %>% select(dataset, all_of(H2_vars)),
    y = dat_LR2$log_R2_1,
    group = dat_LR2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.2,
    xlab = "Scatterplot Matrix of species geometry (H2) - log Ratio 2",
    par.settings =
        list(fontsize = list(text = 4)))
```

## Feature plot: H3

```{r}
#| label: feature-plot-H3
#| fig.width: 8
#| fig.height: 12

featurePlot(x = dat_J1 %>% select(dataset, all_of(H3_vars)),
    y = dat_J1$Jaccard,
    group = dat_J1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.5,
    xlab = "Scatterplot Matrix of diversity metrics (H3) - Jaccard 1",
    par.settings =
        list(fontsize = list(text = 6)))

featurePlot(x = dat_J2 %>% select(dataset, all_of(H3_vars)),
    y = dat_J2$Jaccard,
    group = dat_J2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.5,
    xlab = "Scatterplot Matrix of diversity metrics (H3) - Jaccard 2",
    par.settings =
        list(fontsize = list(text = 6)))

featurePlot(x = dat_LR1 %>% select(dataset, all_of(H3_vars)),
    y = dat_LR1$log_R2_1,
    group = dat_LR1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.5,
    xlab = "Scatterplot Matrix of diversity metrics (H3) - log Ratio 1",
    par.settings =
        list(fontsize = list(text = 6)))

featurePlot(x = dat_LR2 %>% select(dataset, all_of(H3_vars)),
    y = dat_LR2$log_R2_1,
    group = dat_LR2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.3,
    cex = 0.5,
    xlab = "Scatterplot Matrix of diversity metrics (H3) - log Ratio 1",
    par.settings =
        list(fontsize = list(text = 6)))

```

## Feature plot: H4

```{r}
#| label: feature-plot-H4
#| fig.width: 8
#| fig.height: 12

featurePlot(x = dat_J1 %>% select(dataset, all_of(H4_vars)),
    y = dat_J1$Jaccard,
    group = dat_J1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.6,
    cex = 0.5,
    xlab = "Scatterplot Matrix of atlas specifics (H4) - Jaccard 2",
    par.settings =
        list(fontsize = list(text = 5)))

featurePlot(x = dat_J2 %>% select(dataset, all_of(H4_vars)),
    y = dat_J2$Jaccard,
    group = dat_J2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.6,
    cex = 0.5,
    xlab = "Scatterplot Matrix of atlas specifics (H4) - Jaccard 2",
    par.settings =
        list(fontsize = list(text = 5)))

featurePlot(x = dat_LR1 %>% select(dataset, all_of(H4_vars)),
    y = dat_LR1$log_R2_1,
    group = dat_LR1$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.6,
    cex = 0.5,
    xlab = "Scatterplot Matrix of atlas specifics (H4) - log Ratio 1",
    par.settings =
        list(fontsize = list(text = 5)))

featurePlot(x = dat_LR2 %>% select(dataset, all_of(H4_vars)),
    y = dat_LR2$log_R2_1,
    group = dat_LR2$dataset,
    plot = "pairs",
    pch = 16,
    alpha = 0.6,
    cex = 0.5,
    xlab = "Scatterplot Matrix of atlas specifics (H4) - log Ratio 2",
    par.settings =
        list(fontsize = list(text = 5)))
```
:::

## Correlation Matrix

Next, we will check how correlated the predictors are and remove those with r \> 0.85.

Those variables are:

-   Jaccard tp 1 (n = 17)

```         
 [1] "maxDist_toBorder_centr"  "atlas_lengthMinRect"     "mean_cell_length"        "maxDist_toBorder_border"  [5] "mean_area"               "atlas_circ"              "atlas_elonMinRect"       "Total_area_samp"          [9] "GammaSR"                 "atlas_bearing"           "lengthMinRect"           "rel_ewDist"              [13] "rel_nsDist"              "AtlasCOG_lat"            "rel_circNorm"            "AtlasCOG_long"           [17] "atlas_bearingMinRect"  
```

-   Jaccard tp 2 (n = 18)

```         
 [1] "atlas_lengthMinRect"     "maxDist_toBorder_centr"  "mean_cell_length"        "maxDist_toBorder_border"  [5] "mean_area"               "atlas_circ"              "atlas_elonMinRect"       "Total_area_samp"          [9] "GammaSR"                 "atlas_bearing"           "lengthMinRect"           "rel_ewDist"              [13] "rel_nsDist"              "AtlasCOG_lat"            "rel_circNorm"            "AtlasCOG_long"           [17] "mean_prob_cooccur"       "atlas_bearingMinRect" 
```

-   log Ratio 1 (n = 17)

```         
[1] "maxDist_toBorder_centr"  "atlas_lengthMinRect"     "mean_cell_length"        "maxDist_toBorder_border"  [5] "mean_area"               "atlas_circ"              "atlas_elonMinRect"       "Total_area_samp"          [9] "GammaSR"                 "atlas_bearing"           "lengthMinRect"           "rel_ewDist"              [13] "rel_nsDist"              "AtlasCOG_lat"            "rel_circNorm"            "AtlasCOG_long"           [17] "atlas_bearingMinRect"   
```

-   log Ratio 2 (n =

::: panel-tabset
## corr Jaccard 1

```{r}
#| label: correlation-matrix-J1
#| fig.width: 30
#| fig.height: 30

cor_df <- dat_J1 %>% select(-Jaccard)
p.mat <- model.matrix(~ 0 + ., data = cor_df) %>%
    cor_pmat()

correlation_matrix <- cor_df %>%
    select_if(is.numeric) %>%
    cor(use = "pairwise.complete.obs")
correlation_matrix %>%
    ggcorrplot(
        hc.order = TRUE,
        lab = TRUE,
        lab_size = 3,
        p.mat = p.mat,
        insig = "blank"
    )

# We will set the threshold for excluding correlations = 0.85
# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity

cor_vars <- findCorrelation(correlation_matrix,
    cutoff = .85,
    names = TRUE,
    exact = TRUE,
    verbose = FALSE)

# cor_vars # 17 variables seemed to be highly correlated. We will exclude

model_dat_J1 <- dat_J1 %>%
    select(!all_of(cor_vars))

summarize_NA(model_dat_J1)
# leaves us with 39 predictor variables to predict the response
```

## corr Jaccard 2

```{r}
#| label: correlation-matrix-J2
#| fig.width: 30
#| fig.height: 30

cor_df <- dat_J2 %>% select(-Jaccard)
p.mat <- model.matrix(~ 0 + ., data = cor_df) %>%
    cor_pmat()

correlation_matrix <- cor_df %>%
    select_if(is.numeric) %>%
    cor(use = "pairwise.complete.obs")
correlation_matrix %>%
    ggcorrplot(
        hc.order = TRUE,
        lab = TRUE,
        lab_size = 3,
        p.mat = p.mat,
        insig = "blank"
    )

# We will set the threshold for excluding correlations = 0.85
# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity

cor_vars <- findCorrelation(correlation_matrix,
    cutoff = .85,
    names = TRUE,
    exact = TRUE,
    verbose = FALSE)

 cor_vars # 18 variables seemed to be highly correlated. We will exclude

model_dat_J2 <- dat_J2 %>%
    select(!all_of(cor_vars))

summarize_NA(model_dat_J2)
# leaves us with 39 predictor variables to predict the response
```

## corr log ratio 1

```{r}
#| label: correlation-matrix-LR1
#| fig.width: 30
#| fig.height: 30

cor_df <- dat_LR1 %>% select(-log_R2_1)
p.mat <- model.matrix(~ 0 + ., data = cor_df) %>%
    cor_pmat()

correlation_matrix <- cor_df %>%
    select_if(is.numeric) %>%
    cor(use = "pairwise.complete.obs")
correlation_matrix %>%
    ggcorrplot(
        hc.order = TRUE,
        lab = TRUE,
        lab_size = 3,
        p.mat = p.mat,
        insig = "blank"
    )

# We will set the threshold for excluding correlations = 0.85
# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity

cor_vars <- findCorrelation(correlation_matrix,
    cutoff = .85,
    names = TRUE,
    exact = TRUE,
    verbose = FALSE)

 cor_vars # 17 variables seemed to be highly correlated. We will exclude

model_dat_LR1 <- dat_LR1 %>%
    select(!all_of(cor_vars))

summarize_NA(model_dat_LR1)
# leaves us with 39 predictor variables to predict the response
```

## corr log ratio 2

```{r}
#| label: correlation-matrix-LR1
#| fig.width: 30
#| fig.height: 30

cor_df <- dat_LR1 %>% select(-log_R2_1)
p.mat <- model.matrix(~ 0 + ., data = cor_df) %>%
    cor_pmat()

correlation_matrix <- cor_df %>%
    select_if(is.numeric) %>%
    cor(use = "pairwise.complete.obs")
correlation_matrix %>%
    ggcorrplot(
        hc.order = TRUE,
        lab = TRUE,
        lab_size = 3,
        p.mat = p.mat,
        insig = "blank"
    )

# We will set the threshold for excluding correlations = 0.85
# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity

cor_vars <- findCorrelation(correlation_matrix,
    cutoff = .85,
    names = TRUE,
    exact = TRUE,
    verbose = FALSE)

 cor_vars # 17 variables seemed to be highly correlated. We will exclude

model_dat_LR1 <- dat_LR1 %>%
    select(!all_of(cor_vars))

summarize_NA(model_dat_LR1)
# leaves us with 39 predictor variables to predict the response
```
:::

```{r}
#| label: correlation-matrix
#| fig.width: 30
#| fig.height: 30

cor_df <- dat %>% select(-Jaccard)
p.mat <- model.matrix(~ 0 + ., data = cor_df) %>%
    cor_pmat()

correlation_matrix <- cor_df %>%
    select_if(is.numeric) %>%
    cor(use = "pairwise.complete.obs")
correlation_matrix %>%
    ggcorrplot(
        hc.order = TRUE,
        lab = TRUE,
        lab_size = 3,
        p.mat = p.mat,
        insig = "blank"
    )

# We will set the threshold for excluding correlations = 0.85
# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity

cor_vars <- findCorrelation(correlation_matrix,
    cutoff = .85,
    names = TRUE,
    exact = TRUE,
    verbose = TRUE
)
# cor_vars # 17 variables seemed to be highly correlated. We will exclude

model_dat <- dat %>%
    select(!all_of(cor_vars))

summarize_NA(model_dat)
# leaves us with 39 predictor variables to predict the response
```

## Modeling

### Pre-processing:

First we have to check if there are (near) zero variance variables in the predictors. These can be removed since they will not explain a lot generally.

Second, we will exclude all correlated variables with pearson's pairwise correlations coefficients \> 0.85.

Third, we will impute NA values based on knn-imputation with 5 neighbors (default).

```{r}
#| label: recipe-pre-processing

# Step 1. Near Zero Vars
nzv <- nearZeroVar(model_dat, saveMetrics = T)
nzv %>% filter(nzv == T)
# only IUCN, but this is an important predictor (!) we will keep it.

# Step 2 & 3: imputing missing values & removing highly correlated variables
recipe_pp <- recipe(Jaccard ~ .,
    data = dat
) %>%
    step_corr(all_numeric_predictors(), threshold = .85) %>%
    step_impute_knn(all_predictors())

# Estimate recipe on data:
recipe_pp_prepped <- prep(recipe_pp, dat)

# Removed columns:
recipe_pp_prepped$steps[[1]]$removals

# apply the recipe to the data:
dat_v2 <- bake(recipe_pp_prepped, dat)

```

### Training & Validation sets:

```{r}
set.seed(42)
# Initial split to training and validation set (for final evaluation, keep dat_test completely out of the training sets)
index <- createDataPartition(dat_v2$Jaccard, p = 0.8, 1, list = FALSE)

dat_train <- dat_v2[index, ]
dat_test <- dat_v2[-index, ]

# Cross-validation resampling indices 
indices <- createDataPartition(dat_train$Jaccard, p = 0.8, 10) # 10 resamples
```

### Variation Partitioning between Hypotheses

```{r}
#| label: var-part-vegan

# Create a data frame for each set of variables
H1_data <- dat_train %>% select(any_of(H1_vars))
H2_data <- dat_train %>% select(any_of(H2_vars))
H3_data <- dat_train %>% select(any_of(H3_vars))
H4_data <- dat_train %>% select(any_of(H4_vars))

# Perform basic variation partitioning (vegan package)
varpart_model <- varpart(dat_train$Jaccard, H1_data, H2_data, H3_data, H4_data)

# Print the results
print(varpart_model)
summary(varpart_model)



## VENN-diagram
plot(varpart_model,
     Xnames = c("Species Traits", "Species Range Geometry", "Diversity", "Atlas characteristics"), # name the partitions
     bg = c("seagreen3", "mediumpurple", "darkorange", "gold"), alpha = 80, # colour the circles
     digits = 2, # only show 2 digits
     cex = 0.8,
     id.size = 1)


# Test for significance in results:
anova.cca(rda(dat_train$Jaccard, H1_data)) # sign. p = 0.001
anova.cca(rda(dat_train$Jaccard, H2_data)) # sign. p = 0.001
anova.cca(rda(dat_train$Jaccard, H3_data)) # sign. p = 0.001
anova.cca(rda(dat_train$Jaccard, H4_data)) # sign. p = 0.001

```

```{r}
#| label: var-part-rf
#| eval: FALSE

trainControl <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    savePredictions = "final",
    returnResamp = "final",
    verboseIter = TRUE,
    index = indices)

tictoc::tic("ranger full model")
set.seed(42)
full_ranger <- train(
    Jaccard ~ .,
    data = dat_train,
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)
tictoc::toc()


# Train ranger model
tictoc::tic("ranger H1")
set.seed(42)
H1_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(H1_vars)),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)
tictoc::toc()


tictoc::tic("ranger H2")
set.seed(42)
H2_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(H2_vars)),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)
tictoc::toc()


tictoc::tic("ranger H3")
set.seed(42)
H3_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(H3_vars)),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)
tictoc::toc()

tictoc::tic("ranger H4")
set.seed(42)
H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(H4_vars)),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)
tictoc::toc()


### combinations of 2 hypotheses:

set.seed(42)
H1H2_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H2_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H1H3_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H3_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H1H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H4_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)


set.seed(42)
H2H3_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H2_vars, H3_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)


set.seed(42)
H2H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H2_vars, H4_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H3H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H3_vars, H4_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)


### combinations of 3 hypotheses together =====

set.seed(42)
H1H2H3_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H2_vars, H3_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H1H2H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H2_vars, H4_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H1H3H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H3_vars, H4_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

set.seed(42)
H2H3H4_ranger <- train(
    Jaccard ~ .,
    data = dat_train %>% select(Jaccard, any_of(c(H1_vars, H2_vars, H3_vars))),
    method = "ranger",
    trControl = trainControl,
    importance = "permutation",
    scale.permutation.importance = TRUE,
    num.trees = 5000,
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    tuneLength = 5)

# save.image("data/varPart_rfe.RData")

```

```{r}
#| label: var-part-ranger-performance-eval
# ===== Performance eval ======= #

# Predict on your test data
predictions_full <- predict(full_ranger, newdata = dat_test)

predictions_H1 <- predict(H1_ranger, newdata = dat_test)
predictions_H2 <- predict(H2_ranger, newdata = dat_test)
predictions_H3 <- predict(H3_ranger, newdata = dat_test)
predictions_H4 <- predict(H4_ranger, newdata = dat_test)

predictions_H1H2 <- predict(H1H2_ranger, newdata = dat_test)
predictions_H1H3 <- predict(H1H3_ranger, newdata = dat_test)
predictions_H1H4 <- predict(H1H4_ranger, newdata = dat_test)
predictions_H2H3 <- predict(H2H3_ranger, newdata = dat_test)
predictions_H2H4 <- predict(H2H4_ranger, newdata = dat_test)
predictions_H3H4 <- predict(H3H4_ranger, newdata = dat_test)

predictions_H1H2H3 <- predict(H1H2H3_ranger, newdata = dat_test)
predictions_H1H2H4 <- predict(H1H2H4_ranger, newdata = dat_test)
predictions_H1H3H4 <- predict(H1H3H4_ranger, newdata = dat_test)
predictions_H2H3H4 <- predict(H2H3H4_ranger, newdata = dat_test)

# Calculate the performance metrics
perf <- rbind(postResample(pred = predictions_full, obs = dat_test$Jaccard),
              postResample(pred = predictions_H1, obs = dat_test$Jaccard),
              postResample(pred = predictions_H2, obs = dat_test$Jaccard),
              postResample(pred = predictions_H3, obs = dat_test$Jaccard),
              postResample(pred = predictions_H4, obs = dat_test$Jaccard),
              
              postResample(pred = predictions_H1H2, obs = dat_test$Jaccard),
              postResample(pred = predictions_H1H3, obs = dat_test$Jaccard),
              postResample(pred = predictions_H1H4, obs = dat_test$Jaccard),
              postResample(pred = predictions_H2H3, obs = dat_test$Jaccard),
              postResample(pred = predictions_H2H4, obs = dat_test$Jaccard),
              postResample(pred = predictions_H3H4, obs = dat_test$Jaccard),
              
              postResample(pred = predictions_H1H2H3, obs = dat_test$Jaccard),
              postResample(pred = predictions_H1H2H4, obs = dat_test$Jaccard),
              postResample(pred = predictions_H1H3H4, obs = dat_test$Jaccard),
              postResample(pred = predictions_H2H3H4, obs = dat_test$Jaccard)) %>% 
                as.data.frame() %>% 
                round(4)

model <- c("full", 
            seq(1:4), 
            "H1H2", "H1H3", "H1H4", "H2H3", "H2H4", "H3H4", 
            "H1H2H3", "H1H2H4", "H1H3H4", "H2H3H4")
perf$model <- model

# Print the performance metrics

perf %>% 
    kableExtra::kable()
perf %>% 
    kableExtra::kable() %>% 
    write.csv("data/performance_varExpl_rf.csv")

slice_min(perf, RMSE) %>% 
    slice_max(Rsquared)


# Create a bar plot of variance explained
ggplot(perf, aes(x = reorder(model, RMSE), y = Rsquared)) +
  geom_bar(stat = "identity") +
  labs(x = "Model", y = "Variance Explained") +
  theme_minimal()

```

### Predictor importance / Recursive Feature Selection

```{r}
#| eval = FALSE

## Default summary function (rfFuncs)
rfFuncs <- list(
    summary = 
    function (data, lev = NULL, model = NULL) {
    if (is.character(data$obs)) 
        data$obs <- factor(data$obs, levels = lev)
    postResample(data[, "pred"], data[, "obs"])
    },
    
    fit = 
    function (x, y, first, last, ...) {
    loadNamespace("randomForest")
    randomForest::randomForest(x, y, importance = TRUE, ...)
    },
    
    pred = 
    function (object, x) {
    tmp <- predict(object, x)
    if (is.factor(object$y)) {
        out <- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
            x, type = "prob"), stringsAsFactors = TRUE))}
    else out <- tmp
    out
    },
    
    rank = 
    function (object, x, y) {
        vimp <- varImp(object)
        if (is.factor(y)) {
            if (all(levels(y) %in% colnames(vimp))) {
                avImp <- apply(vimp[, levels(y), drop = TRUE], 1, mean)
                vimp$Overall <- avImp}
                }
        vimp <- vimp[order(vimp$Overall, decreasing = TRUE), , drop = FALSE]
        if (ncol(x) == 1) {
            vimp$var <- colnames(x)}
        else vimp$var <- rownames(vimp)
        vimp
    },

    selectSize = 
    function (x, metric, maximize) {
        best <- if (maximize) 
        which.max(x[, metric])
        else which.min(x[, metric])
        min(x[best, "Variables"])
    },

    selectVar = 
    function (y, size) {
        finalImp <- ddply(y[, c("Overall", "var")], .(var), function(x) mean(x$Overall, 
        na.rm = TRUE))
        names(finalImp)[2] <- "Overall"
        finalImp <- finalImp[order(finalImp$Overall, decreasing = TRUE), ]
        as.character(finalImp$var[1:size])
    }
)


## Custom summary function for randomForest (simplified)
rfRFE1 <- list(
    summary = defaultSummary,
    fit = function(x, y, first, last, ...) {
        library(randomForest)
        randomForest(x, y, importance = first, ...)
    },
    pred = function(object, x) predict(object, x),
    rank = function(object, x, y) {
        vimp <- varImp(object, type = 1, scale = TRUE)
        vimp <- vimp[order(vimp$Overall, decreasing = TRUE), , drop = FALSE]
        vimp$var <- rownames(vimp)
        vimp
    },
    selectSize = pickSizeBest,
    selectVar = pickVars
)


## Recursive feature selection:
set.seed(42)
ctrl <- rfeControl(
    functions = rfFuncs,
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    returnResamp = "all", # we need all resamples
    verbose = FALSE,
    index = indices,
    saveDetails = TRUE,
    timingSamps = 10
)

rank <- function (object, x, y){
    vimp <- varImp(object, type = 1, scale = TRUE)
    if (is.factor(y)) {
        if (all(levels(y) %in% colnames(vimp))) {
            avImp <- apply(vimp[, levels(y), drop = TRUE], 1, 
                mean)
            vimp$Overall <- avImp
        }
    }
    vimp <- vimp[order(vimp$Overall, decreasing = TRUE), , drop = FALSE]
    if (ncol(x) == 1) {
        vimp$var <- colnames(x)
    }
    else vimp$var <- rownames(vimp)
    vimp
}
ctrl$functions$rank <- rank

## Variable importance
set.seed(42)
ctrl2 <- rfeControl(
    functions = rfRFE1,
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    returnResamp = "all", # we need all resamples
    verbose = FALSE,
    index = indices,
    saveDetails = TRUE,
    timingSamps = 10
)



set.seed(42)
subsets <- c(1:50)

x <- dat_train %>% select(-Jaccard)
y <- dat_train %>% pull(Jaccard)


# Empty list of lists that will be filled with each iteration of the loop (total = 10 iterations
saved_profiles <- replicate(10, list())
tictoc::tic()
for (i in 1:10){


## First run:
rfProfile <- rfe(x, y, sizes = subsets, rfeControl = ctrl, ntrees = 5000)
rfProfile

# Most important predictors:
rfProfile$fit$importance %>% 
    round(3) %>% 
    as.data.frame() %>%
    select("%IncMSE") %>%
    arrange(desc(.))
imp1 <- varImp(rfProfile) # overall importance (mean across resamples)
## This one selects 35 variables
## Second run: selects 32 variables


## Second run:
rfProfile2 <- rfe(x, y, sizes = subsets, rfeControl = ctrl2, ntrees = 5000)
rfProfile2$fit$importance %>%
    round(3) %>% 
    as.data.frame() %>%
    #select("%IncMSE") %>%
    arrange(desc(.))

imp2 <- varImp(rfProfile2)


## This one selects only 20 variables
## Second run: selects 31 variables

## Comparison between both models:
merge(imp1,imp2, by = "row.names", all = T) %>% 
    mutate_if(is.numeric, round, digits = 3) %>%
    as.data.frame() %>%
    arrange(desc(Overall.x))

saved_profiles[[i]] <- list(rfProfile, rfProfile2)
}

tictoc::toc()

# save.image("data/varPart_rfe.RData")
```

```{r}
#| label: rfe-results
#| fig.height: 10
#| fig.width: 8

results <- replicate(10, list())
for(i in seq_along(1:length(saved_profiles))){
    for (y in seq_along(1:length(saved_profiles[[i]]))){
        resamp_res <- saved_profiles[[i]][[y]]
        res <- slice_min(resamp_res$results, RMSE)
        results[[i]][[y]] <- res
        
    }
}

rfe_res <- do.call(rbind, unlist(results, recursive = FALSE))
rfe_res$model <- rep(c("default", "simple"), 10)
ggplot(data = rfe_res, aes(x = model, y = Variables)) +
    geom_boxplot() +
    geom_point(data = rfe_res %>% group_by(model) %>% summarize(mean_Variables = mean(Variables)), 
    aes(x = model, y = mean_Variables), color = "red") +
    theme_bw()

rfe_res %>% group_by(model) %>% summarize(mean_Variables = mean(Variables)) # 25.4 for both
rfe_res %>% group_by(model) %>% summarize(median_Variables = median(Variables)) # 26 for default, 24 for simple. Let's go with the results for the default model: 26.

saved_res2 <- unlist(saved_profiles, recursive = FALSE)
saved_res3 <- saved_res2[c(seq(from = 1, to = 20, by = 2))] # keep only default models

res_top_vars <- list()
for(i in seq_along(saved_res3)){

res_top_vars[[i]] <- data.frame(
    var = row.names(varImp(saved_res3[[i]], scale = T)),
    Imp = varImp(saved_res3[[i]])$Overall,
    include = c(rep_len(1, as.numeric(saved_res3[[i]]$bestSubset)), 
                rep_len(0,  
                length(row.names(varImp(saved_res3[[i]], scale = T)))-as.numeric(saved_res3[[i]]$bestSubset))
            ),
    model = i
        )
}

res_top_vars_df <- do.call(rbind, res_top_vars)



### Plot =====

# Calculate the maximum count of resamples for scaling
max_count <- res_top_vars_df %>% count(var) %>% pull(n) %>% max()

ggplot(data = res_top_vars_df) +
  # Background rectangles
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 10.5), 
            fill = "lightgray", alpha = 0.9) +
  # Box plot
  geom_boxplot(aes(y = reorder(var, Imp), x = Imp)) +
  # Bar plot scaled to the secondary axis
  geom_bar(aes(y = reorder(var, Imp), x = after_stat(count) / max_count * max(res_top_vars_df$Imp), 
               fill = factor(include)), stat = "count",  alpha = 0.3) +
  # Horizontal line
  geom_hline(yintercept = 10.5) +
  # Secondary axis that stretches through the entire range
  scale_x_continuous(sec.axis = sec_axis(~ . * max_count / max(res_top_vars_df$Imp), 
                                         name = "Count Resamples (Secondary Axis)")) +
  # Scale and theme
  scale_fill_manual(values = c("#D55E00", "#009E73")) +
  theme_bw() +
  theme(legend.position = "right") +
  labs(title = "Top Variables by Importance", x = "Importance", y = "Variable")


```

### Individual models

#### Hyperparameter tuning

```{r}
#| label: hyperparameter-tuning

# Define training control ==========================================================
trainControl <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    savePredictions = "final",
    returnResamp = "all",
    verboseIter = FALSE,
    index = indices)

## Train ranger model ==========================================================
# set.seed(42)
# tictoc::tic("ranger")
# rangerModel_t <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "ranger",
#     trControl = trainControl,
#     importance = "permutation",
#     scale.permutation.importance = TRUE,
#     num.trees = 5000,
#     respect.unordered.factors = TRUE,
#     oob.error = TRUE,
#     tuneLength = 20)
# saveRDS(rangerModel_t, "./data/rangerModel_all.rds")
# tictoc::toc()
rangerModel_t <- readRDS("./data/rangerModel_all.rds")

### Model results:
p_rangerModel <- plot(rangerModel_t)
p_rangerModel
rangerModel_t$finalModel

## Train xgbTree model ==========================================================
xgb_grid <- expand.grid(
  nrounds = c(1000),
  eta = c(0.1, 0.3),
  max_depth = c(2,3, 5),
  gamma = c(0, 0.01, 0.1),
  colsample_bytree = 0.6,
  min_child_weight = 1,
  subsample = c(0.75, 1))

# tictoc::tic("xgb")
# set.seed(42)
# xgbModel_t <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "xgbTree",
#     trControl = trainControl,
#     tuneGrid = xgb_grid)
# tictoc::toc()
# saveRDS(xgbModel_t, "./data/xgbModel_all_TLCUSTOM.rds")

xgbModel_t <- readRDS("./data/xgbModel_all_TL3.rds")

### Model results:
p_xgbModel <- plot(xgbModel_t)
p_xgbModel
slice_min(xgbModel_t$results, RMSE)
slice_max(xgbModel_t$results, Rsquared)

## Train gbm model ==========================================================
# set.seed(42)
# tictoc::tic("gbm")
# gbmModel_t <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "gbm",
#     trControl = trainControl,
#     tuneLength= 20,
#     verbose = FALSE)
# saveRDS(gbmModel_t, "./data/gbmModel_all.rds")
# tictoc::toc()
gbmModel_t <- readRDS("./data/gbmModel_all.rds")

### Model results:
summary.gbm(gbmModel_t$finalModel)
p_gbmModel <- plot(gbmModel_t)
p_gbmModel
gbmModel_t$finalModel
slice_min(gbmModel_t$results, RMSE)
slice_max(gbmModel_t$results, Rsquared)

# save.image("./data/hyper_para_tuning.RData")
```

#### Final models

##### Insights:

1.  Random Forest with Ranger Keeping in mind that splitrule = "extratrees" ignores the mtry argument (and rather performs random selection of variables). This should prevent overfitting and achieves a similar performance to mtry = intermediate number and splitrule = "variance".

2.  Extreme Gradient Boosted Trees

3.  Boosted Regression Trees

```{r}
#| label: final-models-ranger

trainControl <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    savePredictions = "final",
    returnResamp = "final",
    verboseIter = FALSE,
    index = indices)

# Train ranger model
ranger_grid <- expand.grid(
    splitrule = "variance",
    mtry = 29,
    min.node.size = 5)

# tictoc::tic("ranger")
# set.seed(42)
# rangerModel <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "ranger",
#     trControl = trainControl,
#     importance = "permutation",
#     scale.permutation.importance = TRUE,
#     num.trees = 5000,
#     respect.unordered.factors = TRUE,
#     oob.error = TRUE,
#     tuneGrid = ranger_grid)
# tictoc::toc()
# saveRDS(rangerModel, "./data/rangerModel_final.rds")

rangerModel <- readRDS("./data/rangerModel_final.rds")

# Performance checks ======
## with external data (from initial split)
test_performance_rf <- data.frame(
    prediction = predict(rangerModel, newdata = dat_test),
    observed = dat_test$Jaccard) %>%
    mutate(test_error = observed-prediction)

round(mean(test_performance_rf$test_error), 4) 
# mean test error = 0.0092 (mtry = 29) // 0.0099 (mtry = 57)
postResample(test_performance_rf$observed, 
test_performance_rf$prediction)


p_pred_rangerModel <- ggplot(aes(observed, prediction), 
                             data = test_performance_rf)+
  geom_point()+
  geom_smooth(method = "lm")+
  ylim(0,1)+
  xlim(0,1)+
  theme_bw()

p_pred_rangerModel

```

```{r}
#| label: final-models-xgb
## custom tuning for xgb:
xgb_grid <- expand.grid(
  nrounds = 150,
  eta = 0.3,
  max_depth = 2,
  gamma = 0,
  colsample_bytree = 0.6,
  min_child_weight = 1,
  subsample = 1)

# tictoc::tic("xgb")
# set.seed(42)
# xgbModel <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "xgbTree",
#     trControl = trainControl,
#     tuneGrid = xgb_grid)
# tictoc::toc()
# saveRDS(xgbModel, "./data/xgbModel_final.rds")

xgbModel <- readRDS("./data/xgbModel_final.rds")

## Check test performance with external data (from initial split)
test_performance_xgb <- data.frame(
    prediction = predict(xgbModel, newdata = dat_test),
    observed = dat_test$Jaccard) %>%
    mutate(test_error = observed-prediction)

round(mean(test_performance_xgb$test_error), 4) 
# mean test error = 
postResample(test_performance_xgb$observed, 
test_performance_xgb$prediction)

p_pred_xgbModel <- ggplot(aes(observed, prediction), data = test_performance_xgb)+
    geom_point()+
    geom_smooth(method = "lm")+
    ylim(0,1)+
    xlim(0,1)+
    theme_bw()
p_pred_xgbModel

```

```{r}
#| label: final-models-gbm
# Train gbm model
gbm_grid <-  expand.grid(interaction.depth = 15, 
                        n.trees = 350, 
                        shrinkage = 0.1,
                        n.minobsinnode = 10)
# tictoc::tic("gbm")
# set.seed(42)
# gbmModel <- train(
#     Jaccard ~ .,
#     data = dat_train,
#     method = "gbm",
#     trControl = trainControl,
#     tuneGrid = gbm_grid,
#     verbose = FALSE)
# tictoc::toc()
# saveRDS(gbmModel, "./data/gbmModel_final.rds")

gbmModel <- readRDS("./data/gbmModel_final.rds")

## Check test performance with external data (from initial split)
test_performance_gbm <- data.frame(
    prediction = predict(gbmModel, newdata = dat_test),
    observed = dat_test$Jaccard) %>%
    mutate(test_error = observed-prediction)

round(mean(test_performance_gbm$test_error), 4) 
# mean test error = 
postResample(test_performance_gbm$observed, test_performance_gbm$prediction)

p_pred_gbmModel <- ggplot(aes(observed, prediction), data = test_performance_gbm)+
    geom_point()+
    geom_smooth(method = "lm")+
    ylim(0,1)+
    xlim(0,1)+
    theme_bw()
p_pred_gbmModel

```

### Compare models

```{r}
#| label: final-models-comparison
# Prediction error for test data ============
test_error_all <- data.frame(
    observed = dat_test$Jaccard,
    pred_ranger = predict(rangerModel, newdata=dat_test),
    pred_xgb = predict(xgbModel, newdata=dat_test),
    pred_gbm = predict(gbmModel, newdata=dat_test)) %>%
    mutate(
        test_error_ranger = observed-pred_ranger,
        test_error_xgb = observed-pred_xgb,
        test_error_gbm = observed-pred_gbm)

round(mean(test_error_all$test_error_ranger),4)
round(mean(test_error_all$test_error_xgb),4)
round(mean(test_error_all$test_error_gbm),4)
print(test_error_all)

# Variable importance ============
imp_rf <- varImp(rangerModel, type=1)$importance %>% 
    rename("Imp_ranger" = "Overall")
imp_xgb <- varImp(xgbModel, type=1)$importance %>% 
    rename("Imp_xgb" = "Overall")
imp_gbm <- varImp(gbmModel, type=1)$importance %>% 
    rename("Imp_gbm" = "Overall")

imp_temp <- merge(imp_rf, imp_xgb, by = "row.names", all = TRUE)
imp_all <- merge(imp_temp, imp_gbm, by.x = "Row.names", by.y = "row.names", all = TRUE) %>% mutate_if(is.numeric, round, 2) %>%
arrange(desc(Imp_ranger))

# Compare models using resamples ============
resamples <- resamples(list(
    ranger = rangerModel,
    xgbTree = xgbModel,
    gbm = gbmModel))

summary(resamples)
xyplot(resamples)

# Plot comparison
bwplot(resamples)
```

```{r}
#| eval: FALSE
#| label: save-image-load-image
save.image("./data/models.RData")
load("./data/models.RData")
```

### Partial Dependence Plots

```{r}
#| label: partial-dependence-plots
#| fig.width: 18
#| fig.height: 40

## Partial dependence plots
# Get all partial dependencies ==================
# pp_list_ranger <- replicate(39, list())
# for (var in seq_along(1:length(names(dat_train %>% select(-Jaccard))))){
#     pred_var <- names(dat_train %>% select(-Jaccard))[var]
#     pp_list_ranger[[var]] <- pdp::partial(
#         rangerModel, 
#         pred.var = pred_var, 
#         plot = FALSE)
# }
# 
# 
# 
# pp_list_xgb <- replicate(39, list())
# for (var in seq_along(1:length(names(dat_train %>% select(-Jaccard))))){
#     pred_var <- names(dat_train %>% select(-Jaccard))[var]
#     pp_list_xgb[[var]] <- partial(
#         xgbModel, 
#         pred.var = pred_var, 
#         plot = FALSE)
# }
# 
# 
# 
# 
# pp_list_gbm <- replicate(39, list())
# for (var in seq_along(1:length(names(dat_train %>% select(-Jaccard))))){
#     pred_var <- names(dat_train %>% select(-Jaccard))[var]
#     pp_list_gbm[[var]] <- partial(
#         gbmModel, 
#         pred.var = pred_var, 
#         plot = FALSE)
# }
# 
# 
# pp_list <- list(pp_list_ranger, pp_list_xgb, pp_list_gbm)
#
# saveRDS(pp_list, "data/pp_list.rds")

pp_list <- readRDS("./data/pp_list.rds")
pp_list_ranger <- pp_list[[1]]
pp_list_xgb <- pp_list[[2]]
pp_list_gbm <- pp_list[[3]]


## Evaluate model =====

# Plotting

# Define the predictor names
predictors <- names(dat_train %>% select(-Jaccard))

# Initialize an empty list to store the plots
plots <- list()

# Function to determine if a column is categorical
is_categorical <- function(column) {
  is.factor(column) || is.character(column)
}

# Loop through the indices and create plots
for (i in seq_along(1:length(predictors))) {
  predictor <- names(pp_list_ranger[[i]])[1]
  
  if (is_categorical(dat_train[[predictor]])) {
    # Create boxplot for categorical predictors
    plots[[i]] <- ggplot() +
      geom_boxplot(data = pp_list_ranger[[i]], aes(x = .data[[predictor]], y = yhat)) +
      geom_boxplot(data = pp_list_xgb[[i]],aes(x = .data[[predictor]], y = yhat), linetype = "dashed") +
      geom_boxplot(data = pp_list_gbm[[i]], aes(x = .data[[predictor]], y = yhat), linetype = "dotted") +
      labs(x = paste(predictor), y = "Partial Dependence", title = "Partial Dependence Boxplots") +
      theme_bw() +
      ylim(0, 1)
  } else {
    # Create line plot for continuous predictors
    plots[[i]] <- ggplot() +
      geom_line(data = pp_list_ranger[[i]], aes(x = .data[[predictor]], y = yhat)) +
      geom_line(data = pp_list_xgb[[i]], aes(x = .data[[predictor]], y = yhat), linetype = "dashed") +
      geom_line(data = pp_list_gbm[[i]], aes(x = .data[[predictor]], y = yhat), linetype = "dotted") +
      labs(x = paste(predictor), y = "Partial Dependence", title = "Partial Dependence Plots") +
      theme_bw() +
      ylim(0, 1)
  }
}

# Arrange the plots in a grid
gridExtra::grid.arrange(grobs = plots, ncol = 6)

```

### Ensemble model

Now we will compare randomForest to boosed regression trees or extreme gradient boosting.

```{r}
#| label: ensemble-model


set.seed(42)
trained_control <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 3,
    savePredictions = "final",
    returnResamp = "final",
    verboseIter = TRUE,
    index = indices)

modelsList <- caretList(
    Jaccard ~ .,
    data = dat_train,
    trControl = trained_control,
    methodList = list("gbm", "xgbTree"),
    tuneList = list(
      ranger = caretModelSpec(
        method = "ranger", 
        importance = "permutation",
        scale.permutation.importance = TRUE,
        num.trees = 5000,
        respect.unordered.factors = TRUE,
        oob.error = TRUE, 
        tuneGrid = ranger_grid)))

p <- as.data.frame(predict(modelsList, newdata = dat_test)) %>% 
  cbind(dat_test$Jaccard) %>%
  mutate(
    error_ranger = dat_test$Jaccard-ranger,
    error_gbm = dat_test$Jaccard-gbm,
    error_xgb = dat_test$Jaccard-xgbTree) 
p %>%
  summarise(mean_ranger = mean(error_ranger),
            mean_gbm = mean(error_gbm),
            mean_xgb = mean(error_xgb)) %>% kableExtra::kable() # ranger performs best

xyplot(resamples(modelsList))

# Create the ensemble model
ensembleModel <- caretEnsemble(
    modelsList,
    metric = "Rsquared",
    trControl = trained_control)
summary(ensembleModel)


# The ensemble model is not better than the ranger model alone. In fact, it's a bit worse. We will discard the ensembleModel approach therefore.

imp_ranger <- varImp(modelsList[[1]])$importance %>% as.data.frame() %>% rename("imp_ranger" = "Overall")
imp_gbm <- varImp(modelsList[[2]])$importance %>% as.data.frame()%>% rename("imp_gbm" = "Overall")
imp_xgb <- varImp(modelsList[[3]])$importance %>% as.data.frame()%>% rename("imp_xgb" = "Overall")


imp_ranger$var <- row.names(imp_ranger)
imp_gbm$var <- row.names(imp_gbm)
imp_xgb$var <- row.names(imp_xgb)


imp_merged <- merge(imp_ranger, imp_gbm)
imp_merged_all <- merge(imp_merged, imp_xgb) %>% arrange(desc(imp_ranger))

imp_merged_all %>% kableExtra::kable()


```

### Train Neural Network

We will train 10 sets of neural networks (one for each resample of the data splitting)

```{r}
#| label: train-neural-network
#| include: TRUE
#| eval: FALSE


for (resamp in seq_along(indices)) {
    train_df <- dat_v2[indices[[resamp]], ] # 826 rows
    test_df <- dat_v2[-indices[[resamp]], ] # 204 rows

    nn_fit <- dnn(Jaccard ~ .,
        data = train_df %>% select(Jaccard, AOO, D_AOO_a, IUCN),
        validation = 0.2,
        hidden = c(50L, 50L, 50L, 50L),
        activation = "relu",
        loss = "mse",
        lr = 0.0001,
        epochs = 6000L,
        lr_scheduler = config_lr_scheduler("reduce_on_plateau", patience = 10, factor = 0.5),
        plot = TRUE,
        tuning = config_optimizer("adam"),
        verbose = TRUE,
        bootstrap = 1000
    )


    analyze_training(nn_fit)

    predictions <- predict(nn_fit, test_df)

    plot(test_df$Jaccard, predictions)

    summary(nn_fit)
}


```

## /
