{
  "hash": "82d01f640822a1f99d2ce02dd1aa5eca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Script 2 - Recursive Feature Selection\"\nauthor: \n  - name: \"MSc. Friederike Johanna Rosa Wölke\"\n    orcid: \"0000-0001-9034-4883\"\n    url: \"https://friedarosa.github.io\"\n    email: \"wolke@fzp.czu.cz\"\n    corresponding: true\ndate: \"2023-05-29\"\n---\n\n\n# Recursive feature elimination\n\nI will use recursive feature elimination to reduce the dimensionality of the data by removing variables that do not lead to an increase of model performance when included.\nI will apply this method, because most of my predictors were calculated from the same data and are thus not independent.\nAlthough checking for high correlations in one of the previous steps, any correlations between predictor variables may confuse the model during variation partitioning, as correlations make it impossible to discern which variable explains how much of the variation.\nThis also reduces the probability of overfitting the model to the data as redundant features with correlated noise signals are removed.\n\nThe method being used relies on the `randomForest` package to recursively eliminate one predictor after another from the model, calculate the variable importance, rank these, average the importance across resamples and comparing the fit across models with different subsets of the set of predictors.\nThe workflow is set in the `caret` helper function `rfFuncs()`.\n\n::: panel-tabset\n## Source custom functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls())\n\nsource(\"src/functions.R\")\n```\n:::\n\n\n## MachineLearning packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\npckgs <- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"ggcorrplot\", \n           \"caret\",  \"recipes\",   \"caretEnsemble\", \n           \"randomForest\", \n           \"gridExtra\", \"kableExtra\", \"tidyr\")\n\ninstall_and_load(pckgs)\n```\n:::\n\n\n## Load RData to reduce computing time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load workspace to save computing time:\n## it has: varPart from ranger models\n## recursive feature selection results\n\nload(\"data/RData/02_rfe_full_vs_reduced.RData\")\n```\n:::\n\n:::\n\n### Predictor importance / Recursive Feature Selection\n\nWe will set up a loop that runs through the four response variables that I am investigating.\nThe models from which the variable importance is calculated are run for 5000 trees each across 10 resamples.\nAgain I will be using 10-fold repeated cross-validation with 3 repeats to evaluate the performance of the models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindex_list <- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list <- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\n\nsaved_profiles <- replicate(4, list())\n# names(saved_profiles) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nsave_imp <- replicate(4, list())\n# names(save_imp) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\n\nresponse_list <- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nfor(j in seq_along(1:4)){\n  ## Loop through differet datasets/Analyses\n  indices <- index_list[[j]]\n  dat_train <- dat_train_list[[j]]\n  response <- response_list[[j]] \n  saved_profiles[[j]] <- replicate(4, list())\n  save_imp[[j]] <- replicate(4, list())\n  \n  ## Recursive feature selection:\n  set.seed(42)\n  ctrl <- rfeControl(\n    functions = rfFuncs,\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    returnResamp = \"all\", # we need all resamples\n    verbose = FALSE,\n    index = indices,\n    saveDetails = TRUE)\n\n  ctrl$functions$rank <- rank #adjust rank function\n\n  ## Set variables for recursive feature elimination\n  subsets <- c(1:50) # number of predictors in each run\n  x <- dat_train %>% select(!all_of(response))\n  y <- dat_train %>% pull(response)\n  \n  ## First run:\n  set.seed(42)\n  rfProfile <- rfe(x, y, ntree = 5000, sizes = subsets, rfeControl = ctrl)\n  rfProfile\n    \n  # Most important predictors:\n  imp <- as.data.frame(rfProfile$fit$importance) %>%\n                   round(3) %>%\n                   select(`%IncMSE`) %>% \n                   mutate(var = row.names(.)) %>%\n                   arrange(desc(`%IncMSE`))    \n    \n  saved_profiles[[j]] <- rfProfile\n  save_imp[[j]] <- imp\n}\n\nsaveRDS(saved_profiles, file = \"data/02_rfe_saved_profiles_5000.rds\")\nsave.image(file = \"data/RData/02_rfe_5000.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsaved_profiles[[1]]$bestSubset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 19\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[2]]$bestSubset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[3]]$bestSubset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 38\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[4]]$bestSubset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 33\n```\n\n\n:::\n\n```{.r .cell-code}\nresults <- replicate(4, list())\n    for (i in seq_along(1:4)){\n        resamp_res <- saved_profiles[[i]]\n        res <- slice_min(resamp_res$results, RMSE)\n        results[[i]] <- res\n    }\n\nnames(results) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nrfe_res <- do.call(rbind, results)\nrfe_res$dd <- rownames(rfe_res)\n\n\n# Bar plot: Nr. Vars selected for each analysis \nggplot(data = rfe_res, aes(x = dd, y = Variables)) +\n    geom_col(fill = \"lightgrey\") +\n    geom_point(data = rfe_res %>% group_by(dd) %>% summarize(mean_Variables = mean(Variables)), \n    aes(x = dd, y = mean_Variables), color = \"red\") +\n    theme_classic()+\n    labs(title = \"Number of variables selected per analysis\", y = \"Number of variables selected\", x = \"Analysis\")\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-eval-1.png){width=384}\n:::\n\n```{.r .cell-code}\n# Add mean importance across resamples to results\nsaved_profiles[[1]]$variables <- saved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall)) %>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[2]]$variables <- saved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[3]]$variables <- saved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[4]]$variables <- saved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n```\n:::\n\n\n::: panel-tabset\n## Jaccard\n\nThe following plots show, that the data that can be used to predict Jaccard can be reduced enormously without losing predictive performance.\n\nFor *`Jaccard1`* the\n\n-   reduced model yields: RMSE = 0.1116 and R² = 0.8450,\n\n-   while the full model yields: RMSE = 0.1118 and R² = 0.8455.\n\nshowing even a slightly reduced RMSE compared to the full model.\n\nFor Jaccard 2 the\n\n-   reduced model yields: RMSE = 0.1248 and R² = 0.8077,\n\n-   while the full model yields: RMSE = 0.1273 and R² = 0.8022.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# On best models (best hyper parameters)\nsaved_profiles[[1]]$fit \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(x = x, y = y, ntree = 5000, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 5000\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 0.0125195\n                    % Var explained: 84.32\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[1]]$fit %>% varImp()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      Overall\nmoran                91.33522\nrel_occ_Ncells       73.52737\nAOO                  71.12573\nD_AOO_a              64.34177\ncirc                 70.42473\nrel_relCirc          61.36701\nx_intercept          50.75122\nmean_prob_cooccur    45.22394\nGlobRangeSize_m2     47.23681\nBetaSR_sp            40.10775\nsp_centr_lat         49.11838\nDist_centroid_to_COG 37.57963\nsp_centr_lon         36.88745\nSouthernness         32.69555\nAlphaSR_sp           32.19241\nWesternness          28.18731\nTrophic.Niche        32.41874\nwidthMinRect         25.17650\nHabitat              26.49305\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(saved_profiles[[1]])\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-j-1.png){width=960}\n:::\n\n```{.r .cell-code}\nsaved_profiles[[2]]$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(x = x, y = y, ntree = 5000, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 5000\nNo. of variables tried at each split: 5\n\n          Mean of squared residuals: 0.01538098\n                    % Var explained: 80.63\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[2]]$fit %>% varImp()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       Overall\nmoran                102.00481\nrel_occ_Ncells        92.96592\nAOO                   69.26302\ncirc                  77.51338\nD_AOO_a               62.50982\nrel_relCirc           54.99068\nx_intercept           49.80143\nBetaSR_sp             61.36546\nDist_centroid_to_COG  51.54155\nsp_centr_lat          57.36471\nWesternness           47.74906\nAlphaSR_sp            48.76303\nsp_centr_lon          42.66040\nSouthernness          39.52615\nGlobRangeSize_m2      40.83284\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(saved_profiles[[2]])\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-j-2.png){width=960}\n:::\n\n```{.r .cell-code}\n## Plot the importances\n\ngrid.arrange(ncol=2,\nsaved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[1]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 1\", x = \"Importance\", y = \"Variable\"),\n\n\nsaved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[2]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-j-3.png){width=960}\n:::\n:::\n\n\n## Log Ratio\n\nFor `log ratio of AOO,` we can see that we need more predictors than for `Jaccard` to predict it from the data.\nAs expected before, the model performance is generally low (both R² = 0.147) and the models try to include more information to discern the relationship between predictors and the response that does not capture a big signal from temporal change.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# On best models (best hyper parameters)\nsaved_profiles[[3]]$fit \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(x = x, y = y, ntree = 5000, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 5000\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 0.2725458\n                    % Var explained: 14.17\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[3]]$fit %>% varImp()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                         Overall\nD_AOO_a                53.467034\nAlphaSR_sp             43.856405\nmean_prob_cooccur      42.023773\nrel_occ_Ncells         42.780196\nAOO                    42.377593\nWesternness            40.014980\nSouthernness           35.325639\nMass                   32.166045\nwidthMinRect           32.908904\nrel_relCirc            32.760661\nsp_centr_lon           29.445915\nBetaSR_sp              27.553994\nsp_centr_lat           24.911582\ncirc                   24.646248\nrel_maxDist            25.929813\nrel_lin                27.227265\nrel_elonRatio          25.132525\nminDist_toBorder_centr 25.416581\nmoran                  26.181974\nbearing                25.113030\nx_intercept            21.269034\ndataset                20.681599\nbearingMinRect         23.702513\nDist_centroid_to_COG   21.905751\nHabitat                19.012495\nHand.Wing.Index        15.466506\nIUCN                   13.876409\nelonMinRect            12.003937\nsd_PC1                 12.825717\nTrophic.Niche           7.359268\nGlobRangeSize_m2       10.700723\nPrimary.Lifestyle       9.529687\nHabitat.Density         7.941689\nsd_PC2                  5.152136\nMigration               6.605318\natlas_widthMinRect      6.358681\natlas_bearingMinRect    4.616524\nFP                      3.202217\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(saved_profiles[[3]])\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-lr-1.png){width=960}\n:::\n\n```{.r .cell-code}\nsaved_profiles[[4]]$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(x = x, y = y, ntree = 5000, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 5000\nNo. of variables tried at each split: 11\n\n          Mean of squared residuals: 0.2661927\n                    % Var explained: 14.17\n```\n\n\n:::\n\n```{.r .cell-code}\nsaved_profiles[[4]]$fit %>% varImp()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                         Overall\nMass                   45.115448\nrel_occ_Ncells         43.674567\nD_AOO_a                39.134253\nAOO                    37.233098\nWesternness            36.412832\nrel_lin                32.723516\nsp_centr_lon           31.450716\nbearing                29.763169\nrel_relCirc            29.311984\nAlphaSR_sp             26.790928\nx_intercept            29.585169\nmoran                  28.806598\nwidthMinRect           28.432642\nSouthernness           27.255666\nrel_maxDist            26.508840\nrel_elonRatio          21.958488\nDist_centroid_to_COG   22.487426\nminDist_toBorder_centr 21.648734\nBetaSR_sp              19.452901\nsp_centr_lat           21.197363\ncirc                   19.820529\nbearingMinRect         17.630352\nelonMinRect            21.386879\nGlobRangeSize_m2       15.568014\nsd_PC1                 13.742048\nHabitat                10.761857\nHand.Wing.Index        11.910291\nsd_PC2                 11.296875\nTrophic.Niche          11.374176\ndataset                12.149539\nFP                      8.880384\nIUCN                    8.369908\natlas_bearingMinRect    8.242545\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(saved_profiles[[4]])\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-lr-2.png){width=960}\n:::\n\n```{.r .cell-code}\ngrid.arrange(ncol=2,\nsaved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[3]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 1\", x = \"Importance\", y = \"Variable\"),\n\nsaved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[4]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n\n::: {.cell-output-display}\n![](02_rfe_files/figure-html/rfe-results-boxplot-lr-3.png){width=960}\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nImp_list <- replicate(4, list())\nImp_list[[1]] <- saved_profiles[[1]]$variables %>% \n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\") %>%\n  mutate(model = \"J1\")\nImp_list[[2]] <- saved_profiles[[2]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"J2\")\n\nImp_list[[3]] <- saved_profiles[[3]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR1\")\n\nImp_list[[4]] <- saved_profiles[[4]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR2\")\n\n# Included vars:\nJ1_vars <- saved_profiles[[1]]$fit$importance\nJ2_vars <- saved_profiles[[2]]$fit$importance\nLR1_vars <- saved_profiles[[3]]$fit$importance\nLR2_vars <- saved_profiles[[4]]$fit$importance\n\nImp_df <- do.call(rbind, Imp_list)\nwide <- Imp_df %>%\n  tidyr::pivot_wider(names_from = c(model), \n                     values_from = imp, names_sep = \"_\", \n                     values_fn = mean) %>% \n  arrange(desc(J1)) %>% \n  group_by(var) %>%  \n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\")) %>%\n  mutate(Include_J1 = case_when(var %in% c(row.names(J1_vars)) ~ 1, \n         .default = 0), \n         Include_J2 = case_when(var %in% c(row.names(J2_vars)) ~ 1, \n         .default = 0),\n         Include_LR1 = case_when(var %in% c(row.names(LR1_vars)) ~ 1, \n         .default = 0),\n         Include_LR2 = case_when(var %in% c(row.names(LR2_vars)) ~ 1, \n         .default = 0))\n\nwide %>% write.csv(\"data/csv/02_all_var_imp_5000.csv\")\nwide %>% kableExtra::kable()\n```\n\n::: {.cell-output-display}\n\n\n|var                    |         J1|         J2|        LR1|       LR2|hypo | Include_J1| Include_J2| Include_LR1| Include_LR2|\n|:----------------------|----------:|----------:|----------:|---------:|:----|----------:|----------:|-----------:|-----------:|\n|moran                  | 82.8820210| 84.8191381| 22.1424265| 24.604191|H2   |          1|          1|           1|           1|\n|rel_occ_Ncells         | 70.8652725| 83.9735638| 39.0385763| 38.272210|H2   |          1|          1|           1|           1|\n|AOO                    | 66.4743491| 63.3642598| 37.3632755| 33.766163|H2   |          1|          1|           1|           1|\n|D_AOO_a                | 62.8893578| 59.6703099| 50.6253329| 34.479309|H2   |          1|          1|           1|           1|\n|circ                   | 61.9599030| 63.1757655| 26.0497190| 17.091905|H2   |          1|          1|           1|           1|\n|rel_relCirc            | 56.5495632| 49.1513509| 30.1353236| 26.420131|H2   |          1|          1|           1|           1|\n|x_intercept            | 45.3946346| 44.9922471| 20.1251542| 25.695247|H2   |          1|          1|           1|           1|\n|mean_prob_cooccur      | 41.1078158|         NA| 39.8594797|        NA|H2   |          1|          0|           1|           0|\n|GlobRangeSize_m2       | 34.9742767| 26.5297079|  9.0472291| 13.432265|H1   |          1|          1|           1|           1|\n|sp_centr_lat           | 34.2845946| 37.3041200| 26.2374161| 18.604352|H2   |          1|          1|           1|           1|\n|BetaSR_sp              | 34.1365896| 40.5188540| 27.0775119| 18.650763|H3   |          1|          1|           1|           1|\n|Dist_centroid_to_COG   | 31.6362380| 38.6065431| 18.2630677| 19.590861|H2   |          1|          1|           1|           1|\n|sp_centr_lon           | 29.5053361| 30.9034602| 26.8312540| 27.639437|H2   |          1|          1|           1|           1|\n|Southernness           | 25.9914367| 27.6526078| 31.9572134| 23.865281|H2   |          1|          1|           1|           1|\n|AlphaSR_sp             | 24.7835928| 31.4293611| 41.3503511| 26.141546|H3   |          1|          1|           1|           1|\n|Westernness            | 24.2591336| 32.3956430| 32.8733254| 31.817972|H2   |          1|          1|           1|           1|\n|Trophic.Niche          | 22.6129904| 21.0759365|  8.2593365|  8.666313|H1   |          1|          0|           1|           1|\n|widthMinRect           | 22.1044076| 20.8770370| 30.1256173| 24.599412|H2   |          1|          0|           1|           1|\n|Habitat                | 20.9731215| 24.2638400| 16.2730421| 11.263801|H1   |          1|          0|           1|           1|\n|dataset                | 16.6119794| 16.8023604| 19.6170457|  9.470622|H4   |          0|          0|           1|           1|\n|sd_PC2                 | 15.1902667| 17.3002761|  5.5403415|  9.895485|H1   |          0|          0|           1|           1|\n|rel_maxDist            | 15.1561925| 18.9873656| 25.0670073| 23.590558|H2   |          0|          0|           1|           1|\n|bearingMinRect         | 12.5638852| 12.8551977| 19.0792310| 16.874969|H2   |          0|          0|           1|           1|\n|sd_PC1                 | 12.3681107|  8.1894032| 10.9575858| 12.084903|H1   |          0|          0|           1|           1|\n|minDist_toBorder_centr | 12.2949982| 18.7114889| 22.2163198| 18.690911|H2   |          0|          0|           1|           1|\n|rel_lin                | 11.4411405| 16.9448564| 24.6350665| 27.526147|H2   |          0|          0|           1|           1|\n|rel_elonRatio          | 11.2294263| 14.1807573| 24.7065904| 20.822811|H2   |          0|          0|           1|           1|\n|Mass                   | 11.1457824| 13.4190191| 29.7072242| 39.248570|H1   |          0|          0|           1|           1|\n|Hand.Wing.Index        | 10.4048676| 11.5684847| 13.3822936| 11.264265|H1   |          0|          0|           1|           1|\n|bearing                |  9.9802882| 18.2672334| 20.5966645| 26.434523|H2   |          0|          0|           1|           1|\n|atlas_bearingMinRect   |  8.9047101|  8.5954997|  6.3225686|  6.762056|H4   |          0|          0|           1|           1|\n|atlas_widthMinRect     |  6.3332383|  7.3012182|  6.4013864|  6.075062|H4   |          0|          0|           1|           0|\n|IUCN                   |  5.6639585|  0.6671593| 11.3100986|  5.923484|H1   |          0|          0|           1|           1|\n|Primary.Lifestyle      |  5.1618988|  6.8808857|  8.3712787|  1.415528|H1   |          0|          0|           1|           0|\n|Habitat.Density        |  4.4324581|  6.2460900|  6.8261565|  1.311792|H1   |          0|          0|           1|           0|\n|elonMinRect            |  4.4299425| 12.8025365| 11.6332086| 15.595609|H2   |          0|          0|           1|           1|\n|Migration              |  3.0650051|  4.4710763|  6.2178447|  3.972399|H1   |          0|          0|           1|           0|\n|Trophic.Level          |  2.3325089|  4.0068964|  0.4568504|  5.428407|H1   |          0|          0|           0|           0|\n|FP                     |  0.6754497| -2.5765705|  3.6091049|  8.141638|H1   |          0|          0|           1|           1|\n\n\n:::\n:::\n\n\n\n# Compare reduced and full ranger models\nSince the rfe function works with the randomForest package, we will check if we get similarly better results with ranger and the reduced model.\n::: panel-tabset\n## Jaccard 1\n\n::: {.cell}\n\n```{.r .cell-code}\nJ1_vars <- wide %>% filter(Include_J1 == 1) %>% pull(var)\n\nresponse <- \"Jaccard\"\n\nindices <- indices_J1\n\ndat_train <- dat_train_J1\n\n\n# Define training control ==========================================================\n  trainControl <- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"all\",\n    verboseIter = FALSE,\n    index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_full <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\ntictoc::toc()\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_reduced <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train %>% select(response, all_of(J1_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\n  \ntictoc::toc()\nJ1_full$finalModel\nJ1_full$results\nJ1_reduced$finalModel\nJ1_reduced$results\n```\n:::\n\n\n## Jaccard 2\n\n::: {.cell}\n\n```{.r .cell-code}\nJ2_vars <- wide %>% filter(Include_J2 == 1) %>% pull(var)\nresponse <- \"Jaccard\"\nindices <- indices_J2\ndat_train <- dat_train_J2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(J2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nJ2_full$finalModel\nJ2_full$results\nJ2_reduced$finalModel\nJ2_reduced$results\n```\n:::\n\n\n## Log Ratio 1\n\n::: {.cell}\n\n```{.r .cell-code}\nLR1_vars <- wide %>% filter(Include_LR1 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR1\ndat_train <- dat_train_LR1\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR1_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n\ntictoc::toc()\nLR1_full$finalModel\nLR1_full$results\nLR1_reduced$finalModel\nLR1_reduced$results\n```\n:::\n\n\n\n## Log Ratio 2\n\n::: {.cell}\n\n```{.r .cell-code}\nLR2_vars <- wide %>% filter(Include_LR2 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR2\ndat_train <- dat_train_LR2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n  \n  \n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \ntictoc::toc()\n  \nLR2_full$finalModel\nLR2_full$results\nLR2_reduced$finalModel\nLR2_reduced$results\n```\n:::\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save.image(\"data/RData/02_rfe_full_vs_reduced.RData\")\n# selected_predictors <- list(J1_vars, J2_vars, LR1_vars, LR2_vars)\n# saveRDS(selected_predictors, \"data/rds/selected_predictors_list.rds\")\n```\n:::\n",
    "supporting": [
      "02_rfe_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}