{
  "hash": "0269b240560b6811ccf44f776591aec7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Script 3 - Hyperparameter Tuning\"\nauthor: \n  - name: \"MSc. Friederike Johanna Rosa WÃ¶lke\"\n    orcid: \"0000-0001-9034-4883\"\n    url: \"https://friedarosa.github.io\"\n    email: \"wolke@fzp.czu.cz\"\n    corresponding: true\ndate: \"2023-05-29\"\n---\n\n\n::: panel-tabset\n## Source custom functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls())\nsource(\"src/functions.R\")\n```\n:::\n\n\n## MachineLearning packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\npckgs <- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"caret\", \"caretEnsemble\", \n           \"randomForest\", \"ranger\", \"gbm\", \"xgboost\", \n           \"gridExtra\", \"kableExtra\")\n\n\ninstall_and_load(pckgs)\n```\n:::\n\n\n## Load RData/RDS objects to reduce computing time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load workspace to save computing time:\nload(\"data/RData/03_reduced_hyper_para_tuning_all_models.RData\")\n\n# make list of models for evaluation ========\nsaved_models <- list(Jaccard1 = models_J1, \n                   Jaccard2 = models_J2, \n                   LogRatio1 = models_LR1, \n                   LogRatio2 = models_LR2)\n\n# saveRDS(saved_models, \"data/models/reducedModels/03_List_all_reduced_models.rds\")\n\n# List of variables to keep in each model\nreduced_predictors <- readRDS(\"data/rds/selected_predictors_list.rds\")\n\n\nH1_vars <- c(\n    \"sd_PC1\", \"sd_PC2\", # Climatic Niche Breadth\n    \"GlobRangeSize_m2\", \"IUCN\", \"Mass\", \"Habitat\", \"Habitat.Density\",\n    \"Migration\", \"Trophic.Level\", \"Trophic.Niche\", \"Primary.Lifestyle\",\n    \"FP\", # Phylogenetic Distinctness\n    \"Hand.Wing.Index\") # Measure of dispersal ability\nH2_vars <- c(\n    \"AOO\", \"rel_occ_Ncells\", \"mean_prob_cooccur\", \"D_AOO_a\", \n    \"moran\", \"x_intercept\", \"sp_centr_lon\", \"sp_centr_lat\",\n    \"lengthMinRect\", \"widthMinRect\", \"elonMinRect\", \"bearingMinRect\",\n    \"circ\", \"bearing\", \"Southernness\", \"Westernness\",\n    \"rel_maxDist\", \"rel_ewDist\", \"rel_nsDist\", \"rel_elonRatio\",\n    \"rel_relCirc\", \"rel_circNorm\", \"rel_lin\", \"Dist_centroid_to_COG\",\n    \"maxDist_toBorder_border\", \"maxDist_toBorder_centr\",\n    \"minDist_toBorder_centr\")\nH3_vars <- c(\"GammaSR\", \"AlphaSR_sp\", \"BetaSR_sp\")\nH4_vars <- c(\n    \"dataset\", \"mean_area\", \"Total_area_samp\", \"Total_Ncells_samp\",\n    \"mean_cell_length\", \"atlas_lengthMinRect\", \"atlas_widthMinRect\",\n    \"atlas_elonMinRect\", \"atlas_circ\", \"atlas_bearingMinRect\",\n    \"atlas_bearing\", \"AtlasCOG_long\", \"AtlasCOG_lat\")\n```\n:::\n\n:::\n\n### Individual models\n\n#### Hyperparameter tuning\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nindex_list <- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list <- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\n\nsaved_models <- replicate(4, list())\nnames(saved_models) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\n\nresponse_list <- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nfor(j in c(1:4)){\n  ## Loop through differet datasets/Analyses\n  indices <- index_list[[j]]\n  response <- response_list[[j]] \n  predictors <- rownames(reduced_predictors[[j]])\n  \n  # Subset the data\n  dat_train <- dat_train_list[[j]] %>% \n                    select(any_of(c(response,predictors)))\n\n  \n  # Define training control ==========================================================\n  trainControl <- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"all\",\n    verboseIter = FALSE,\n    index = indices)\n\n  ## Train ranger model ==========================================================\n  set.seed(42)\n  tictoc::tic(\"ranger\")\n  rangerModel_t <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\n  \n  saveRDS(rangerModel_t, paste0(\"data/models/reducedModels/03_reduced_rangerModel_\", response, j, \"all.rds\"))\n  tictoc::toc()\n  \n  # rangerModel_t <- readRDS( paste0(\"data/models/reducedModels/rangerModel_\", response, j, \"all.rds\"))\n\n  ### Model results:\n  p_rangerModel <- plot(rangerModel_t)\n  p_rangerModel\n  rangerModel_t$finalModel\n\n  ## Train xgbTree model ==========================================================\n  xgb_grid <- expand.grid(\n    nrounds = c(1000),\n    eta = c(0.1, 0.3),\n    max_depth = c(2,3, 5),\n    gamma = c(0, 0.01, 0.1),\n    colsample_bytree = 0.6,\n    min_child_weight = 1,\n    subsample =1)\n  \n  tictoc::tic(\"xgb\")\n  set.seed(42)\n  xgbModel_t <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"xgbTree\",\n    trControl = trainControl,\n    tuneGrid = xgb_grid)\n  saveRDS(xgbModel_t, paste0(\"data/models/reducedModels/03_reduced_xgbModel_all_\", response, j, \"TLCUSTOM.rds\"))\n  tictoc::toc()\n  \n  # xgbModel_t <- readRDS(paste0(\"data/models/reducedModels/xgbModel_all_\", response, j, \"TLCUSTOM.rds\"))\n\n  ### Model results:\n  p_xgbModel <- plot(xgbModel_t)\n  p_xgbModel\n  slice_min(xgbModel_t$results, RMSE)\n  slice_max(xgbModel_t$results, Rsquared)\n\n  \n  ## Train gbm model ==========================================================\n  set.seed(42)\n  tictoc::tic(\"gbm\")\n  gbmModel_t <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"gbm\",\n    trControl = trainControl,\n    tuneLength= 20,\n    verbose = FALSE)\n  saveRDS(gbmModel_t, paste0(\"data/models/reducedModels/03_reduced_gbmModel_\", response, j, \"all.rds\"))\n  tictoc::toc()\n\n  ### Model results:\n  summary.gbm(gbmModel_t$finalModel)\n  p_gbmModel <- plot(gbmModel_t)\n  p_gbmModel\n  gbmModel_t$finalModel\n  slice_min(gbmModel_t$results, RMSE)\n  slice_max(gbmModel_t$results, Rsquared)\n  \n  saved_models[[j]] <- list(rangerModel_t, xgbModel_t, gbmModel_t)\n}\n\n# saveRDS(saved_models, \"data/models/reducedModels/03_List_all_reduced_models.rds\")\n# save.image(\"data/RData/03_reduced_hyper_para_tuning.RData\")\n```\n:::\n\n\n#### Model Evaluation\nHere we will extract the model performance, best hyperparameters and variable importances from each model and compare them.\n\n::: panel-tabset\n## Jaccard 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reduced predictor models:\n\nranger_red <- saved_models[[1]][[1]]\nxgb_red <- saved_models[[1]][[2]]\ngbm_red <- saved_models[[1]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      826 \nNumber of independent variables:  36 \nMtry:                             12 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01255428 \nR squared (OOB):                  0.842953 \n```\n\n\n:::\n\n```{.r .cell-code}\nranger_red$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mtry min.node.size  splitrule      RMSE  Rsquared        MAE      RMSESD\n1     2             5   variance 0.1254667 0.8251798 0.09432341 0.007866969\n2     2             5 extratrees 0.1433463 0.8069389 0.11376738 0.007243472\n3     3             5   variance 0.1162028 0.8378857 0.08264697 0.008493842\n4     3             5 extratrees 0.1239683 0.8257686 0.09142955 0.008905706\n5     5             5   variance 0.1127714 0.8431616 0.07726843 0.009281521\n6     5             5 extratrees 0.1158077 0.8377883 0.08107427 0.009436044\n7     7             5   variance 0.1120340 0.8441887 0.07558047 0.009731207\n8     7             5 extratrees 0.1139346 0.8406778 0.07817203 0.009799280\n9     9             5   variance 0.1114101 0.8454293 0.07451086 0.010060776\n10    9             5 extratrees 0.1131202 0.8419704 0.07666131 0.009829115\n11   10             5   variance 0.1113424 0.8454794 0.07416096 0.010251817\n12   10             5 extratrees 0.1127374 0.8427572 0.07612782 0.009882439\n13   12             5   variance 0.1111970 0.8455695 0.07364971 0.010845162\n14   12             5 extratrees 0.1125581 0.8428030 0.07552565 0.010047851\n15   14             5   variance 0.1112450 0.8452995 0.07329932 0.011082041\n16   14             5 extratrees 0.1123174 0.8431734 0.07502352 0.010198308\n17   16             5   variance 0.1114307 0.8446849 0.07299860 0.011160060\n18   16             5 extratrees 0.1122693 0.8431345 0.07470521 0.010309894\n19   18             5   variance 0.1113786 0.8447690 0.07278867 0.011339312\n20   18             5 extratrees 0.1122118 0.8431386 0.07441526 0.010351843\n21   19             5   variance 0.1116185 0.8440440 0.07272567 0.011550396\n22   19             5 extratrees 0.1120336 0.8435511 0.07421197 0.010503697\n23   21             5   variance 0.1117216 0.8437035 0.07259284 0.011623650\n24   21             5 extratrees 0.1122036 0.8429443 0.07407919 0.010530430\n25   23             5   variance 0.1118380 0.8432769 0.07255752 0.011993990\n26   23             5 extratrees 0.1120763 0.8432083 0.07382346 0.010593439\n27   25             5   variance 0.1120119 0.8428018 0.07248863 0.012029803\n28   25             5 extratrees 0.1121203 0.8430174 0.07370156 0.010599123\n29   27             5   variance 0.1121443 0.8423662 0.07247942 0.012261101\n30   27             5 extratrees 0.1121384 0.8429046 0.07356852 0.010666715\n31   28             5   variance 0.1122884 0.8419467 0.07247270 0.012323806\n32   28             5 extratrees 0.1120747 0.8430957 0.07348672 0.010652228\n33   30             5   variance 0.1125005 0.8413136 0.07253342 0.012482113\n34   30             5 extratrees 0.1121711 0.8427283 0.07340361 0.010788097\n35   32             5   variance 0.1126637 0.8408520 0.07256008 0.012454792\n36   32             5 extratrees 0.1123006 0.8423463 0.07337539 0.010748973\n37   34             5   variance 0.1129129 0.8401382 0.07268725 0.012689290\n38   34             5 extratrees 0.1122970 0.8422977 0.07328178 0.010836723\n39   36             5   variance 0.1131537 0.8393838 0.07281461 0.012812062\n40   36             5 extratrees 0.1122648 0.8423172 0.07314710 0.010951207\n   RsquaredSD       MAESD\n1  0.03055050 0.003974652\n2  0.03397330 0.004744941\n3  0.02879728 0.003775940\n4  0.03217047 0.005025291\n5  0.02906748 0.003851168\n6  0.03001990 0.004825038\n7  0.02956819 0.003892900\n8  0.03032316 0.004813523\n9  0.02989661 0.003836167\n10 0.02988290 0.004745772\n11 0.03028848 0.003914733\n12 0.02981438 0.004777269\n13 0.03184243 0.004084179\n14 0.03022429 0.004946745\n15 0.03248639 0.004079712\n16 0.03040113 0.004842249\n17 0.03259931 0.004031980\n18 0.03053069 0.004881325\n19 0.03300013 0.004127697\n20 0.03063005 0.004957603\n21 0.03370734 0.004120164\n22 0.03089294 0.005019679\n23 0.03388870 0.004159916\n24 0.03107748 0.005036566\n25 0.03502004 0.004190612\n26 0.03108341 0.005021662\n27 0.03496946 0.004085399\n28 0.03100310 0.005003647\n29 0.03573363 0.004072790\n30 0.03118679 0.004985610\n31 0.03593341 0.004161666\n32 0.03112147 0.004939953\n33 0.03632342 0.004105860\n34 0.03150875 0.005115531\n35 0.03633219 0.004116852\n36 0.03143579 0.005014301\n37 0.03703642 0.004177651\n38 0.03159180 0.005004110\n39 0.03744262 0.004201171\n40 0.03191157 0.004981865\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(ranger_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## XGB ======\nplot(xgb_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nxgb_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neXtreme Gradient Boosting \n\n826 samples\n 19 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE       \n  0.1  2          0.00   0.1118070  0.8446102  0.07369056\n  0.1  2          0.01   0.1102063  0.8478990  0.07321260\n  0.1  2          0.10   0.1194187  0.8236726  0.08266637\n  0.1  3          0.00   0.1097821  0.8490233  0.07146877\n  0.1  3          0.01   0.1105528  0.8472857  0.07279250\n  0.1  3          0.10   0.1137796  0.8394713  0.07826346\n  0.1  5          0.00   0.1096201  0.8498558  0.07206656\n  0.1  5          0.01   0.1101902  0.8484793  0.07238673\n  0.1  5          0.10   0.1130442  0.8417327  0.07777401\n  0.3  2          0.00   0.1141524  0.8374329  0.07623701\n  0.3  2          0.01   0.1133456  0.8401277  0.07609019\n  0.3  2          0.10   0.1199386  0.8209386  0.08380543\n  0.3  3          0.00   0.1133470  0.8409218  0.07640170\n  0.3  3          0.01   0.1126855  0.8421184  0.07477419\n  0.3  3          0.10   0.1159277  0.8326335  0.07964922\n  0.3  5          0.00   0.1144612  0.8368645  0.07534337\n  0.3  5          0.01   0.1129137  0.8408449  0.07452666\n  0.3  5          0.10   0.1179038  0.8257194  0.07989957\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample\n = 1.\n```\n\n\n:::\n\n```{.r .cell-code}\n## GBM =====\ngbm_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA gradient boosted model with gaussian loss function.\n300 iterations were performed.\nThere were 36 predictors of which 31 had non-zero influence.\n```\n\n\n:::\n\n```{.r .cell-code}\ngbm_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStochastic Gradient Boosting \n\n826 samples\n 19 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE       \n   1                   50     0.1453374  0.7558990  0.11270542\n   1                  100     0.1289298  0.7968217  0.09436793\n   1                  150     0.1227746  0.8130044  0.08692107\n   1                  200     0.1211241  0.8176810  0.08455328\n   1                  250     0.1186376  0.8243122  0.08194397\n   1                  300     0.1175363  0.8275557  0.08088948\n   1                  350     0.1168408  0.8295444  0.08009465\n   1                  400     0.1166267  0.8302205  0.07976559\n   1                  450     0.1165562  0.8302532  0.07977221\n   1                  500     0.1163796  0.8305591  0.07940275\n   1                  550     0.1164298  0.8307084  0.07930831\n   1                  600     0.1166490  0.8301586  0.07916662\n   1                  650     0.1166229  0.8302922  0.07903669\n   1                  700     0.1164948  0.8306774  0.07876904\n   1                  750     0.1162696  0.8312881  0.07880899\n   1                  800     0.1162676  0.8315504  0.07858171\n   1                  850     0.1160615  0.8319353  0.07834910\n   1                  900     0.1163859  0.8311009  0.07839108\n   1                  950     0.1164178  0.8313684  0.07838938\n   1                 1000     0.1164582  0.8313685  0.07819886\n   2                   50     0.1247676  0.8109618  0.09000949\n   2                  100     0.1169673  0.8298555  0.08096768\n   2                  150     0.1155937  0.8330150  0.07900767\n   2                  200     0.1148729  0.8351281  0.07805390\n   2                  250     0.1137115  0.8385979  0.07690696\n   2                  300     0.1139379  0.8378319  0.07673802\n   2                  350     0.1138836  0.8381328  0.07656764\n   2                  400     0.1137209  0.8386412  0.07648715\n   2                  450     0.1139224  0.8382901  0.07645012\n   2                  500     0.1139598  0.8382803  0.07648368\n   2                  550     0.1142135  0.8378188  0.07650678\n   2                  600     0.1141230  0.8380017  0.07648719\n   2                  650     0.1142878  0.8377684  0.07660950\n   2                  700     0.1140709  0.8383336  0.07658041\n   2                  750     0.1139938  0.8385548  0.07644036\n   2                  800     0.1139313  0.8385619  0.07645051\n   2                  850     0.1137421  0.8391433  0.07649776\n   2                  900     0.1137695  0.8390701  0.07654395\n   2                  950     0.1137925  0.8388728  0.07649111\n   2                 1000     0.1140195  0.8385503  0.07666889\n   3                   50     0.1189724  0.8250200  0.08342629\n   3                  100     0.1140092  0.8376412  0.07759369\n   3                  150     0.1128369  0.8411181  0.07582167\n   3                  200     0.1125331  0.8417550  0.07544162\n   3                  250     0.1128486  0.8409744  0.07557854\n   3                  300     0.1122436  0.8426731  0.07525752\n   3                  350     0.1128212  0.8412139  0.07552856\n   3                  400     0.1127797  0.8412309  0.07538995\n   3                  450     0.1129368  0.8408223  0.07554079\n   3                  500     0.1129223  0.8407906  0.07549643\n   3                  550     0.1129996  0.8408311  0.07553183\n   3                  600     0.1131481  0.8404655  0.07567619\n   3                  650     0.1132707  0.8402120  0.07581446\n   3                  700     0.1131610  0.8405827  0.07568361\n   3                  750     0.1129443  0.8412505  0.07561206\n   3                  800     0.1130614  0.8409369  0.07566536\n   3                  850     0.1130207  0.8409924  0.07569383\n   3                  900     0.1131045  0.8407793  0.07578185\n   3                  950     0.1131567  0.8407358  0.07578853\n   3                 1000     0.1132802  0.8402919  0.07602095\n   4                   50     0.1154835  0.8341507  0.07837077\n   4                  100     0.1126572  0.8415089  0.07488959\n   4                  150     0.1116744  0.8445624  0.07379697\n   4                  200     0.1114789  0.8451936  0.07360589\n   4                  250     0.1113986  0.8452521  0.07353219\n   4                  300     0.1110988  0.8458559  0.07376527\n   4                  350     0.1108353  0.8468867  0.07378405\n   4                  400     0.1112453  0.8458057  0.07411103\n   4                  450     0.1114257  0.8455253  0.07424983\n   4                  500     0.1115432  0.8452468  0.07440587\n   4                  550     0.1117424  0.8447426  0.07464034\n   4                  600     0.1116771  0.8450425  0.07458276\n   4                  650     0.1121899  0.8436529  0.07485827\n   4                  700     0.1123517  0.8433128  0.07498572\n   4                  750     0.1122322  0.8436231  0.07488180\n   4                  800     0.1124768  0.8429594  0.07496479\n   4                  850     0.1126783  0.8424707  0.07504317\n   4                  900     0.1128018  0.8421813  0.07516802\n   4                  950     0.1128704  0.8420455  0.07523006\n   4                 1000     0.1128398  0.8421247  0.07532680\n   5                   50     0.1147182  0.8357200  0.07744488\n   5                  100     0.1127834  0.8409669  0.07443970\n   5                  150     0.1127285  0.8411105  0.07432455\n   5                  200     0.1129253  0.8408366  0.07433093\n   5                  250     0.1126308  0.8417328  0.07430120\n   5                  300     0.1124856  0.8421890  0.07400270\n   5                  350     0.1125105  0.8421371  0.07408824\n   5                  400     0.1125924  0.8417729  0.07419449\n   5                  450     0.1126305  0.8418316  0.07424855\n   5                  500     0.1126437  0.8418528  0.07434069\n   5                  550     0.1128031  0.8413877  0.07443978\n   5                  600     0.1129749  0.8410222  0.07464416\n   5                  650     0.1130505  0.8407691  0.07481935\n   5                  700     0.1131943  0.8403461  0.07488762\n   5                  750     0.1132781  0.8401328  0.07502383\n   5                  800     0.1132754  0.8400697  0.07504798\n   5                  850     0.1132677  0.8400960  0.07501872\n   5                  900     0.1134174  0.8397231  0.07510891\n   5                  950     0.1135016  0.8395041  0.07514660\n   5                 1000     0.1136219  0.8392270  0.07523409\n   6                   50     0.1134738  0.8387884  0.07630523\n   6                  100     0.1112866  0.8449952  0.07437753\n   6                  150     0.1116498  0.8441935  0.07451806\n   6                  200     0.1114086  0.8450053  0.07433774\n   6                  250     0.1116904  0.8446003  0.07450528\n   6                  300     0.1116405  0.8447067  0.07460745\n   6                  350     0.1120175  0.8437966  0.07477339\n   6                  400     0.1119667  0.8440214  0.07476552\n   6                  450     0.1121760  0.8434861  0.07499075\n   6                  500     0.1122244  0.8433772  0.07514079\n   6                  550     0.1122506  0.8433603  0.07526066\n   6                  600     0.1122688  0.8433015  0.07528187\n   6                  650     0.1123367  0.8431433  0.07535241\n   6                  700     0.1124893  0.8427805  0.07549024\n   6                  750     0.1124589  0.8428081  0.07553631\n   6                  800     0.1125934  0.8424594  0.07564791\n   6                  850     0.1126293  0.8423557  0.07570012\n   6                  900     0.1127100  0.8421271  0.07570814\n   6                  950     0.1127572  0.8419953  0.07577021\n   6                 1000     0.1127874  0.8419329  0.07580536\n   7                   50     0.1125015  0.8422207  0.07506903\n   7                  100     0.1111397  0.8459996  0.07323311\n   7                  150     0.1111699  0.8460714  0.07309644\n   7                  200     0.1116112  0.8448118  0.07374291\n   7                  250     0.1113905  0.8454904  0.07355970\n   7                  300     0.1115168  0.8449676  0.07370424\n   7                  350     0.1115537  0.8449419  0.07386732\n   7                  400     0.1116880  0.8445208  0.07404607\n   7                  450     0.1118584  0.8441245  0.07424290\n   7                  500     0.1118135  0.8443609  0.07427428\n   7                  550     0.1118715  0.8441660  0.07432583\n   7                  600     0.1120839  0.8436000  0.07449454\n   7                  650     0.1122996  0.8430489  0.07466681\n   7                  700     0.1124466  0.8426628  0.07477854\n   7                  750     0.1124656  0.8426620  0.07484187\n   7                  800     0.1125503  0.8424901  0.07492728\n   7                  850     0.1126420  0.8422935  0.07505970\n   7                  900     0.1127063  0.8421388  0.07511347\n   7                  950     0.1127420  0.8420142  0.07514704\n   7                 1000     0.1127848  0.8419209  0.07519591\n   8                   50     0.1134442  0.8390744  0.07517079\n   8                  100     0.1113542  0.8453119  0.07365406\n   8                  150     0.1111771  0.8462489  0.07358146\n   8                  200     0.1117109  0.8448927  0.07391530\n   8                  250     0.1113348  0.8460525  0.07379929\n   8                  300     0.1114117  0.8460919  0.07380980\n   8                  350     0.1113601  0.8461744  0.07371058\n   8                  400     0.1115123  0.8458106  0.07371669\n   8                  450     0.1115202  0.8457846  0.07375731\n   8                  500     0.1115910  0.8456670  0.07389795\n   8                  550     0.1117072  0.8454298  0.07398178\n   8                  600     0.1117628  0.8452755  0.07405285\n   8                  650     0.1119484  0.8448660  0.07414512\n   8                  700     0.1119255  0.8449389  0.07418292\n   8                  750     0.1118843  0.8450571  0.07421084\n   8                  800     0.1120240  0.8447453  0.07431664\n   8                  850     0.1121129  0.8445211  0.07437079\n   8                  900     0.1121847  0.8443480  0.07447542\n   8                  950     0.1122337  0.8442344  0.07448805\n   8                 1000     0.1122457  0.8442135  0.07452872\n   9                   50     0.1121301  0.8430789  0.07446649\n   9                  100     0.1106031  0.8471191  0.07303017\n   9                  150     0.1113339  0.8455348  0.07312638\n   9                  200     0.1116384  0.8447969  0.07336973\n   9                  250     0.1117298  0.8445624  0.07370333\n   9                  300     0.1114822  0.8453314  0.07377300\n   9                  350     0.1115669  0.8451223  0.07384065\n   9                  400     0.1117214  0.8447831  0.07402333\n   9                  450     0.1117696  0.8446269  0.07406763\n   9                  500     0.1119380  0.8442322  0.07426571\n   9                  550     0.1120356  0.8440328  0.07439376\n   9                  600     0.1121426  0.8437248  0.07449799\n   9                  650     0.1121849  0.8436342  0.07456437\n   9                  700     0.1122436  0.8434723  0.07460770\n   9                  750     0.1123410  0.8432376  0.07471884\n   9                  800     0.1123425  0.8432636  0.07475188\n   9                  850     0.1124113  0.8430882  0.07481762\n   9                  900     0.1124623  0.8429731  0.07483556\n   9                  950     0.1124724  0.8429332  0.07485113\n   9                 1000     0.1124804  0.8429258  0.07486100\n  10                   50     0.1125329  0.8419627  0.07423121\n  10                  100     0.1109795  0.8463898  0.07302537\n  10                  150     0.1107044  0.8469210  0.07302168\n  10                  200     0.1103046  0.8479983  0.07276967\n  10                  250     0.1101989  0.8483237  0.07275326\n  10                  300     0.1099151  0.8491608  0.07268969\n  10                  350     0.1101296  0.8487303  0.07284724\n  10                  400     0.1102292  0.8484347  0.07288301\n  10                  450     0.1102154  0.8484738  0.07294780\n  10                  500     0.1102776  0.8483017  0.07292208\n  10                  550     0.1102967  0.8482732  0.07297980\n  10                  600     0.1103522  0.8481305  0.07304615\n  10                  650     0.1103438  0.8481346  0.07311375\n  10                  700     0.1103962  0.8480408  0.07318450\n  10                  750     0.1105087  0.8477763  0.07327629\n  10                  800     0.1104959  0.8478212  0.07328697\n  10                  850     0.1105327  0.8477226  0.07332758\n  10                  900     0.1105954  0.8475670  0.07337949\n  10                  950     0.1105441  0.8477055  0.07336182\n  10                 1000     0.1105557  0.8476579  0.07337891\n  11                   50     0.1125197  0.8414815  0.07368924\n  11                  100     0.1110939  0.8455271  0.07274269\n  11                  150     0.1112469  0.8452484  0.07289503\n  11                  200     0.1114980  0.8447607  0.07304767\n  11                  250     0.1117493  0.8442900  0.07323927\n  11                  300     0.1120365  0.8434408  0.07349975\n  11                  350     0.1120915  0.8433073  0.07359549\n  11                  400     0.1122045  0.8430514  0.07361647\n  11                  450     0.1121791  0.8430437  0.07369199\n  11                  500     0.1123262  0.8427237  0.07380589\n  11                  550     0.1123061  0.8428324  0.07386247\n  11                  600     0.1124338  0.8425433  0.07397561\n  11                  650     0.1124787  0.8424397  0.07404825\n  11                  700     0.1125376  0.8422566  0.07410800\n  11                  750     0.1125699  0.8422021  0.07416902\n  11                  800     0.1126495  0.8419872  0.07421064\n  11                  850     0.1126935  0.8419189  0.07421886\n  11                  900     0.1127041  0.8418878  0.07422131\n  11                  950     0.1127020  0.8418864  0.07423385\n  11                 1000     0.1127409  0.8418014  0.07428164\n  12                   50     0.1138047  0.8385718  0.07434719\n  12                  100     0.1122265  0.8431903  0.07282290\n  12                  150     0.1122766  0.8429706  0.07286829\n  12                  200     0.1126828  0.8419639  0.07353919\n  12                  250     0.1128720  0.8414933  0.07380936\n  12                  300     0.1125650  0.8423696  0.07390821\n  12                  350     0.1126444  0.8422611  0.07400306\n  12                  400     0.1126037  0.8424018  0.07413544\n  12                  450     0.1126501  0.8422381  0.07424561\n  12                  500     0.1127401  0.8420509  0.07435252\n  12                  550     0.1127314  0.8420666  0.07445691\n  12                  600     0.1127963  0.8419111  0.07454040\n  12                  650     0.1127702  0.8420194  0.07456929\n  12                  700     0.1127896  0.8419794  0.07461111\n  12                  750     0.1128090  0.8419230  0.07463127\n  12                  800     0.1127870  0.8419980  0.07462415\n  12                  850     0.1128296  0.8419052  0.07467478\n  12                  900     0.1128188  0.8419309  0.07467296\n  12                  950     0.1128546  0.8418360  0.07471009\n  12                 1000     0.1128452  0.8418740  0.07472713\n  13                   50     0.1142697  0.8377481  0.07420000\n  13                  100     0.1126579  0.8428432  0.07331850\n  13                  150     0.1130368  0.8420538  0.07380871\n  13                  200     0.1126539  0.8432476  0.07377520\n  13                  250     0.1127526  0.8429539  0.07390364\n  13                  300     0.1129154  0.8426248  0.07411403\n  13                  350     0.1128190  0.8428538  0.07405083\n  13                  400     0.1129205  0.8426879  0.07415125\n  13                  450     0.1128204  0.8428946  0.07418998\n  13                  500     0.1129501  0.8425824  0.07431134\n  13                  550     0.1129461  0.8425239  0.07437962\n  13                  600     0.1129409  0.8425408  0.07443808\n  13                  650     0.1129371  0.8425598  0.07443888\n  13                  700     0.1129351  0.8425863  0.07442023\n  13                  750     0.1130065  0.8424000  0.07443876\n  13                  800     0.1130398  0.8423019  0.07448059\n  13                  850     0.1130509  0.8423064  0.07447182\n  13                  900     0.1130906  0.8421975  0.07450368\n  13                  950     0.1130594  0.8422888  0.07448156\n  13                 1000     0.1130608  0.8422905  0.07448304\n  14                   50     0.1126369  0.8420395  0.07354179\n  14                  100     0.1128960  0.8416422  0.07348016\n  14                  150     0.1123394  0.8433392  0.07318046\n  14                  200     0.1122478  0.8435616  0.07327454\n  14                  250     0.1124516  0.8429707  0.07360471\n  14                  300     0.1127593  0.8421768  0.07385269\n  14                  350     0.1129961  0.8415917  0.07413896\n  14                  400     0.1130062  0.8416126  0.07416940\n  14                  450     0.1130618  0.8414101  0.07432499\n  14                  500     0.1129875  0.8416486  0.07432808\n  14                  550     0.1130492  0.8414819  0.07437773\n  14                  600     0.1130555  0.8414152  0.07443019\n  14                  650     0.1131076  0.8413375  0.07451082\n  14                  700     0.1131065  0.8413504  0.07453191\n  14                  750     0.1130964  0.8413805  0.07455071\n  14                  800     0.1130887  0.8414151  0.07454242\n  14                  850     0.1131360  0.8413176  0.07458052\n  14                  900     0.1131351  0.8413214  0.07459388\n  14                  950     0.1130822  0.8414517  0.07456862\n  14                 1000     0.1131068  0.8413938  0.07459236\n  15                   50     0.1131305  0.8403465  0.07349639\n  15                  100     0.1123321  0.8431827  0.07287891\n  15                  150     0.1118004  0.8446679  0.07282148\n  15                  200     0.1120324  0.8439525  0.07332044\n  15                  250     0.1119005  0.8443481  0.07349359\n  15                  300     0.1116865  0.8449179  0.07357513\n  15                  350     0.1119159  0.8442780  0.07376522\n  15                  400     0.1117553  0.8447825  0.07373375\n  15                  450     0.1117660  0.8448326  0.07389888\n  15                  500     0.1115844  0.8452712  0.07385394\n  15                  550     0.1118022  0.8447157  0.07395732\n  15                  600     0.1118108  0.8447591  0.07399147\n  15                  650     0.1118719  0.8445354  0.07403653\n  15                  700     0.1118839  0.8445297  0.07404660\n  15                  750     0.1118374  0.8446646  0.07403235\n  15                  800     0.1118474  0.8446509  0.07406386\n  15                  850     0.1118260  0.8447102  0.07403923\n  15                  900     0.1117808  0.8448196  0.07401107\n  15                  950     0.1117840  0.8448252  0.07402911\n  15                 1000     0.1117652  0.8448745  0.07401993\n  16                   50     0.1141391  0.8375297  0.07406268\n  16                  100     0.1121129  0.8435288  0.07292953\n  16                  150     0.1121899  0.8435102  0.07338521\n  16                  200     0.1119181  0.8442792  0.07344087\n  16                  250     0.1122253  0.8435785  0.07393778\n  16                  300     0.1122620  0.8434988  0.07417501\n  16                  350     0.1126126  0.8426088  0.07453239\n  16                  400     0.1124789  0.8429325  0.07454633\n  16                  450     0.1125580  0.8426936  0.07469981\n  16                  500     0.1125500  0.8426901  0.07472204\n  16                  550     0.1125751  0.8426911  0.07474228\n  16                  600     0.1125733  0.8426905  0.07477102\n  16                  650     0.1126125  0.8425720  0.07482068\n  16                  700     0.1125686  0.8427024  0.07481747\n  16                  750     0.1126200  0.8425544  0.07483699\n  16                  800     0.1125920  0.8426440  0.07484748\n  16                  850     0.1125509  0.8427445  0.07483909\n  16                  900     0.1125916  0.8426379  0.07487934\n  16                  950     0.1126015  0.8426075  0.07488019\n  16                 1000     0.1125854  0.8426562  0.07487126\n  17                   50     0.1133941  0.8398378  0.07294471\n  17                  100     0.1126347  0.8423077  0.07290355\n  17                  150     0.1137885  0.8392116  0.07402004\n  17                  200     0.1139283  0.8387646  0.07444590\n  17                  250     0.1140582  0.8385842  0.07466001\n  17                  300     0.1141314  0.8384539  0.07476493\n  17                  350     0.1140994  0.8385512  0.07480841\n  17                  400     0.1142548  0.8381093  0.07490541\n  17                  450     0.1141576  0.8383890  0.07490032\n  17                  500     0.1142423  0.8381209  0.07494723\n  17                  550     0.1141920  0.8383328  0.07496546\n  17                  600     0.1141366  0.8384829  0.07491644\n  17                  650     0.1142033  0.8383281  0.07498024\n  17                  700     0.1141768  0.8384235  0.07496827\n  17                  750     0.1142274  0.8383156  0.07502256\n  17                  800     0.1142559  0.8382422  0.07503381\n  17                  850     0.1142619  0.8382555  0.07505778\n  17                  900     0.1143011  0.8381534  0.07507514\n  17                  950     0.1143168  0.8381244  0.07508052\n  17                 1000     0.1143637  0.8380077  0.07509405\n  18                   50     0.1133819  0.8398295  0.07350387\n  18                  100     0.1136137  0.8396218  0.07364100\n  18                  150     0.1130604  0.8411272  0.07365593\n  18                  200     0.1134891  0.8401435  0.07386500\n  18                  250     0.1132350  0.8408405  0.07394343\n  18                  300     0.1133004  0.8406767  0.07406930\n  18                  350     0.1134083  0.8404483  0.07420219\n  18                  400     0.1135125  0.8401787  0.07425763\n  18                  450     0.1133526  0.8406305  0.07421093\n  18                  500     0.1133832  0.8405383  0.07430299\n  18                  550     0.1134025  0.8405343  0.07431838\n  18                  600     0.1134555  0.8403742  0.07435045\n  18                  650     0.1134723  0.8403659  0.07432233\n  18                  700     0.1134564  0.8403773  0.07431042\n  18                  750     0.1134600  0.8403946  0.07434606\n  18                  800     0.1134416  0.8404452  0.07431402\n  18                  850     0.1134453  0.8404420  0.07433191\n  18                  900     0.1134832  0.8403340  0.07433309\n  18                  950     0.1134722  0.8403798  0.07432612\n  18                 1000     0.1134970  0.8403208  0.07432418\n  19                   50     0.1128946  0.8401795  0.07299429\n  19                  100     0.1132095  0.8399601  0.07333296\n  19                  150     0.1130428  0.8403917  0.07356504\n  19                  200     0.1133440  0.8394295  0.07399058\n  19                  250     0.1131280  0.8400183  0.07423971\n  19                  300     0.1136914  0.8386044  0.07454144\n  19                  350     0.1135952  0.8388618  0.07466089\n  19                  400     0.1135656  0.8389527  0.07481595\n  19                  450     0.1136120  0.8388584  0.07485000\n  19                  500     0.1134450  0.8393081  0.07478708\n  19                  550     0.1134875  0.8392416  0.07487141\n  19                  600     0.1135867  0.8390107  0.07495247\n  19                  650     0.1135482  0.8391352  0.07495277\n  19                  700     0.1136328  0.8388962  0.07499157\n  19                  750     0.1135513  0.8391350  0.07498136\n  19                  800     0.1135918  0.8390274  0.07501198\n  19                  850     0.1136301  0.8389561  0.07503862\n  19                  900     0.1136142  0.8390024  0.07502570\n  19                  950     0.1136217  0.8389901  0.07504057\n  19                 1000     0.1136317  0.8389646  0.07505510\n  20                   50     0.1133590  0.8398770  0.07329153\n  20                  100     0.1133497  0.8404356  0.07362630\n  20                  150     0.1133742  0.8406328  0.07369263\n  20                  200     0.1134622  0.8403462  0.07386509\n  20                  250     0.1135787  0.8399785  0.07398267\n  20                  300     0.1134219  0.8404992  0.07388051\n  20                  350     0.1136676  0.8398728  0.07406826\n  20                  400     0.1135035  0.8403044  0.07408810\n  20                  450     0.1135800  0.8401002  0.07409903\n  20                  500     0.1134690  0.8403542  0.07406907\n  20                  550     0.1133482  0.8406803  0.07403952\n  20                  600     0.1133517  0.8406998  0.07407751\n  20                  650     0.1133925  0.8406453  0.07409429\n  20                  700     0.1133773  0.8406790  0.07410704\n  20                  750     0.1133733  0.8406936  0.07411591\n  20                  800     0.1133378  0.8408165  0.07414581\n  20                  850     0.1133412  0.8408601  0.07415678\n  20                  900     0.1132905  0.8409679  0.07411478\n  20                  950     0.1133220  0.8408902  0.07411604\n  20                 1000     0.1133335  0.8408656  0.07411751\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 300, interaction.depth =\n 10, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n\n\n:::\n\n```{.r .cell-code}\nmodels_J1 <- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n## Importance frame (wide format) ===\nimportances_wide <- cbind(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(var_ranger = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  \n  varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_gbm\" = \"Overall\"))  %>% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_J1 <- full_join(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Importance\" = \"rel.inf\") %>% \n    select(variable, model, Importance)) %>% \n  full_join(varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\")) %>%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n```\n\n\n:::\n:::\n\n\n## Jaccard 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Jaccard 2\nranger_red <- saved_models[[2]][[1]]\nxgb_red <- saved_models[[2]][[2]]\ngbm_red <- saved_models[[2]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      823 \nNumber of independent variables:  15 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01544412 \nR squared (OOB):                  0.8057051 \n```\n\n\n:::\n\n```{.r .cell-code}\nranger_red$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mtry min.node.size  splitrule      RMSE  Rsquared        MAE      RMSESD\n1     2             5   variance 0.1260406 0.8056930 0.08926665 0.007312223\n2     2             5 extratrees 0.1289977 0.7973707 0.09216982 0.007723962\n3     3             5   variance 0.1249849 0.8078292 0.08748564 0.007485030\n4     3             5 extratrees 0.1276692 0.8002262 0.09018152 0.007813283\n5     4             5   variance 0.1244352 0.8091078 0.08658932 0.007405179\n6     4             5 extratrees 0.1267373 0.8026290 0.08905400 0.007921564\n7     5             5   variance 0.1242679 0.8094102 0.08621790 0.007355492\n8     5             5 extratrees 0.1263655 0.8034284 0.08842375 0.008066713\n9     6             5   variance 0.1243970 0.8088235 0.08615208 0.007526901\n10    6             5 extratrees 0.1261296 0.8039488 0.08794364 0.008209857\n11    7             5   variance 0.1244752 0.8084954 0.08608531 0.007552705\n12    7             5 extratrees 0.1259829 0.8042480 0.08771178 0.008187244\n13    8             5   variance 0.1245474 0.8081827 0.08612129 0.007602701\n14    8             5 extratrees 0.1257252 0.8049353 0.08736545 0.008345148\n15    9             5   variance 0.1247794 0.8074774 0.08621205 0.007444275\n16    9             5 extratrees 0.1256311 0.8051973 0.08712087 0.008266322\n17   10             5   variance 0.1249122 0.8069866 0.08619433 0.007518964\n18   10             5 extratrees 0.1257293 0.8047977 0.08709033 0.008474355\n19   11             5   variance 0.1252386 0.8059722 0.08641960 0.007562893\n20   11             5 extratrees 0.1256002 0.8051663 0.08693110 0.008463631\n21   12             5   variance 0.1254442 0.8052826 0.08648914 0.007540639\n22   12             5 extratrees 0.1257058 0.8047797 0.08694106 0.008491981\n23   13             5   variance 0.1256391 0.8046830 0.08653946 0.007654048\n24   13             5 extratrees 0.1256432 0.8049140 0.08682399 0.008505333\n25   14             5   variance 0.1258031 0.8041699 0.08661178 0.007615353\n26   14             5 extratrees 0.1257965 0.8043991 0.08681364 0.008568683\n27   15             5   variance 0.1260365 0.8034445 0.08676195 0.007622122\n28   15             5 extratrees 0.1256931 0.8046933 0.08675762 0.008710886\n   RsquaredSD       MAESD\n1  0.02540306 0.005493561\n2  0.02787693 0.004927717\n3  0.02529038 0.005827222\n4  0.02756015 0.005006648\n5  0.02464231 0.005894750\n6  0.02747694 0.005133563\n7  0.02420319 0.005914532\n8  0.02764641 0.005184050\n9  0.02484508 0.006043449\n10 0.02793602 0.005311346\n11 0.02490062 0.006111946\n12 0.02771323 0.005369005\n13 0.02500967 0.006134414\n14 0.02802708 0.005424229\n15 0.02465914 0.005921552\n16 0.02763188 0.005383124\n17 0.02478801 0.005980159\n18 0.02830268 0.005577527\n19 0.02516815 0.006006034\n20 0.02815352 0.005520004\n21 0.02515206 0.005983238\n22 0.02825150 0.005613019\n23 0.02553690 0.005927629\n24 0.02816930 0.005617544\n25 0.02530233 0.005946151\n26 0.02837556 0.005561156\n27 0.02542279 0.005936225\n28 0.02881966 0.005665576\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(ranger_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## XGB ======\nplot(xgb_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nxgb_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neXtreme Gradient Boosting \n\n823 samples\n 15 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 660, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE       \n  0.1  2          0.00   0.1284578  0.7979565  0.08993810\n  0.1  2          0.01   0.1260796  0.8037433  0.08842593\n  0.1  2          0.10   0.1314116  0.7878631  0.09497730\n  0.1  3          0.00   0.1284644  0.7981403  0.08934380\n  0.1  3          0.01   0.1275459  0.7995220  0.08711394\n  0.1  3          0.10   0.1293986  0.7939568  0.09083660\n  0.1  5          0.00   0.1266654  0.8025459  0.08635013\n  0.1  5          0.01   0.1249692  0.8072804  0.08511744\n  0.1  5          0.10   0.1273035  0.8005622  0.08995105\n  0.3  2          0.00   0.1338256  0.7829319  0.09464282\n  0.3  2          0.01   0.1313921  0.7884705  0.09233459\n  0.3  2          0.10   0.1329644  0.7816596  0.09544929\n  0.3  3          0.00   0.1304433  0.7926532  0.09167521\n  0.3  3          0.01   0.1284496  0.7977904  0.08988310\n  0.3  3          0.10   0.1337157  0.7791898  0.09367822\n  0.3  5          0.00   0.1329975  0.7845899  0.09044138\n  0.3  5          0.01   0.1308238  0.7899613  0.08975483\n  0.3  5          0.10   0.1289239  0.7951159  0.09133874\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.01, colsample_bytree = 0.6, min_child_weight = 1\n and subsample = 1.\n```\n\n\n:::\n\n```{.r .cell-code}\n## GBM =====\ngbm_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 15 predictors of which 15 had non-zero influence.\n```\n\n\n:::\n\n```{.r .cell-code}\ngbm_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStochastic Gradient Boosting \n\n823 samples\n 15 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 660, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE       \n   1                   50     0.1584074  0.7086926  0.12246133\n   1                  100     0.1435215  0.7485171  0.10591372\n   1                  150     0.1394734  0.7599662  0.10037732\n   1                  200     0.1384146  0.7634577  0.09872444\n   1                  250     0.1373744  0.7669744  0.09733579\n   1                  300     0.1367024  0.7691750  0.09668674\n   1                  350     0.1368078  0.7688117  0.09669316\n   1                  400     0.1368899  0.7689408  0.09654413\n   1                  450     0.1360716  0.7716790  0.09593482\n   1                  500     0.1362813  0.7710574  0.09585209\n   1                  550     0.1362871  0.7711316  0.09590389\n   1                  600     0.1366642  0.7698342  0.09601912\n   1                  650     0.1367353  0.7700177  0.09584530\n   1                  700     0.1360659  0.7719509  0.09565328\n   1                  750     0.1360073  0.7722783  0.09541158\n   1                  800     0.1354786  0.7737655  0.09516774\n   1                  850     0.1353131  0.7746747  0.09487182\n   1                  900     0.1349485  0.7757720  0.09489295\n   1                  950     0.1351550  0.7750624  0.09504177\n   1                 1000     0.1351889  0.7751391  0.09462931\n   2                   50     0.1412315  0.7560463  0.10319051\n   2                  100     0.1341781  0.7778110  0.09424480\n   2                  150     0.1331899  0.7811748  0.09280546\n   2                  200     0.1320531  0.7850610  0.09153427\n   2                  250     0.1305759  0.7900238  0.09056218\n   2                  300     0.1309629  0.7888887  0.09059028\n   2                  350     0.1311929  0.7881394  0.09090977\n   2                  400     0.1305613  0.7900991  0.09058572\n   2                  450     0.1306261  0.7897671  0.09075583\n   2                  500     0.1307201  0.7898244  0.09079529\n   2                  550     0.1308576  0.7894471  0.09094342\n   2                  600     0.1306390  0.7900256  0.09097633\n   2                  650     0.1301794  0.7914600  0.09081898\n   2                  700     0.1298772  0.7926054  0.09062494\n   2                  750     0.1302201  0.7916673  0.09084508\n   2                  800     0.1301071  0.7920354  0.09101021\n   2                  850     0.1299781  0.7925779  0.09074899\n   2                  900     0.1302216  0.7918534  0.09112141\n   2                  950     0.1299370  0.7927066  0.09111246\n   2                 1000     0.1302079  0.7920151  0.09138006\n   3                   50     0.1354702  0.7737838  0.09583116\n   3                  100     0.1313452  0.7870555  0.09107133\n   3                  150     0.1307724  0.7890376  0.09040923\n   3                  200     0.1300830  0.7913672  0.09001012\n   3                  250     0.1298266  0.7923981  0.09011055\n   3                  300     0.1306013  0.7899641  0.09064603\n   3                  350     0.1305521  0.7902618  0.09042964\n   3                  400     0.1303383  0.7910292  0.09035448\n   3                  450     0.1307514  0.7898099  0.09056579\n   3                  500     0.1304741  0.7907988  0.09052507\n   3                  550     0.1304651  0.7911283  0.09039566\n   3                  600     0.1305976  0.7907643  0.09047638\n   3                  650     0.1305774  0.7907636  0.09057271\n   3                  700     0.1307723  0.7902601  0.09074473\n   3                  750     0.1310882  0.7894701  0.09089443\n   3                  800     0.1310127  0.7897196  0.09107430\n   3                  850     0.1310305  0.7898513  0.09114423\n   3                  900     0.1313108  0.7890661  0.09116317\n   3                  950     0.1313835  0.7889494  0.09124426\n   3                 1000     0.1313361  0.7892466  0.09138830\n   4                   50     0.1319902  0.7849885  0.09238716\n   4                  100     0.1290531  0.7943175  0.08889128\n   4                  150     0.1289522  0.7952563  0.08858471\n   4                  200     0.1289310  0.7954851  0.08843235\n   4                  250     0.1293091  0.7943071  0.08885981\n   4                  300     0.1296344  0.7937031  0.08890821\n   4                  350     0.1297294  0.7931637  0.08903844\n   4                  400     0.1298753  0.7929470  0.08909999\n   4                  450     0.1302956  0.7920786  0.08937468\n   4                  500     0.1300753  0.7927773  0.08936576\n   4                  550     0.1303154  0.7922350  0.08953787\n   4                  600     0.1302035  0.7926392  0.08955912\n   4                  650     0.1308081  0.7908603  0.09010048\n   4                  700     0.1308628  0.7909072  0.09010682\n   4                  750     0.1308826  0.7909193  0.09020055\n   4                  800     0.1313222  0.7897089  0.09049264\n   4                  850     0.1312623  0.7899755  0.09036911\n   4                  900     0.1316414  0.7889157  0.09063450\n   4                  950     0.1316719  0.7889149  0.09068012\n   4                 1000     0.1318779  0.7883700  0.09085566\n   5                   50     0.1312362  0.7868265  0.09155599\n   5                  100     0.1297050  0.7924267  0.08958106\n   5                  150     0.1288361  0.7950959  0.08947081\n   5                  200     0.1290651  0.7948451  0.08901226\n   5                  250     0.1298312  0.7928330  0.08967155\n   5                  300     0.1292395  0.7948574  0.08958048\n   5                  350     0.1294041  0.7945368  0.08987393\n   5                  400     0.1296468  0.7938478  0.09031521\n   5                  450     0.1298362  0.7934427  0.09042180\n   5                  500     0.1296866  0.7940207  0.09026027\n   5                  550     0.1301230  0.7928043  0.09031782\n   5                  600     0.1302336  0.7926273  0.09063277\n   5                  650     0.1303377  0.7924550  0.09052715\n   5                  700     0.1306044  0.7917702  0.09075715\n   5                  750     0.1307128  0.7915818  0.09074932\n   5                  800     0.1310031  0.7907407  0.09095152\n   5                  850     0.1311243  0.7905124  0.09106542\n   5                  900     0.1311272  0.7904726  0.09105686\n   5                  950     0.1311169  0.7906518  0.09101796\n   5                 1000     0.1311131  0.7906268  0.09097333\n   6                   50     0.1305803  0.7892275  0.09119871\n   6                  100     0.1300404  0.7913173  0.08984256\n   6                  150     0.1295650  0.7929390  0.08990302\n   6                  200     0.1300656  0.7918445  0.09002398\n   6                  250     0.1294736  0.7941055  0.08961028\n   6                  300     0.1291066  0.7953429  0.08929145\n   6                  350     0.1294622  0.7945403  0.08988877\n   6                  400     0.1299236  0.7931482  0.09007615\n   6                  450     0.1299937  0.7931098  0.09004024\n   6                  500     0.1303328  0.7922860  0.09032943\n   6                  550     0.1302410  0.7926275  0.09044957\n   6                  600     0.1304352  0.7920596  0.09052825\n   6                  650     0.1302516  0.7928019  0.09038328\n   6                  700     0.1302509  0.7928315  0.09042086\n   6                  750     0.1304720  0.7923310  0.09057977\n   6                  800     0.1303916  0.7925953  0.09054716\n   6                  850     0.1305320  0.7922091  0.09061801\n   6                  900     0.1305726  0.7921536  0.09063236\n   6                  950     0.1306469  0.7920458  0.09078435\n   6                 1000     0.1305765  0.7922752  0.09070045\n   7                   50     0.1307579  0.7888864  0.09004225\n   7                  100     0.1293564  0.7937211  0.08916414\n   7                  150     0.1303358  0.7910600  0.08975257\n   7                  200     0.1305526  0.7903710  0.09004413\n   7                  250     0.1305003  0.7908379  0.09001871\n   7                  300     0.1304427  0.7911130  0.09019726\n   7                  350     0.1306958  0.7907275  0.09039593\n   7                  400     0.1306112  0.7911365  0.09050001\n   7                  450     0.1308759  0.7906046  0.09063660\n   7                  500     0.1310556  0.7901675  0.09069579\n   7                  550     0.1311143  0.7901550  0.09068275\n   7                  600     0.1310974  0.7902719  0.09068295\n   7                  650     0.1312212  0.7900023  0.09084907\n   7                  700     0.1311993  0.7902051  0.09083951\n   7                  750     0.1312898  0.7899968  0.09093972\n   7                  800     0.1313557  0.7898352  0.09098680\n   7                  850     0.1314379  0.7895758  0.09106189\n   7                  900     0.1313992  0.7897153  0.09109252\n   7                  950     0.1315073  0.7894371  0.09115280\n   7                 1000     0.1314459  0.7896975  0.09110312\n   8                   50     0.1290474  0.7943049  0.08918481\n   8                  100     0.1288383  0.7958471  0.08789753\n   8                  150     0.1290646  0.7950620  0.08838693\n   8                  200     0.1294661  0.7939105  0.08840908\n   8                  250     0.1298856  0.7928578  0.08871989\n   8                  300     0.1298379  0.7930958  0.08866591\n   8                  350     0.1302061  0.7922050  0.08896111\n   8                  400     0.1301889  0.7924098  0.08903648\n   8                  450     0.1301035  0.7926759  0.08906734\n   8                  500     0.1302924  0.7922154  0.08923156\n   8                  550     0.1305103  0.7916771  0.08932237\n   8                  600     0.1304275  0.7919268  0.08930586\n   8                  650     0.1305183  0.7916841  0.08930643\n   8                  700     0.1305092  0.7917822  0.08932119\n   8                  750     0.1307274  0.7911322  0.08954350\n   8                  800     0.1307436  0.7911650  0.08958573\n   8                  850     0.1307089  0.7912562  0.08954295\n   8                  900     0.1306806  0.7913698  0.08952019\n   8                  950     0.1307375  0.7912389  0.08954107\n   8                 1000     0.1307916  0.7911372  0.08956973\n   9                   50     0.1280873  0.7971016  0.08969457\n   9                  100     0.1281615  0.7973094  0.08888332\n   9                  150     0.1289845  0.7954264  0.08927496\n   9                  200     0.1295955  0.7937968  0.08988226\n   9                  250     0.1297861  0.7933117  0.09002900\n   9                  300     0.1296351  0.7940924  0.09024541\n   9                  350     0.1298201  0.7937558  0.09054436\n   9                  400     0.1297383  0.7940841  0.09043395\n   9                  450     0.1298392  0.7937550  0.09066053\n   9                  500     0.1300532  0.7932424  0.09087560\n   9                  550     0.1299519  0.7935931  0.09093514\n   9                  600     0.1299662  0.7936721  0.09099519\n   9                  650     0.1300879  0.7933198  0.09109683\n   9                  700     0.1301281  0.7932973  0.09111824\n   9                  750     0.1300880  0.7934240  0.09106746\n   9                  800     0.1299914  0.7937800  0.09105585\n   9                  850     0.1300942  0.7935239  0.09113102\n   9                  900     0.1301089  0.7934939  0.09113778\n   9                  950     0.1301074  0.7935411  0.09114665\n   9                 1000     0.1301709  0.7933976  0.09119489\n  10                   50     0.1294101  0.7935070  0.08919532\n  10                  100     0.1293842  0.7941388  0.08888102\n  10                  150     0.1294039  0.7943717  0.08921797\n  10                  200     0.1290413  0.7956726  0.08899274\n  10                  250     0.1293973  0.7950923  0.08924317\n  10                  300     0.1302726  0.7925906  0.08990092\n  10                  350     0.1305757  0.7918577  0.08997181\n  10                  400     0.1307520  0.7915413  0.08993049\n  10                  450     0.1308591  0.7913457  0.08997757\n  10                  500     0.1308761  0.7912503  0.09008533\n  10                  550     0.1311018  0.7907374  0.09027126\n  10                  600     0.1311056  0.7907181  0.09027360\n  10                  650     0.1312674  0.7902953  0.09033327\n  10                  700     0.1313612  0.7900840  0.09033570\n  10                  750     0.1314093  0.7899434  0.09038508\n  10                  800     0.1314101  0.7899732  0.09041920\n  10                  850     0.1314461  0.7898900  0.09044895\n  10                  900     0.1315016  0.7897552  0.09049643\n  10                  950     0.1315404  0.7896625  0.09051017\n  10                 1000     0.1315276  0.7897137  0.09050945\n  11                   50     0.1298912  0.7912710  0.08873469\n  11                  100     0.1289184  0.7950283  0.08835305\n  11                  150     0.1300495  0.7919703  0.08929061\n  11                  200     0.1303396  0.7913036  0.08963485\n  11                  250     0.1298290  0.7932145  0.08961927\n  11                  300     0.1298668  0.7930711  0.08961964\n  11                  350     0.1300702  0.7927230  0.08989050\n  11                  400     0.1300960  0.7929156  0.08999414\n  11                  450     0.1300060  0.7932126  0.09001372\n  11                  500     0.1301325  0.7929498  0.09020387\n  11                  550     0.1302263  0.7926915  0.09024974\n  11                  600     0.1302208  0.7927429  0.09027466\n  11                  650     0.1302365  0.7927751  0.09023380\n  11                  700     0.1302353  0.7927517  0.09024268\n  11                  750     0.1301756  0.7930190  0.09019921\n  11                  800     0.1301983  0.7929603  0.09020172\n  11                  850     0.1301710  0.7930425  0.09019712\n  11                  900     0.1301582  0.7930907  0.09021776\n  11                  950     0.1301617  0.7930798  0.09022603\n  11                 1000     0.1301266  0.7932013  0.09021028\n  12                   50     0.1312779  0.7877039  0.08992192\n  12                  100     0.1306737  0.7903910  0.08938858\n  12                  150     0.1315707  0.7880055  0.09000626\n  12                  200     0.1308805  0.7903374  0.08987014\n  12                  250     0.1311043  0.7899336  0.09014911\n  12                  300     0.1313764  0.7893201  0.09048457\n  12                  350     0.1315764  0.7887876  0.09069644\n  12                  400     0.1316176  0.7887486  0.09070754\n  12                  450     0.1313914  0.7895788  0.09061801\n  12                  500     0.1313849  0.7896667  0.09062124\n  12                  550     0.1314037  0.7896541  0.09067148\n  12                  600     0.1312638  0.7901187  0.09057093\n  12                  650     0.1311911  0.7903658  0.09062612\n  12                  700     0.1311561  0.7904883  0.09067743\n  12                  750     0.1312354  0.7902894  0.09070888\n  12                  800     0.1311985  0.7904217  0.09069063\n  12                  850     0.1311593  0.7905380  0.09070009\n  12                  900     0.1311351  0.7906209  0.09070878\n  12                  950     0.1311183  0.7907100  0.09069721\n  12                 1000     0.1311325  0.7906813  0.09071575\n  13                   50     0.1290159  0.7940237  0.08859232\n  13                  100     0.1284857  0.7962826  0.08817150\n  13                  150     0.1288077  0.7959142  0.08847121\n  13                  200     0.1286499  0.7966336  0.08869121\n  13                  250     0.1284464  0.7975734  0.08858228\n  13                  300     0.1285547  0.7973087  0.08892125\n  13                  350     0.1284592  0.7977466  0.08881224\n  13                  400     0.1285081  0.7976546  0.08901085\n  13                  450     0.1285311  0.7975825  0.08902213\n  13                  500     0.1286994  0.7971841  0.08914167\n  13                  550     0.1286856  0.7972548  0.08912285\n  13                  600     0.1287239  0.7971719  0.08913940\n  13                  650     0.1286591  0.7973717  0.08912730\n  13                  700     0.1287349  0.7971281  0.08915678\n  13                  750     0.1287791  0.7970337  0.08916986\n  13                  800     0.1287876  0.7970090  0.08919717\n  13                  850     0.1287659  0.7970721  0.08919024\n  13                  900     0.1287309  0.7971844  0.08915607\n  13                  950     0.1287015  0.7972794  0.08912467\n  13                 1000     0.1287078  0.7972574  0.08914573\n  14                   50     0.1311164  0.7880510  0.08937666\n  14                  100     0.1298100  0.7928814  0.08877579\n  14                  150     0.1302723  0.7918733  0.08957876\n  14                  200     0.1297475  0.7938496  0.08958215\n  14                  250     0.1301249  0.7924913  0.09002031\n  14                  300     0.1302548  0.7922999  0.09033256\n  14                  350     0.1301869  0.7925474  0.09039868\n  14                  400     0.1304242  0.7919742  0.09050790\n  14                  450     0.1303992  0.7921621  0.09053508\n  14                  500     0.1304396  0.7920103  0.09053280\n  14                  550     0.1303686  0.7922361  0.09055737\n  14                  600     0.1303924  0.7921728  0.09055475\n  14                  650     0.1302704  0.7925936  0.09050165\n  14                  700     0.1302372  0.7927129  0.09046533\n  14                  750     0.1301894  0.7928540  0.09046913\n  14                  800     0.1302311  0.7927410  0.09049042\n  14                  850     0.1302218  0.7927599  0.09049122\n  14                  900     0.1301894  0.7928722  0.09046549\n  14                  950     0.1301115  0.7931089  0.09042556\n  14                 1000     0.1301672  0.7929461  0.09044900\n  15                   50     0.1293394  0.7932176  0.08902379\n  15                  100     0.1293676  0.7939149  0.08910845\n  15                  150     0.1289981  0.7954545  0.08899498\n  15                  200     0.1287252  0.7965825  0.08927223\n  15                  250     0.1287964  0.7966462  0.08926076\n  15                  300     0.1287307  0.7971388  0.08938199\n  15                  350     0.1289106  0.7965844  0.08960395\n  15                  400     0.1288413  0.7968719  0.08968034\n  15                  450     0.1285493  0.7978364  0.08958763\n  15                  500     0.1284915  0.7980992  0.08956695\n  15                  550     0.1285328  0.7980323  0.08955751\n  15                  600     0.1284838  0.7981427  0.08955447\n  15                  650     0.1284961  0.7981681  0.08952568\n  15                  700     0.1284951  0.7982075  0.08956355\n  15                  750     0.1284428  0.7983794  0.08953797\n  15                  800     0.1284424  0.7983892  0.08955916\n  15                  850     0.1283946  0.7985536  0.08953630\n  15                  900     0.1283703  0.7986165  0.08953146\n  15                  950     0.1283980  0.7985396  0.08954209\n  15                 1000     0.1283362  0.7987338  0.08951513\n  16                   50     0.1283689  0.7966960  0.08753356\n  16                  100     0.1305577  0.7908714  0.08804681\n  16                  150     0.1300207  0.7925528  0.08787229\n  16                  200     0.1295665  0.7945949  0.08807212\n  16                  250     0.1297753  0.7941035  0.08811500\n  16                  300     0.1299268  0.7938731  0.08850874\n  16                  350     0.1301912  0.7931743  0.08884685\n  16                  400     0.1301226  0.7934396  0.08878903\n  16                  450     0.1300845  0.7936367  0.08878791\n  16                  500     0.1301251  0.7935943  0.08876769\n  16                  550     0.1301321  0.7935444  0.08884653\n  16                  600     0.1300905  0.7937549  0.08884831\n  16                  650     0.1300608  0.7938861  0.08883626\n  16                  700     0.1299751  0.7941605  0.08882584\n  16                  750     0.1299608  0.7942125  0.08882290\n  16                  800     0.1299421  0.7942970  0.08881900\n  16                  850     0.1298566  0.7945542  0.08879357\n  16                  900     0.1298904  0.7944596  0.08879694\n  16                  950     0.1298423  0.7946215  0.08877699\n  16                 1000     0.1298347  0.7946693  0.08876285\n  17                   50     0.1299001  0.7920579  0.08804108\n  17                  100     0.1303908  0.7914290  0.08871953\n  17                  150     0.1302473  0.7922894  0.08876135\n  17                  200     0.1310583  0.7903206  0.08943456\n  17                  250     0.1317316  0.7883230  0.08964818\n  17                  300     0.1318872  0.7880527  0.08978061\n  17                  350     0.1321416  0.7874030  0.08994031\n  17                  400     0.1318971  0.7881878  0.08989739\n  17                  450     0.1317146  0.7887874  0.08982716\n  17                  500     0.1318036  0.7885520  0.08986517\n  17                  550     0.1316090  0.7891810  0.08983734\n  17                  600     0.1315521  0.7893552  0.08986816\n  17                  650     0.1314947  0.7895565  0.08982508\n  17                  700     0.1313526  0.7899884  0.08977059\n  17                  750     0.1313675  0.7899868  0.08975431\n  17                  800     0.1312992  0.7901946  0.08972015\n  17                  850     0.1312571  0.7903161  0.08970528\n  17                  900     0.1312500  0.7903422  0.08968051\n  17                  950     0.1312753  0.7902607  0.08967387\n  17                 1000     0.1312726  0.7902832  0.08969394\n  18                   50     0.1288523  0.7950338  0.08779494\n  18                  100     0.1293887  0.7942816  0.08819209\n  18                  150     0.1294105  0.7946735  0.08836900\n  18                  200     0.1291991  0.7955358  0.08860491\n  18                  250     0.1296125  0.7943926  0.08895640\n  18                  300     0.1299678  0.7934253  0.08917967\n  18                  350     0.1297991  0.7941214  0.08927997\n  18                  400     0.1299479  0.7937556  0.08940463\n  18                  450     0.1300402  0.7935270  0.08949518\n  18                  500     0.1299248  0.7938850  0.08947491\n  18                  550     0.1299464  0.7938901  0.08949894\n  18                  600     0.1299581  0.7938742  0.08955784\n  18                  650     0.1298389  0.7942222  0.08952583\n  18                  700     0.1298570  0.7942070  0.08950299\n  18                  750     0.1298009  0.7944071  0.08948176\n  18                  800     0.1298550  0.7942394  0.08952284\n  18                  850     0.1298549  0.7942623  0.08952792\n  18                  900     0.1298579  0.7942758  0.08952034\n  18                  950     0.1297876  0.7944822  0.08949148\n  18                 1000     0.1298257  0.7943640  0.08951648\n  19                   50     0.1291501  0.7935604  0.08843129\n  19                  100     0.1291973  0.7945503  0.08838429\n  19                  150     0.1296598  0.7932487  0.08895094\n  19                  200     0.1298871  0.7929046  0.08926857\n  19                  250     0.1298335  0.7932069  0.08936154\n  19                  300     0.1300422  0.7925868  0.08949688\n  19                  350     0.1296960  0.7938090  0.08953113\n  19                  400     0.1295477  0.7942420  0.08952061\n  19                  450     0.1295322  0.7943277  0.08954136\n  19                  500     0.1295516  0.7943049  0.08963444\n  19                  550     0.1295864  0.7942245  0.08966258\n  19                  600     0.1294248  0.7947457  0.08957554\n  19                  650     0.1293530  0.7949840  0.08956101\n  19                  700     0.1292330  0.7953503  0.08951184\n  19                  750     0.1292615  0.7952788  0.08949811\n  19                  800     0.1292182  0.7954010  0.08945505\n  19                  850     0.1291611  0.7955963  0.08944151\n  19                  900     0.1291467  0.7956260  0.08942938\n  19                  950     0.1291718  0.7955394  0.08944470\n  19                 1000     0.1291472  0.7956384  0.08941426\n  20                   50     0.1288098  0.7947898  0.08878282\n  20                  100     0.1288240  0.7955616  0.08882946\n  20                  150     0.1290725  0.7952653  0.08891402\n  20                  200     0.1295040  0.7940545  0.08921330\n  20                  250     0.1293822  0.7946422  0.08918458\n  20                  300     0.1294966  0.7943708  0.08922025\n  20                  350     0.1298036  0.7934646  0.08950123\n  20                  400     0.1294723  0.7945474  0.08947917\n  20                  450     0.1294167  0.7948002  0.08953956\n  20                  500     0.1292847  0.7952329  0.08949908\n  20                  550     0.1291894  0.7955359  0.08950960\n  20                  600     0.1291469  0.7956658  0.08952481\n  20                  650     0.1290690  0.7959300  0.08950182\n  20                  700     0.1290244  0.7960974  0.08950991\n  20                  750     0.1291098  0.7958526  0.08953859\n  20                  800     0.1290576  0.7960356  0.08951678\n  20                  850     0.1290655  0.7959940  0.08955958\n  20                  900     0.1290418  0.7960687  0.08955726\n  20                  950     0.1290526  0.7960397  0.08956962\n  20                 1000     0.1290149  0.7961653  0.08955349\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 9, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n\n\n:::\n\n```{.r .cell-code}\nmodels_J2 <- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n\n\n## Importance frame (wide format) ===\nimportances_wide <- cbind(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(var_ranger = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  \n  varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_gbm\" = \"Overall\"))  %>% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_J2 <- full_join(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Importance\" = \"rel.inf\") %>% \n    select(variable, model, Importance)) %>% \n  \n  full_join(varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\")) %>%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n```\n\n\n:::\n:::\n\n\n## Log Ratio 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## reduced:\nranger_red <- saved_models[[3]][[1]]\nxgb_red <- saved_models[[3]][[2]]\ngbm_red <- saved_models[[3]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      826 \nNumber of independent variables:  65 \nMtry:                             28 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        extratrees \nNumber of random splits:          1 \nOOB prediction error (MSE):       0.2534179 \nR squared (OOB):                  0.2028804 \n```\n\n\n:::\n\n```{.r .cell-code}\nranger_red$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mtry min.node.size  splitrule      RMSE  Rsquared       MAE     RMSESD\n1     2             5   variance 0.5320747 0.1600207 0.3170141 0.07035462\n2     2             5 extratrees 0.5402015 0.1490651 0.3229499 0.07203334\n3     5             5   variance 0.5313187 0.1566525 0.3149512 0.06979858\n4     5             5 extratrees 0.5283157 0.1691224 0.3144486 0.07256819\n5     8             5   variance 0.5329553 0.1520773 0.3164223 0.06968052\n6     8             5 extratrees 0.5250628 0.1765168 0.3127621 0.07155879\n7    11             5   variance 0.5339281 0.1501889 0.3171846 0.07116278\n8    11             5 extratrees 0.5234105 0.1809372 0.3121375 0.07059536\n9    15             5   variance 0.5347444 0.1490098 0.3184222 0.07112692\n10   15             5 extratrees 0.5219943 0.1847293 0.3114732 0.06978990\n11   18             5   variance 0.5356479 0.1467067 0.3192681 0.07070322\n12   18             5 extratrees 0.5210211 0.1886790 0.3116950 0.06955238\n13   21             5   variance 0.5359480 0.1462804 0.3198207 0.07120191\n14   21             5 extratrees 0.5206855 0.1892880 0.3114805 0.06957676\n15   25             5   variance 0.5372980 0.1430518 0.3210183 0.07213182\n16   25             5 extratrees 0.5207829 0.1891704 0.3115545 0.06936890\n17   28             5   variance 0.5374314 0.1431692 0.3215648 0.07229645\n18   28             5 extratrees 0.5203579 0.1905113 0.3116032 0.06878515\n19   31             5   variance 0.5377223 0.1430242 0.3222687 0.07247417\n20   31             5 extratrees 0.5205012 0.1901582 0.3118028 0.06845785\n21   35             5   variance 0.5383492 0.1419107 0.3230841 0.07346656\n22   35             5 extratrees 0.5205606 0.1898853 0.3120008 0.06806734\n23   38             5   variance 0.5389178 0.1396532 0.3233412 0.07306683\n24   38             5 extratrees 0.5210864 0.1880556 0.3125096 0.06842888\n25   41             5   variance 0.5392749 0.1398874 0.3240600 0.07367190\n26   41             5 extratrees 0.5212167 0.1874728 0.3123936 0.06772508\n27   45             5   variance 0.5398524 0.1389578 0.3247134 0.07422217\n28   45             5 extratrees 0.5211841 0.1881407 0.3125285 0.06812320\n29   48             5   variance 0.5401299 0.1386446 0.3249213 0.07418648\n30   48             5 extratrees 0.5216228 0.1866076 0.3130290 0.06762264\n31   51             5   variance 0.5403064 0.1384746 0.3251314 0.07463733\n32   51             5 extratrees 0.5217024 0.1864329 0.3133378 0.06797208\n33   55             5   variance 0.5405742 0.1380994 0.3258049 0.07458579\n34   55             5 extratrees 0.5216594 0.1865792 0.3131444 0.06794926\n35   58             5   variance 0.5409279 0.1375885 0.3262961 0.07486114\n36   58             5 extratrees 0.5222240 0.1852020 0.3138892 0.06753329\n37   61             5   variance 0.5413735 0.1365139 0.3266171 0.07533145\n38   61             5 extratrees 0.5225788 0.1838167 0.3137455 0.06771464\n39   65             5   variance 0.5416691 0.1362006 0.3270010 0.07544421\n40   65             5 extratrees 0.5225338 0.1841976 0.3139425 0.06753457\n   RsquaredSD      MAESD\n1  0.08090549 0.03564697\n2  0.06543634 0.03343963\n3  0.07378990 0.03636950\n4  0.07240260 0.03330323\n5  0.07090402 0.03675435\n6  0.07292773 0.03345122\n7  0.07393080 0.03752717\n8  0.07443596 0.03303714\n9  0.07457408 0.03763041\n10 0.07523777 0.03349860\n11 0.07239201 0.03738859\n12 0.07703531 0.03353334\n13 0.07342991 0.03756284\n14 0.07928663 0.03366847\n15 0.07340964 0.03807907\n16 0.08042089 0.03383967\n17 0.07265267 0.03811247\n18 0.07908738 0.03368864\n19 0.07287818 0.03831340\n20 0.07982387 0.03372108\n21 0.07360244 0.03866009\n22 0.07924003 0.03378310\n23 0.07100787 0.03854148\n24 0.07838398 0.03415703\n25 0.07316551 0.03885919\n26 0.07637090 0.03364518\n27 0.07425041 0.03901070\n28 0.07881821 0.03431316\n29 0.07416434 0.03912824\n30 0.07663446 0.03412192\n31 0.07487497 0.03908243\n32 0.07788429 0.03418646\n33 0.07442515 0.03921508\n34 0.07798208 0.03426959\n35 0.07538911 0.03903597\n36 0.07797287 0.03445604\n37 0.07476799 0.03924676\n38 0.07755816 0.03420463\n39 0.07503576 0.03953798\n40 0.07726157 0.03428784\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(ranger_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## XGB ======\nplot(xgb_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nxgb_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neXtreme Gradient Boosting \n\n826 samples\n 38 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE      \n  0.1  2          0.00   0.5678779  0.1285944  0.3656795\n  0.1  2          0.01   0.5658114  0.1306091  0.3666543\n  0.1  2          0.10   0.5538973  0.1352698  0.3515384\n  0.1  3          0.00   0.5548438  0.1402920  0.3518576\n  0.1  3          0.01   0.5498611  0.1464288  0.3476347\n  0.1  3          0.10   0.5482858  0.1432473  0.3439321\n  0.1  5          0.00   0.5475174  0.1417823  0.3412254\n  0.1  5          0.01   0.5439006  0.1467551  0.3394792\n  0.1  5          0.10   0.5424288  0.1538734  0.3368182\n  0.3  2          0.00   0.5766738  0.1253950  0.3727036\n  0.3  2          0.01   0.5768103  0.1147682  0.3737651\n  0.3  2          0.10   0.5698549  0.1253031  0.3651133\n  0.3  3          0.00   0.5761356  0.1091040  0.3630615\n  0.3  3          0.01   0.5548719  0.1547236  0.3578578\n  0.3  3          0.10   0.5628699  0.1278281  0.3594727\n  0.3  5          0.00   0.5653773  0.1197111  0.3521747\n  0.3  5          0.01   0.5607586  0.1271971  0.3487048\n  0.3  5          0.10   0.5473352  0.1467605  0.3417180\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.1, colsample_bytree = 0.6, min_child_weight = 1 and\n subsample = 1.\n```\n\n\n:::\n\n```{.r .cell-code}\n## GBM =====\ngbm_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 65 predictors of which 31 had non-zero influence.\n```\n\n\n:::\n\n```{.r .cell-code}\ngbm_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStochastic Gradient Boosting \n\n826 samples\n 38 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared    MAE      \n   1                   50     0.5453209  0.11276686  0.3330431\n   1                  100     0.5411494  0.13314990  0.3350526\n   1                  150     0.5427483  0.13222468  0.3396717\n   1                  200     0.5464531  0.12960125  0.3434847\n   1                  250     0.5481550  0.12903630  0.3465817\n   1                  300     0.5497724  0.12826980  0.3500051\n   1                  350     0.5494769  0.12887435  0.3515606\n   1                  400     0.5563459  0.12222251  0.3570403\n   1                  450     0.5567504  0.12061410  0.3585423\n   1                  500     0.5617986  0.11553853  0.3624929\n   1                  550     0.5637808  0.11299472  0.3635665\n   1                  600     0.5670194  0.10916300  0.3651230\n   1                  650     0.5683427  0.10716338  0.3657090\n   1                  700     0.5708814  0.10468160  0.3683994\n   1                  750     0.5723362  0.10377459  0.3691504\n   1                  800     0.5743099  0.10137179  0.3707726\n   1                  850     0.5763730  0.09855171  0.3720541\n   1                  900     0.5776585  0.10028188  0.3736275\n   1                  950     0.5789854  0.09811917  0.3747662\n   1                 1000     0.5802115  0.10014314  0.3765578\n   2                   50     0.5440630  0.12426624  0.3290501\n   2                  100     0.5499272  0.12212763  0.3388875\n   2                  150     0.5565498  0.11706322  0.3456571\n   2                  200     0.5641916  0.10996731  0.3523291\n   2                  250     0.5674625  0.10787902  0.3561385\n   2                  300     0.5726960  0.10603867  0.3594207\n   2                  350     0.5753277  0.10750726  0.3619415\n   2                  400     0.5776105  0.10460018  0.3646806\n   2                  450     0.5795096  0.10528946  0.3650658\n   2                  500     0.5819660  0.10405460  0.3669300\n   2                  550     0.5844087  0.10107821  0.3694271\n   2                  600     0.5879210  0.09875226  0.3714303\n   2                  650     0.5889186  0.09853639  0.3725080\n   2                  700     0.5915793  0.09583180  0.3742596\n   2                  750     0.5921395  0.09794384  0.3751922\n   2                  800     0.5939875  0.09595940  0.3764275\n   2                  850     0.5952447  0.09487132  0.3771570\n   2                  900     0.5944024  0.09674908  0.3770566\n   2                  950     0.5956077  0.09658741  0.3782954\n   2                 1000     0.5969042  0.09511108  0.3788724\n   3                   50     0.5410392  0.13588204  0.3289230\n   3                  100     0.5533776  0.12680546  0.3414003\n   3                  150     0.5638061  0.11740749  0.3527015\n   3                  200     0.5709343  0.10801164  0.3575787\n   3                  250     0.5735119  0.10786719  0.3602379\n   3                  300     0.5764019  0.11053800  0.3617477\n   3                  350     0.5772037  0.11302853  0.3612185\n   3                  400     0.5818488  0.10710322  0.3635196\n   3                  450     0.5815981  0.10923716  0.3638935\n   3                  500     0.5837401  0.10798408  0.3659239\n   3                  550     0.5851383  0.10917401  0.3672633\n   3                  600     0.5857292  0.10810662  0.3678001\n   3                  650     0.5872661  0.10734404  0.3692470\n   3                  700     0.5880378  0.10739893  0.3703138\n   3                  750     0.5885932  0.10747738  0.3707743\n   3                  800     0.5900401  0.10586809  0.3714613\n   3                  850     0.5905495  0.10561139  0.3718648\n   3                  900     0.5914810  0.10505618  0.3726712\n   3                  950     0.5921080  0.10457601  0.3732081\n   3                 1000     0.5928487  0.10455697  0.3741254\n   4                   50     0.5444219  0.13027568  0.3331173\n   4                  100     0.5576479  0.12247768  0.3452013\n   4                  150     0.5696806  0.10827659  0.3540370\n   4                  200     0.5743478  0.10693725  0.3571289\n   4                  250     0.5804285  0.10262226  0.3612115\n   4                  300     0.5851436  0.09928730  0.3658404\n   4                  350     0.5884409  0.09703926  0.3680604\n   4                  400     0.5892196  0.09815016  0.3687581\n   4                  450     0.5905166  0.09689026  0.3700734\n   4                  500     0.5921847  0.09602372  0.3717528\n   4                  550     0.5943531  0.09485277  0.3741240\n   4                  600     0.5946995  0.09406454  0.3748678\n   4                  650     0.5961893  0.09352154  0.3760616\n   4                  700     0.5970976  0.09274192  0.3767004\n   4                  750     0.5979700  0.09206338  0.3777760\n   4                  800     0.5986542  0.09155883  0.3779312\n   4                  850     0.5991979  0.09109155  0.3781263\n   4                  900     0.6000346  0.09034042  0.3789374\n   4                  950     0.6001051  0.09052130  0.3791347\n   4                 1000     0.6005364  0.09005158  0.3794271\n   5                   50     0.5443950  0.13118248  0.3328844\n   5                  100     0.5622369  0.10984850  0.3469523\n   5                  150     0.5717406  0.10324053  0.3542386\n   5                  200     0.5771477  0.09992301  0.3581685\n   5                  250     0.5808652  0.10021719  0.3619747\n   5                  300     0.5820350  0.10060560  0.3642813\n   5                  350     0.5850374  0.10042490  0.3661130\n   5                  400     0.5870263  0.09888269  0.3677625\n   5                  450     0.5884556  0.09943217  0.3691780\n   5                  500     0.5905174  0.09695466  0.3707680\n   5                  550     0.5909464  0.09736413  0.3718966\n   5                  600     0.5921720  0.09599165  0.3729199\n   5                  650     0.5922132  0.09634844  0.3732672\n   5                  700     0.5924012  0.09634830  0.3737686\n   5                  750     0.5929264  0.09601922  0.3741128\n   5                  800     0.5934031  0.09548468  0.3743741\n   5                  850     0.5935378  0.09535237  0.3745566\n   5                  900     0.5935863  0.09549574  0.3747803\n   5                  950     0.5940690  0.09509605  0.3749366\n   5                 1000     0.5941774  0.09503007  0.3752002\n   6                   50     0.5463812  0.13075363  0.3356204\n   6                  100     0.5635853  0.11427642  0.3489985\n   6                  150     0.5676499  0.11606195  0.3544246\n   6                  200     0.5752242  0.10982679  0.3585567\n   6                  250     0.5749509  0.11177638  0.3597475\n   6                  300     0.5773803  0.11098790  0.3619288\n   6                  350     0.5790786  0.10906313  0.3635821\n   6                  400     0.5808699  0.10811569  0.3653593\n   6                  450     0.5814797  0.10822440  0.3659935\n   6                  500     0.5824142  0.10765385  0.3661235\n   6                  550     0.5832740  0.10643209  0.3670268\n   6                  600     0.5836597  0.10588553  0.3676875\n   6                  650     0.5841380  0.10564793  0.3678690\n   6                  700     0.5849629  0.10456947  0.3687008\n   6                  750     0.5852889  0.10442105  0.3692048\n   6                  800     0.5852480  0.10467103  0.3691186\n   6                  850     0.5853728  0.10461166  0.3693694\n   6                  900     0.5855389  0.10447792  0.3695638\n   6                  950     0.5857330  0.10430570  0.3696827\n   6                 1000     0.5858491  0.10426293  0.3697533\n   7                   50     0.5478909  0.12609556  0.3337780\n   7                  100     0.5595918  0.12107848  0.3484217\n   7                  150     0.5670040  0.11610722  0.3546119\n   7                  200     0.5710763  0.11356550  0.3584137\n   7                  250     0.5738548  0.11102720  0.3603681\n   7                  300     0.5754928  0.11068284  0.3618532\n   7                  350     0.5776298  0.10894878  0.3629420\n   7                  400     0.5784484  0.10831222  0.3635879\n   7                  450     0.5795603  0.10729956  0.3644478\n   7                  500     0.5807213  0.10625747  0.3646954\n   7                  550     0.5811771  0.10591867  0.3647988\n   7                  600     0.5814677  0.10574170  0.3652090\n   7                  650     0.5818583  0.10537949  0.3652956\n   7                  700     0.5820734  0.10530370  0.3656986\n   7                  750     0.5825560  0.10473030  0.3660817\n   7                  800     0.5825511  0.10483975  0.3660106\n   7                  850     0.5827906  0.10454118  0.3662170\n   7                  900     0.5828693  0.10441064  0.3663414\n   7                  950     0.5829886  0.10431509  0.3664187\n   7                 1000     0.5830266  0.10430467  0.3664293\n   8                   50     0.5523564  0.12586920  0.3411786\n   8                  100     0.5690197  0.11122994  0.3559012\n   8                  150     0.5779470  0.10538885  0.3632349\n   8                  200     0.5813751  0.10255625  0.3657659\n   8                  250     0.5846984  0.10043011  0.3677826\n   8                  300     0.5861157  0.09972193  0.3690053\n   8                  350     0.5883513  0.09702352  0.3710379\n   8                  400     0.5895595  0.09649964  0.3719499\n   8                  450     0.5907514  0.09583195  0.3725195\n   8                  500     0.5917408  0.09487975  0.3731959\n   8                  550     0.5919380  0.09444271  0.3733007\n   8                  600     0.5924805  0.09376117  0.3734105\n   8                  650     0.5925623  0.09389444  0.3735469\n   8                  700     0.5930666  0.09347747  0.3740045\n   8                  750     0.5932415  0.09320250  0.3740608\n   8                  800     0.5935403  0.09281725  0.3742281\n   8                  850     0.5936825  0.09274038  0.3743080\n   8                  900     0.5937853  0.09281246  0.3743816\n   8                  950     0.5938222  0.09270083  0.3743452\n   8                 1000     0.5939396  0.09253259  0.3744127\n   9                   50     0.5563687  0.11530772  0.3422639\n   9                  100     0.5692173  0.10327635  0.3519663\n   9                  150     0.5779961  0.09934933  0.3601293\n   9                  200     0.5828063  0.09421369  0.3646538\n   9                  250     0.5873796  0.09061348  0.3676249\n   9                  300     0.5909195  0.08655142  0.3697890\n   9                  350     0.5926151  0.08469021  0.3711098\n   9                  400     0.5935386  0.08352374  0.3722860\n   9                  450     0.5947803  0.08216616  0.3732742\n   9                  500     0.5949191  0.08203437  0.3735599\n   9                  550     0.5951215  0.08193482  0.3737775\n   9                  600     0.5957051  0.08138625  0.3741841\n   9                  650     0.5961081  0.08112482  0.3744294\n   9                  700     0.5962783  0.08110244  0.3745588\n   9                  750     0.5964757  0.08077213  0.3748366\n   9                  800     0.5965272  0.08087575  0.3748803\n   9                  850     0.5966257  0.08081866  0.3749956\n   9                  900     0.5968004  0.08067883  0.3751649\n   9                  950     0.5968574  0.08058286  0.3752239\n   9                 1000     0.5969473  0.08052850  0.3752979\n  10                   50     0.5585276  0.11475001  0.3432607\n  10                  100     0.5739204  0.10760476  0.3561855\n  10                  150     0.5808675  0.10294254  0.3606709\n  10                  200     0.5851172  0.09827250  0.3645676\n  10                  250     0.5876309  0.09793282  0.3658774\n  10                  300     0.5895311  0.09560984  0.3670094\n  10                  350     0.5915264  0.09464707  0.3686008\n  10                  400     0.5923614  0.09398358  0.3692125\n  10                  450     0.5934603  0.09287847  0.3698782\n  10                  500     0.5939678  0.09244280  0.3702435\n  10                  550     0.5945292  0.09175479  0.3707086\n  10                  600     0.5948583  0.09187014  0.3710613\n  10                  650     0.5950586  0.09168525  0.3710830\n  10                  700     0.5951853  0.09144702  0.3711415\n  10                  750     0.5954792  0.09110628  0.3712865\n  10                  800     0.5956088  0.09097161  0.3713197\n  10                  850     0.5957780  0.09071000  0.3713616\n  10                  900     0.5960723  0.09049473  0.3715091\n  10                  950     0.5961418  0.09035351  0.3715490\n  10                 1000     0.5962568  0.09025067  0.3716236\n  11                   50     0.5639931  0.10680744  0.3459628\n  11                  100     0.5772753  0.09402876  0.3542837\n  11                  150     0.5841737  0.09101564  0.3605798\n  11                  200     0.5919958  0.08413469  0.3660693\n  11                  250     0.5949281  0.08155938  0.3684235\n  11                  300     0.5969892  0.07992388  0.3703856\n  11                  350     0.5991508  0.07831475  0.3721080\n  11                  400     0.5991172  0.07903944  0.3719415\n  11                  450     0.6000440  0.07798655  0.3723846\n  11                  500     0.6006022  0.07756201  0.3725514\n  11                  550     0.6009102  0.07719116  0.3727869\n  11                  600     0.6013365  0.07657459  0.3731125\n  11                  650     0.6014612  0.07635450  0.3731959\n  11                  700     0.6019771  0.07596137  0.3734776\n  11                  750     0.6022825  0.07568023  0.3735249\n  11                  800     0.6024224  0.07538872  0.3735851\n  11                  850     0.6026717  0.07505094  0.3736911\n  11                  900     0.6029406  0.07486509  0.3737900\n  11                  950     0.6031083  0.07477807  0.3738129\n  11                 1000     0.6032737  0.07451094  0.3738814\n  12                   50     0.5583360  0.12073746  0.3421563\n  12                  100     0.5745297  0.10755002  0.3563421\n  12                  150     0.5845424  0.10090200  0.3631475\n  12                  200     0.5906087  0.09478600  0.3672328\n  12                  250     0.5928565  0.09500117  0.3690731\n  12                  300     0.5948135  0.09253340  0.3700709\n  12                  350     0.5963781  0.09170245  0.3710345\n  12                  400     0.5975501  0.09062917  0.3716502\n  12                  450     0.5983625  0.09008512  0.3721548\n  12                  500     0.5989921  0.08961780  0.3725404\n  12                  550     0.5988918  0.08949913  0.3725463\n  12                  600     0.5992446  0.08921890  0.3726716\n  12                  650     0.5996385  0.08896780  0.3728582\n  12                  700     0.6001016  0.08852763  0.3730779\n  12                  750     0.6000472  0.08877987  0.3729958\n  12                  800     0.6003254  0.08848207  0.3731727\n  12                  850     0.6005529  0.08831577  0.3732504\n  12                  900     0.6006412  0.08829242  0.3733424\n  12                  950     0.6009422  0.08784738  0.3734805\n  12                 1000     0.6010598  0.08773740  0.3735000\n  13                   50     0.5551587  0.12016745  0.3423942\n  13                  100     0.5707515  0.11244470  0.3559943\n  13                  150     0.5785372  0.10390353  0.3624828\n  13                  200     0.5811777  0.10303882  0.3653982\n  13                  250     0.5840305  0.09926156  0.3664499\n  13                  300     0.5854905  0.09824418  0.3677243\n  13                  350     0.5880952  0.09571832  0.3692982\n  13                  400     0.5894755  0.09398528  0.3701120\n  13                  450     0.5897342  0.09435716  0.3707219\n  13                  500     0.5908199  0.09285631  0.3712537\n  13                  550     0.5915571  0.09221136  0.3715800\n  13                  600     0.5918141  0.09211228  0.3718835\n  13                  650     0.5924270  0.09122910  0.3722037\n  13                  700     0.5927368  0.09103374  0.3723603\n  13                  750     0.5930038  0.09069762  0.3725468\n  13                  800     0.5933188  0.09010738  0.3726363\n  13                  850     0.5934733  0.08995927  0.3727239\n  13                  900     0.5936346  0.08977685  0.3728276\n  13                  950     0.5938233  0.08949740  0.3728881\n  13                 1000     0.5938868  0.08949223  0.3729242\n  14                   50     0.5548040  0.11770271  0.3413240\n  14                  100     0.5673774  0.11344807  0.3538676\n  14                  150     0.5768105  0.10397719  0.3601051\n  14                  200     0.5829904  0.09848051  0.3638240\n  14                  250     0.5864827  0.09681275  0.3664618\n  14                  300     0.5890569  0.09307091  0.3679592\n  14                  350     0.5901545  0.09170456  0.3684894\n  14                  400     0.5919491  0.09040603  0.3695066\n  14                  450     0.5937192  0.08844721  0.3703378\n  14                  500     0.5946707  0.08735185  0.3709686\n  14                  550     0.5952045  0.08670526  0.3711476\n  14                  600     0.5961833  0.08590756  0.3718125\n  14                  650     0.5964806  0.08561896  0.3719604\n  14                  700     0.5968289  0.08510170  0.3720568\n  14                  750     0.5970696  0.08508368  0.3722736\n  14                  800     0.5975294  0.08447190  0.3724847\n  14                  850     0.5979976  0.08428179  0.3727531\n  14                  900     0.5980134  0.08409285  0.3727326\n  14                  950     0.5981583  0.08394068  0.3728827\n  14                 1000     0.5983978  0.08368324  0.3730040\n  15                   50     0.5528125  0.12820457  0.3400189\n  15                  100     0.5685021  0.12037565  0.3522105\n  15                  150     0.5757411  0.11378866  0.3585352\n  15                  200     0.5804401  0.10979712  0.3628368\n  15                  250     0.5835773  0.10721830  0.3650292\n  15                  300     0.5852602  0.10531309  0.3658620\n  15                  350     0.5872909  0.10313677  0.3668085\n  15                  400     0.5879709  0.10226976  0.3675585\n  15                  450     0.5891570  0.10100887  0.3682194\n  15                  500     0.5903144  0.10000720  0.3686548\n  15                  550     0.5908475  0.09963579  0.3688267\n  15                  600     0.5910235  0.09924186  0.3688363\n  15                  650     0.5915252  0.09861472  0.3691664\n  15                  700     0.5921822  0.09795344  0.3694418\n  15                  750     0.5924998  0.09765016  0.3696287\n  15                  800     0.5926619  0.09752606  0.3697795\n  15                  850     0.5930280  0.09687002  0.3699757\n  15                  900     0.5933279  0.09680841  0.3700959\n  15                  950     0.5934832  0.09636745  0.3702184\n  15                 1000     0.5935980  0.09627017  0.3703229\n  16                   50     0.5550912  0.12315054  0.3421371\n  16                  100     0.5683244  0.11382871  0.3536149\n  16                  150     0.5778940  0.10741484  0.3599092\n  16                  200     0.5823046  0.10361431  0.3623247\n  16                  250     0.5855040  0.10094928  0.3646521\n  16                  300     0.5873605  0.10003270  0.3657116\n  16                  350     0.5897280  0.09769164  0.3673586\n  16                  400     0.5908082  0.09638033  0.3677169\n  16                  450     0.5918074  0.09602485  0.3682480\n  16                  500     0.5926390  0.09521776  0.3684173\n  16                  550     0.5937057  0.09376815  0.3689461\n  16                  600     0.5941788  0.09326162  0.3691744\n  16                  650     0.5943592  0.09280916  0.3693076\n  16                  700     0.5947568  0.09225621  0.3695166\n  16                  750     0.5949403  0.09208724  0.3695637\n  16                  800     0.5952754  0.09163793  0.3697149\n  16                  850     0.5956522  0.09114995  0.3698401\n  16                  900     0.5956991  0.09109746  0.3698880\n  16                  950     0.5959031  0.09103893  0.3699368\n  16                 1000     0.5958828  0.09090244  0.3699651\n  17                   50     0.5545557  0.12166054  0.3393389\n  17                  100     0.5692239  0.11046381  0.3537977\n  17                  150     0.5770709  0.10234490  0.3588184\n  17                  200     0.5816371  0.09791505  0.3628589\n  17                  250     0.5833785  0.09829753  0.3642594\n  17                  300     0.5856648  0.09458468  0.3653939\n  17                  350     0.5867969  0.09357249  0.3661471\n  17                  400     0.5876315  0.09277722  0.3666249\n  17                  450     0.5891141  0.09084876  0.3676131\n  17                  500     0.5898168  0.09053690  0.3679031\n  17                  550     0.5904756  0.08992150  0.3681307\n  17                  600     0.5907264  0.09000211  0.3682468\n  17                  650     0.5911765  0.08959616  0.3685972\n  17                  700     0.5912397  0.08953562  0.3685547\n  17                  750     0.5917070  0.08906404  0.3687844\n  17                  800     0.5915780  0.08911650  0.3687076\n  17                  850     0.5917135  0.08895452  0.3688157\n  17                  900     0.5922291  0.08854989  0.3690603\n  17                  950     0.5923325  0.08842640  0.3691157\n  17                 1000     0.5924475  0.08826513  0.3691790\n  18                   50     0.5505142  0.12568496  0.3435025\n  18                  100     0.5673477  0.11190458  0.3553793\n  18                  150     0.5757013  0.10236183  0.3615161\n  18                  200     0.5803042  0.09939531  0.3651018\n  18                  250     0.5819240  0.09933722  0.3662938\n  18                  300     0.5839017  0.09807934  0.3677958\n  18                  350     0.5856681  0.09630623  0.3687657\n  18                  400     0.5856128  0.09652281  0.3688245\n  18                  450     0.5867742  0.09519598  0.3693696\n  18                  500     0.5879574  0.09488534  0.3698572\n  18                  550     0.5891588  0.09384420  0.3701872\n  18                  600     0.5894770  0.09314256  0.3704683\n  18                  650     0.5898895  0.09275259  0.3707015\n  18                  700     0.5904731  0.09249504  0.3709105\n  18                  750     0.5906551  0.09196746  0.3710118\n  18                  800     0.5909491  0.09158392  0.3712122\n  18                  850     0.5912767  0.09135489  0.3713378\n  18                  900     0.5915499  0.09106990  0.3714766\n  18                  950     0.5916282  0.09081519  0.3714679\n  18                 1000     0.5916689  0.09075198  0.3715015\n  19                   50     0.5568388  0.12501886  0.3440201\n  19                  100     0.5772961  0.10421439  0.3587060\n  19                  150     0.5849819  0.09890322  0.3638665\n  19                  200     0.5897435  0.09563993  0.3677138\n  19                  250     0.5931317  0.09362189  0.3692032\n  19                  300     0.5955103  0.09370107  0.3705583\n  19                  350     0.5971513  0.09233041  0.3716963\n  19                  400     0.5984851  0.09138619  0.3723932\n  19                  450     0.5992056  0.09095749  0.3728553\n  19                  500     0.6006882  0.08975024  0.3737571\n  19                  550     0.6015137  0.08909809  0.3741917\n  19                  600     0.6019959  0.08847557  0.3745817\n  19                  650     0.6024821  0.08843409  0.3748256\n  19                  700     0.6031734  0.08765087  0.3751474\n  19                  750     0.6034751  0.08731235  0.3754161\n  19                  800     0.6038583  0.08701007  0.3756354\n  19                  850     0.6041995  0.08682277  0.3757715\n  19                  900     0.6043533  0.08660278  0.3759177\n  19                  950     0.6045401  0.08639963  0.3759935\n  19                 1000     0.6048450  0.08626342  0.3761410\n  20                   50     0.5555047  0.12189357  0.3367630\n  20                  100     0.5720407  0.10797435  0.3526250\n  20                  150     0.5769324  0.10461912  0.3572071\n  20                  200     0.5837022  0.09677899  0.3611854\n  20                  250     0.5858576  0.09675706  0.3625016\n  20                  300     0.5897003  0.09442901  0.3644864\n  20                  350     0.5916716  0.09281047  0.3657568\n  20                  400     0.5928146  0.09122657  0.3664630\n  20                  450     0.5942447  0.09000727  0.3673727\n  20                  500     0.5952762  0.08937064  0.3678512\n  20                  550     0.5962510  0.08886451  0.3682580\n  20                  600     0.5972056  0.08792226  0.3688632\n  20                  650     0.5977219  0.08755390  0.3692368\n  20                  700     0.5978585  0.08721149  0.3692640\n  20                  750     0.5984538  0.08678052  0.3695976\n  20                  800     0.5983658  0.08657289  0.3695823\n  20                  850     0.5986457  0.08621178  0.3696953\n  20                  900     0.5988585  0.08604269  0.3698326\n  20                  950     0.5991466  0.08589197  0.3699546\n  20                 1000     0.5993148  0.08577172  0.3699632\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n\n\n:::\n\n```{.r .cell-code}\nmodels_LR1 <- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n\n\n## Importance frame (wide format) ===\nimportance_wide <- cbind(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(var_ranger = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_gbm\" = \"Overall\"))  %>% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_LR1 <- full_join(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Importance\" = \"rel.inf\") %>% \n    select(variable, model, Importance)) %>% \n  full_join(varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\")) %>%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n```\n\n\n:::\n:::\n\n\n## Log Ratio 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## reduced:\nranger_red <- saved_models[[4]][[1]]\nxgb_red <- saved_models[[4]][[2]]\ngbm_red <- saved_models[[4]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      823 \nNumber of independent variables:  55 \nMtry:                             52 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        extratrees \nNumber of random splits:          1 \nOOB prediction error (MSE):       0.260595 \nR squared (OOB):                  0.1607592 \n```\n\n\n:::\n\n```{.r .cell-code}\nranger_red$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mtry min.node.size  splitrule      RMSE  Rsquared       MAE     RMSESD\n1     2             5   variance 0.5394176 0.1827108 0.3306246 0.04327289\n2     2             5 extratrees 0.5523844 0.1492828 0.3349185 0.04477997\n3     4             5   variance 0.5363103 0.1825704 0.3298550 0.04310065\n4     4             5 extratrees 0.5442179 0.1589400 0.3306828 0.04483804\n5     7             5   variance 0.5352169 0.1825738 0.3300774 0.04234869\n6     7             5 extratrees 0.5406468 0.1671234 0.3292856 0.04488829\n7    10             5   variance 0.5351361 0.1822525 0.3302695 0.04206719\n8    10             5 extratrees 0.5391743 0.1707950 0.3288732 0.04492560\n9    13             5   variance 0.5350785 0.1815348 0.3307148 0.04079842\n10   13             5 extratrees 0.5373425 0.1760238 0.3287188 0.04477033\n11   15             5   variance 0.5350091 0.1816557 0.3307483 0.04098157\n12   15             5 extratrees 0.5373931 0.1748776 0.3291373 0.04448990\n13   18             5   variance 0.5351410 0.1810147 0.3312523 0.04113763\n14   18             5 extratrees 0.5362650 0.1789442 0.3288775 0.04440944\n15   21             5   variance 0.5352901 0.1808704 0.3318494 0.04104371\n16   21             5 extratrees 0.5357438 0.1799888 0.3291477 0.04424566\n17   24             5   variance 0.5358828 0.1789333 0.3322976 0.04044356\n18   24             5 extratrees 0.5354245 0.1809650 0.3292783 0.04373709\n19   27             5   variance 0.5361697 0.1778851 0.3326148 0.04012501\n20   27             5 extratrees 0.5350873 0.1816643 0.3292259 0.04383781\n21   29             5   variance 0.5357883 0.1788138 0.3328233 0.03961067\n22   29             5 extratrees 0.5349069 0.1817657 0.3295423 0.04327749\n23   32             5   variance 0.5361444 0.1778585 0.3332305 0.03940203\n24   32             5 extratrees 0.5345948 0.1827137 0.3291906 0.04292078\n25   35             5   variance 0.5361837 0.1778279 0.3332777 0.03939331\n26   35             5 extratrees 0.5347364 0.1823905 0.3294745 0.04321173\n27   38             5   variance 0.5360743 0.1786488 0.3333957 0.03961153\n28   38             5 extratrees 0.5343217 0.1841272 0.3297860 0.04340730\n29   41             5   variance 0.5369598 0.1759463 0.3341374 0.03908748\n30   41             5 extratrees 0.5344115 0.1835345 0.3296798 0.04261963\n31   43             5   variance 0.5363456 0.1775281 0.3339625 0.03935771\n32   43             5 extratrees 0.5340870 0.1844424 0.3298840 0.04273121\n33   46             5   variance 0.5366504 0.1767557 0.3342629 0.03928290\n34   46             5 extratrees 0.5343518 0.1836526 0.3303575 0.04264377\n35   49             5   variance 0.5366978 0.1771860 0.3346697 0.03888073\n36   49             5 extratrees 0.5339151 0.1850448 0.3301933 0.04256041\n37   52             5   variance 0.5359135 0.1793983 0.3342563 0.03911702\n38   52             5 extratrees 0.5335750 0.1858403 0.3301628 0.04240879\n39   55             5   variance 0.5362471 0.1780607 0.3345988 0.03885057\n40   55             5 extratrees 0.5337027 0.1853486 0.3301865 0.04226864\n   RsquaredSD      MAESD\n1  0.07045222 0.02131811\n2  0.05953501 0.02004773\n3  0.07023492 0.02281210\n4  0.05955095 0.01969794\n5  0.07092028 0.02308931\n6  0.06235290 0.02047200\n7  0.07306720 0.02322794\n8  0.06392628 0.02089837\n9  0.07118503 0.02303430\n10 0.06591139 0.02102673\n11 0.07084756 0.02321266\n12 0.06500377 0.02102702\n13 0.07054534 0.02360130\n14 0.06815576 0.02128205\n15 0.07034732 0.02356806\n16 0.06681332 0.02169639\n17 0.06887006 0.02313728\n18 0.06743650 0.02127323\n19 0.06954124 0.02317380\n20 0.06823480 0.02166295\n21 0.06938808 0.02289528\n22 0.06717462 0.02158153\n23 0.06908141 0.02317079\n24 0.06730542 0.02132558\n25 0.06915109 0.02318020\n26 0.06799370 0.02176180\n27 0.07029267 0.02333715\n28 0.07016370 0.02218647\n29 0.06979509 0.02303449\n30 0.06860967 0.02212117\n31 0.06938452 0.02298471\n32 0.06836062 0.02216306\n33 0.06832239 0.02317263\n34 0.06949483 0.02228954\n35 0.07105095 0.02293741\n36 0.07033181 0.02231595\n37 0.07021162 0.02320422\n38 0.06829109 0.02241744\n39 0.06923502 0.02327748\n40 0.06806453 0.02226296\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(ranger_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## XGB ======\nplot(xgb_red)\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nxgb_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neXtreme Gradient Boosting \n\n823 samples\n 33 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 659, 659, 659, 659, 659, 659, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE      \n  0.1  2          0.00   0.5652899  0.1370843  0.3599389\n  0.1  2          0.01   0.5672961  0.1358266  0.3639662\n  0.1  2          0.10   0.5537492  0.1468136  0.3521526\n  0.1  3          0.00   0.5546626  0.1508666  0.3553504\n  0.1  3          0.01   0.5564022  0.1489859  0.3544660\n  0.1  3          0.10   0.5486632  0.1611224  0.3496976\n  0.1  5          0.00   0.5502365  0.1535777  0.3449824\n  0.1  5          0.01   0.5491901  0.1488905  0.3497933\n  0.1  5          0.10   0.5452046  0.1597737  0.3436322\n  0.3  2          0.00   0.5818567  0.1225535  0.3773395\n  0.3  2          0.01   0.5733998  0.1315226  0.3687969\n  0.3  2          0.10   0.5629095  0.1430002  0.3608346\n  0.3  3          0.00   0.5659087  0.1453970  0.3692202\n  0.3  3          0.01   0.5717958  0.1293219  0.3664367\n  0.3  3          0.10   0.5608630  0.1385928  0.3602765\n  0.3  5          0.00   0.5592023  0.1381370  0.3563390\n  0.3  5          0.01   0.5589321  0.1422411  0.3560095\n  0.3  5          0.10   0.5624472  0.1341673  0.3527788\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.1, colsample_bytree = 0.6, min_child_weight = 1 and\n subsample = 1.\n```\n\n\n:::\n\n```{.r .cell-code}\n## GBM =====\ngbm_red$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 55 predictors of which 32 had non-zero influence.\n```\n\n\n:::\n\n```{.r .cell-code}\ngbm_red\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStochastic Gradient Boosting \n\n823 samples\n 33 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 659, 659, 659, 659, 659, 659, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE      \n   1                   50     0.5435340  0.1725076  0.3401265\n   1                  100     0.5386692  0.1778030  0.3401935\n   1                  150     0.5394407  0.1746568  0.3421430\n   1                  200     0.5423092  0.1661049  0.3456837\n   1                  250     0.5429783  0.1619924  0.3483008\n   1                  300     0.5445351  0.1597740  0.3506808\n   1                  350     0.5461206  0.1592224  0.3536882\n   1                  400     0.5474695  0.1562333  0.3560610\n   1                  450     0.5492929  0.1524881  0.3585873\n   1                  500     0.5482417  0.1566053  0.3596622\n   1                  550     0.5488017  0.1556647  0.3604591\n   1                  600     0.5510617  0.1528386  0.3630761\n   1                  650     0.5510306  0.1526667  0.3643218\n   1                  700     0.5522670  0.1508417  0.3668228\n   1                  750     0.5542436  0.1480501  0.3698860\n   1                  800     0.5546132  0.1481969  0.3700358\n   1                  850     0.5569744  0.1448902  0.3718863\n   1                  900     0.5591735  0.1424688  0.3741230\n   1                  950     0.5615479  0.1388344  0.3757025\n   1                 1000     0.5609431  0.1399510  0.3745292\n   2                   50     0.5420891  0.1619915  0.3410091\n   2                  100     0.5446668  0.1521755  0.3486964\n   2                  150     0.5419179  0.1653923  0.3493598\n   2                  200     0.5475804  0.1569544  0.3549530\n   2                  250     0.5495773  0.1560494  0.3597635\n   2                  300     0.5522220  0.1534553  0.3615395\n   2                  350     0.5571143  0.1450038  0.3645586\n   2                  400     0.5586398  0.1440451  0.3660280\n   2                  450     0.5627815  0.1389216  0.3704415\n   2                  500     0.5652501  0.1359031  0.3726427\n   2                  550     0.5662805  0.1350053  0.3737641\n   2                  600     0.5689036  0.1308502  0.3769858\n   2                  650     0.5710852  0.1284044  0.3786963\n   2                  700     0.5722107  0.1271664  0.3800573\n   2                  750     0.5737294  0.1261726  0.3813939\n   2                  800     0.5747590  0.1256980  0.3822457\n   2                  850     0.5748160  0.1274190  0.3821215\n   2                  900     0.5755890  0.1267643  0.3830680\n   2                  950     0.5764352  0.1253102  0.3834816\n   2                 1000     0.5786441  0.1232617  0.3849067\n   3                   50     0.5404720  0.1657643  0.3396822\n   3                  100     0.5418197  0.1688569  0.3456548\n   3                  150     0.5445996  0.1628527  0.3489217\n   3                  200     0.5451721  0.1677858  0.3530574\n   3                  250     0.5508788  0.1590540  0.3589441\n   3                  300     0.5558333  0.1523450  0.3627655\n   3                  350     0.5587983  0.1472432  0.3655728\n   3                  400     0.5624810  0.1418606  0.3673547\n   3                  450     0.5664457  0.1359185  0.3695412\n   3                  500     0.5694455  0.1324511  0.3723059\n   3                  550     0.5708555  0.1321255  0.3737038\n   3                  600     0.5706920  0.1334693  0.3735465\n   3                  650     0.5722406  0.1318835  0.3751978\n   3                  700     0.5739100  0.1300385  0.3766772\n   3                  750     0.5755412  0.1277036  0.3778382\n   3                  800     0.5765745  0.1262573  0.3784540\n   3                  850     0.5778049  0.1246908  0.3793165\n   3                  900     0.5787741  0.1237804  0.3806852\n   3                  950     0.5787050  0.1243689  0.3808195\n   3                 1000     0.5789757  0.1246378  0.3812826\n   4                   50     0.5392465  0.1690900  0.3398406\n   4                  100     0.5437410  0.1631470  0.3481872\n   4                  150     0.5472492  0.1596970  0.3537895\n   4                  200     0.5522099  0.1517441  0.3599104\n   4                  250     0.5552790  0.1508825  0.3615048\n   4                  300     0.5570733  0.1493878  0.3639653\n   4                  350     0.5607036  0.1449125  0.3664775\n   4                  400     0.5611736  0.1461306  0.3667845\n   4                  450     0.5644857  0.1409842  0.3682662\n   4                  500     0.5657666  0.1400594  0.3695611\n   4                  550     0.5661958  0.1404224  0.3694681\n   4                  600     0.5673929  0.1393933  0.3701647\n   4                  650     0.5675766  0.1393574  0.3699474\n   4                  700     0.5685654  0.1382364  0.3707117\n   4                  750     0.5698115  0.1366572  0.3721179\n   4                  800     0.5706100  0.1361610  0.3726183\n   4                  850     0.5709354  0.1359264  0.3728772\n   4                  900     0.5715999  0.1352643  0.3731675\n   4                  950     0.5725604  0.1334728  0.3736229\n   4                 1000     0.5725949  0.1336090  0.3737679\n   5                   50     0.5517576  0.1418638  0.3492837\n   5                  100     0.5557799  0.1457220  0.3573030\n   5                  150     0.5597292  0.1443120  0.3622145\n   5                  200     0.5647009  0.1382170  0.3676612\n   5                  250     0.5666992  0.1367723  0.3702063\n   5                  300     0.5706818  0.1323741  0.3740871\n   5                  350     0.5737315  0.1288594  0.3766271\n   5                  400     0.5736835  0.1300122  0.3767334\n   5                  450     0.5748104  0.1294181  0.3777935\n   5                  500     0.5753617  0.1291891  0.3777419\n   5                  550     0.5765608  0.1273073  0.3785762\n   5                  600     0.5776865  0.1259324  0.3794708\n   5                  650     0.5783598  0.1249584  0.3796550\n   5                  700     0.5785844  0.1252472  0.3801715\n   5                  750     0.5790990  0.1248671  0.3809436\n   5                  800     0.5797999  0.1240231  0.3813883\n   5                  850     0.5799455  0.1240017  0.3815767\n   5                  900     0.5802683  0.1236322  0.3819464\n   5                  950     0.5805969  0.1230546  0.3822003\n   5                 1000     0.5806948  0.1230670  0.3823632\n   6                   50     0.5462504  0.1561230  0.3452123\n   6                  100     0.5514706  0.1491707  0.3570281\n   6                  150     0.5552894  0.1458182  0.3603644\n   6                  200     0.5600065  0.1400114  0.3642936\n   6                  250     0.5645479  0.1338760  0.3676793\n   6                  300     0.5670276  0.1304356  0.3692870\n   6                  350     0.5672593  0.1310641  0.3704915\n   6                  400     0.5696334  0.1290703  0.3723679\n   6                  450     0.5704897  0.1288122  0.3734561\n   6                  500     0.5706045  0.1290527  0.3737405\n   6                  550     0.5718385  0.1273220  0.3744323\n   6                  600     0.5727764  0.1260565  0.3751280\n   6                  650     0.5732075  0.1256415  0.3757613\n   6                  700     0.5732564  0.1258814  0.3761576\n   6                  750     0.5735114  0.1258230  0.3763305\n   6                  800     0.5732936  0.1262874  0.3762633\n   6                  850     0.5730912  0.1268145  0.3762173\n   6                  900     0.5734541  0.1263761  0.3764389\n   6                  950     0.5734033  0.1266493  0.3764823\n   6                 1000     0.5733968  0.1267272  0.3766440\n   7                   50     0.5378094  0.1752265  0.3441194\n   7                  100     0.5433111  0.1747641  0.3504528\n   7                  150     0.5479397  0.1717937  0.3556577\n   7                  200     0.5523310  0.1657099  0.3596427\n   7                  250     0.5555076  0.1613582  0.3619909\n   7                  300     0.5575027  0.1584420  0.3638399\n   7                  350     0.5591596  0.1568336  0.3651577\n   7                  400     0.5602831  0.1557860  0.3656587\n   7                  450     0.5612796  0.1544388  0.3659612\n   7                  500     0.5619919  0.1532327  0.3666425\n   7                  550     0.5630979  0.1516573  0.3676396\n   7                  600     0.5634200  0.1516095  0.3678125\n   7                  650     0.5638781  0.1510006  0.3681610\n   7                  700     0.5639960  0.1507522  0.3682634\n   7                  750     0.5640180  0.1507833  0.3684074\n   7                  800     0.5641614  0.1506221  0.3684997\n   7                  850     0.5642576  0.1505786  0.3685778\n   7                  900     0.5643929  0.1503901  0.3686600\n   7                  950     0.5644964  0.1502899  0.3687694\n   7                 1000     0.5644640  0.1503958  0.3688119\n   8                   50     0.5465278  0.1556335  0.3486363\n   8                  100     0.5536872  0.1513301  0.3596422\n   8                  150     0.5584303  0.1480592  0.3639517\n   8                  200     0.5633647  0.1405858  0.3672596\n   8                  250     0.5669173  0.1354042  0.3702147\n   8                  300     0.5693604  0.1323792  0.3716203\n   8                  350     0.5703517  0.1316742  0.3729022\n   8                  400     0.5709676  0.1323827  0.3740529\n   8                  450     0.5722419  0.1307272  0.3747663\n   8                  500     0.5732558  0.1298752  0.3752286\n   8                  550     0.5739770  0.1289958  0.3759073\n   8                  600     0.5740893  0.1291631  0.3759533\n   8                  650     0.5740911  0.1295278  0.3760576\n   8                  700     0.5745476  0.1289141  0.3764545\n   8                  750     0.5745546  0.1291928  0.3764068\n   8                  800     0.5744373  0.1295084  0.3761990\n   8                  850     0.5745369  0.1294330  0.3762800\n   8                  900     0.5745762  0.1294067  0.3763718\n   8                  950     0.5746131  0.1294200  0.3764258\n   8                 1000     0.5746798  0.1293376  0.3765098\n   9                   50     0.5397554  0.1768093  0.3470516\n   9                  100     0.5458968  0.1715482  0.3552530\n   9                  150     0.5513230  0.1659488  0.3601858\n   9                  200     0.5559849  0.1594670  0.3634127\n   9                  250     0.5582653  0.1579514  0.3654427\n   9                  300     0.5599125  0.1553335  0.3665832\n   9                  350     0.5610806  0.1543081  0.3682041\n   9                  400     0.5615299  0.1541571  0.3687943\n   9                  450     0.5616859  0.1544125  0.3688973\n   9                  500     0.5620566  0.1538690  0.3693203\n   9                  550     0.5627483  0.1527296  0.3699013\n   9                  600     0.5628900  0.1526717  0.3698823\n   9                  650     0.5631089  0.1524520  0.3700319\n   9                  700     0.5630597  0.1524914  0.3699887\n   9                  750     0.5632000  0.1524935  0.3701253\n   9                  800     0.5632287  0.1524949  0.3700930\n   9                  850     0.5632631  0.1525165  0.3700578\n   9                  900     0.5632745  0.1525503  0.3700584\n   9                  950     0.5633185  0.1524831  0.3701258\n   9                 1000     0.5632658  0.1525950  0.3700872\n  10                   50     0.5479329  0.1595555  0.3473598\n  10                  100     0.5560735  0.1498948  0.3590347\n  10                  150     0.5627648  0.1424792  0.3656957\n  10                  200     0.5670025  0.1367890  0.3694764\n  10                  250     0.5689548  0.1341975  0.3705474\n  10                  300     0.5716682  0.1305974  0.3731400\n  10                  350     0.5723019  0.1305110  0.3739690\n  10                  400     0.5730169  0.1304022  0.3749035\n  10                  450     0.5738092  0.1290531  0.3755303\n  10                  500     0.5740077  0.1292188  0.3758022\n  10                  550     0.5741381  0.1292520  0.3759381\n  10                  600     0.5742190  0.1292827  0.3759729\n  10                  650     0.5743181  0.1291403  0.3760833\n  10                  700     0.5743132  0.1291837  0.3761027\n  10                  750     0.5743356  0.1292262  0.3761364\n  10                  800     0.5744417  0.1290907  0.3762873\n  10                  850     0.5745377  0.1289574  0.3763350\n  10                  900     0.5746054  0.1288473  0.3763628\n  10                  950     0.5746386  0.1288578  0.3764010\n  10                 1000     0.5746380  0.1288680  0.3763941\n  11                   50     0.5441818  0.1606796  0.3482712\n  11                  100     0.5497133  0.1583966  0.3576095\n  11                  150     0.5560228  0.1493726  0.3621946\n  11                  200     0.5596621  0.1454334  0.3645923\n  11                  250     0.5626085  0.1423597  0.3666927\n  11                  300     0.5641731  0.1402532  0.3676519\n  11                  350     0.5652659  0.1396496  0.3690342\n  11                  400     0.5660220  0.1385469  0.3696948\n  11                  450     0.5664049  0.1384240  0.3702493\n  11                  500     0.5668304  0.1377082  0.3705263\n  11                  550     0.5671592  0.1373371  0.3707951\n  11                  600     0.5674809  0.1369477  0.3709850\n  11                  650     0.5674525  0.1370396  0.3709618\n  11                  700     0.5676150  0.1368011  0.3710817\n  11                  750     0.5677128  0.1366422  0.3711927\n  11                  800     0.5676845  0.1368104  0.3712545\n  11                  850     0.5677508  0.1367157  0.3712690\n  11                  900     0.5677400  0.1368046  0.3713302\n  11                  950     0.5677959  0.1367216  0.3713916\n  11                 1000     0.5677918  0.1367871  0.3714061\n  12                   50     0.5486969  0.1557104  0.3514358\n  12                  100     0.5552754  0.1516340  0.3599501\n  12                  150     0.5620314  0.1434373  0.3644974\n  12                  200     0.5675687  0.1349275  0.3682724\n  12                  250     0.5680101  0.1358575  0.3695771\n  12                  300     0.5683923  0.1363312  0.3702252\n  12                  350     0.5698257  0.1346120  0.3713010\n  12                  400     0.5708860  0.1329419  0.3720778\n  12                  450     0.5713183  0.1323765  0.3725445\n  12                  500     0.5716847  0.1318076  0.3726861\n  12                  550     0.5717026  0.1321116  0.3727598\n  12                  600     0.5717443  0.1321932  0.3728891\n  12                  650     0.5719468  0.1319131  0.3730767\n  12                  700     0.5718986  0.1321071  0.3731357\n  12                  750     0.5719118  0.1319763  0.3731030\n  12                  800     0.5720270  0.1318131  0.3732050\n  12                  850     0.5722275  0.1315990  0.3732938\n  12                  900     0.5722825  0.1315665  0.3733747\n  12                  950     0.5722455  0.1316775  0.3734095\n  12                 1000     0.5722097  0.1318034  0.3734055\n  13                   50     0.5402188  0.1720905  0.3485329\n  13                  100     0.5522713  0.1518652  0.3598521\n  13                  150     0.5571034  0.1490147  0.3640914\n  13                  200     0.5591038  0.1476936  0.3670438\n  13                  250     0.5613849  0.1458476  0.3687285\n  13                  300     0.5627262  0.1450748  0.3703304\n  13                  350     0.5637731  0.1439518  0.3713033\n  13                  400     0.5645554  0.1429202  0.3723207\n  13                  450     0.5651612  0.1420776  0.3728375\n  13                  500     0.5655916  0.1415256  0.3731084\n  13                  550     0.5658390  0.1413508  0.3732993\n  13                  600     0.5660754  0.1410674  0.3734960\n  13                  650     0.5661553  0.1412253  0.3736626\n  13                  700     0.5662425  0.1411439  0.3737476\n  13                  750     0.5663400  0.1410830  0.3737879\n  13                  800     0.5663475  0.1411831  0.3738168\n  13                  850     0.5664854  0.1409492  0.3739146\n  13                  900     0.5665471  0.1409752  0.3740156\n  13                  950     0.5665988  0.1409574  0.3740534\n  13                 1000     0.5666573  0.1409706  0.3740807\n  14                   50     0.5466903  0.1593075  0.3485899\n  14                  100     0.5538054  0.1531953  0.3569143\n  14                  150     0.5606127  0.1446332  0.3627351\n  14                  200     0.5640223  0.1412382  0.3657916\n  14                  250     0.5665972  0.1386975  0.3672909\n  14                  300     0.5672575  0.1386501  0.3678960\n  14                  350     0.5679869  0.1381003  0.3685011\n  14                  400     0.5688754  0.1365875  0.3692443\n  14                  450     0.5692982  0.1361094  0.3696157\n  14                  500     0.5696525  0.1356693  0.3700169\n  14                  550     0.5698437  0.1356705  0.3700207\n  14                  600     0.5701022  0.1355239  0.3702518\n  14                  650     0.5701231  0.1357210  0.3702273\n  14                  700     0.5702763  0.1356363  0.3703548\n  14                  750     0.5703191  0.1357390  0.3704201\n  14                  800     0.5704118  0.1357495  0.3705184\n  14                  850     0.5704669  0.1357367  0.3705517\n  14                  900     0.5705359  0.1357101  0.3706261\n  14                  950     0.5705255  0.1357297  0.3706455\n  14                 1000     0.5705374  0.1357559  0.3706775\n  15                   50     0.5428859  0.1723794  0.3474859\n  15                  100     0.5509054  0.1585915  0.3567359\n  15                  150     0.5584114  0.1480524  0.3611537\n  15                  200     0.5619715  0.1451323  0.3638634\n  15                  250     0.5649838  0.1416001  0.3661820\n  15                  300     0.5660400  0.1411948  0.3673904\n  15                  350     0.5669079  0.1403230  0.3683613\n  15                  400     0.5676357  0.1396901  0.3687451\n  15                  450     0.5682201  0.1390807  0.3693522\n  15                  500     0.5685675  0.1387953  0.3697387\n  15                  550     0.5689770  0.1386209  0.3700281\n  15                  600     0.5692874  0.1382838  0.3702165\n  15                  650     0.5693541  0.1384084  0.3701966\n  15                  700     0.5692911  0.1386319  0.3701522\n  15                  750     0.5693165  0.1385358  0.3701469\n  15                  800     0.5694504  0.1384136  0.3701988\n  15                  850     0.5695188  0.1383351  0.3702854\n  15                  900     0.5695686  0.1384205  0.3703544\n  15                  950     0.5695529  0.1385538  0.3704281\n  15                 1000     0.5696797  0.1383867  0.3704765\n  16                   50     0.5506425  0.1507968  0.3529644\n  16                  100     0.5616128  0.1362654  0.3634280\n  16                  150     0.5659887  0.1328752  0.3676430\n  16                  200     0.5685411  0.1305190  0.3687205\n  16                  250     0.5698773  0.1300317  0.3702114\n  16                  300     0.5713070  0.1285689  0.3717679\n  16                  350     0.5731035  0.1258093  0.3725103\n  16                  400     0.5733734  0.1263990  0.3729288\n  16                  450     0.5745182  0.1245032  0.3734173\n  16                  500     0.5747431  0.1244774  0.3738280\n  16                  550     0.5749237  0.1248925  0.3740282\n  16                  600     0.5749939  0.1248818  0.3741850\n  16                  650     0.5749882  0.1250946  0.3742661\n  16                  700     0.5751892  0.1250553  0.3744467\n  16                  750     0.5751432  0.1252821  0.3744317\n  16                  800     0.5752108  0.1253867  0.3745526\n  16                  850     0.5752817  0.1252798  0.3746187\n  16                  900     0.5753969  0.1252230  0.3746964\n  16                  950     0.5754475  0.1252255  0.3747401\n  16                 1000     0.5753680  0.1254596  0.3747167\n  17                   50     0.5536390  0.1431971  0.3543933\n  17                  100     0.5613865  0.1402918  0.3627648\n  17                  150     0.5687570  0.1316996  0.3689763\n  17                  200     0.5712300  0.1298115  0.3708426\n  17                  250     0.5740503  0.1266863  0.3735552\n  17                  300     0.5746718  0.1267227  0.3741863\n  17                  350     0.5765736  0.1245523  0.3756650\n  17                  400     0.5774495  0.1240042  0.3763274\n  17                  450     0.5783740  0.1231661  0.3768496\n  17                  500     0.5790800  0.1223745  0.3772687\n  17                  550     0.5794114  0.1220988  0.3773275\n  17                  600     0.5798206  0.1214808  0.3777110\n  17                  650     0.5800443  0.1214582  0.3778407\n  17                  700     0.5800997  0.1216585  0.3779169\n  17                  750     0.5802774  0.1214462  0.3781291\n  17                  800     0.5802877  0.1216969  0.3782201\n  17                  850     0.5803490  0.1217670  0.3782772\n  17                  900     0.5803080  0.1219123  0.3783203\n  17                  950     0.5803740  0.1219102  0.3783546\n  17                 1000     0.5806250  0.1215754  0.3785561\n  18                   50     0.5434862  0.1672202  0.3482437\n  18                  100     0.5515199  0.1590088  0.3586968\n  18                  150     0.5590352  0.1476074  0.3639811\n  18                  200     0.5605794  0.1472273  0.3664598\n  18                  250     0.5636318  0.1432375  0.3682549\n  18                  300     0.5650406  0.1412924  0.3693214\n  18                  350     0.5656539  0.1412437  0.3698649\n  18                  400     0.5665835  0.1403341  0.3706158\n  18                  450     0.5664022  0.1411758  0.3706151\n  18                  500     0.5674829  0.1395665  0.3713515\n  18                  550     0.5675251  0.1397180  0.3714956\n  18                  600     0.5679198  0.1392456  0.3717211\n  18                  650     0.5679891  0.1392878  0.3718608\n  18                  700     0.5681697  0.1391801  0.3719882\n  18                  750     0.5681135  0.1394783  0.3720042\n  18                  800     0.5680679  0.1396554  0.3719883\n  18                  850     0.5681990  0.1394064  0.3721081\n  18                  900     0.5682152  0.1396242  0.3721085\n  18                  950     0.5682717  0.1395839  0.3721704\n  18                 1000     0.5683152  0.1395887  0.3721771\n  19                   50     0.5449640  0.1648043  0.3514489\n  19                  100     0.5546267  0.1543763  0.3625809\n  19                  150     0.5606773  0.1472528  0.3682896\n  19                  200     0.5631468  0.1444462  0.3696570\n  19                  250     0.5671518  0.1396552  0.3719902\n  19                  300     0.5682363  0.1392526  0.3724019\n  19                  350     0.5687737  0.1392618  0.3726532\n  19                  400     0.5700830  0.1378876  0.3734352\n  19                  450     0.5701670  0.1378543  0.3736901\n  19                  500     0.5705627  0.1375201  0.3738553\n  19                  550     0.5709605  0.1371756  0.3740677\n  19                  600     0.5713797  0.1368640  0.3743496\n  19                  650     0.5716557  0.1367334  0.3745341\n  19                  700     0.5716855  0.1367524  0.3745393\n  19                  750     0.5718351  0.1367030  0.3746433\n  19                  800     0.5721004  0.1363798  0.3748130\n  19                  850     0.5721799  0.1362699  0.3748782\n  19                  900     0.5721948  0.1362745  0.3748838\n  19                  950     0.5721885  0.1364253  0.3749420\n  19                 1000     0.5721987  0.1364195  0.3749638\n  20                   50     0.5482480  0.1572856  0.3488554\n  20                  100     0.5556374  0.1497144  0.3599358\n  20                  150     0.5576255  0.1511267  0.3652327\n  20                  200     0.5613336  0.1474933  0.3676494\n  20                  250     0.5639905  0.1441678  0.3695924\n  20                  300     0.5656516  0.1420081  0.3709471\n  20                  350     0.5663072  0.1419029  0.3712257\n  20                  400     0.5670309  0.1418311  0.3718109\n  20                  450     0.5675614  0.1413524  0.3721823\n  20                  500     0.5679185  0.1411196  0.3722005\n  20                  550     0.5689408  0.1394624  0.3726249\n  20                  600     0.5688152  0.1402204  0.3727181\n  20                  650     0.5691985  0.1396950  0.3729195\n  20                  700     0.5695188  0.1394175  0.3731282\n  20                  750     0.5695374  0.1396411  0.3730788\n  20                  800     0.5693568  0.1400280  0.3730525\n  20                  850     0.5694771  0.1399299  0.3730905\n  20                  900     0.5696971  0.1396251  0.3732414\n  20                  950     0.5696151  0.1398476  0.3731960\n  20                 1000     0.5695910  0.1399408  0.3731869\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 7, shrinkage = 0.1 and n.minobsinnode = 10.\n```\n\n\n:::\n\n```{.r .cell-code}\nmodels_LR2 <- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n## Importance frame (wide format) ===\nimportance_wide <- cbind(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(var_ranger = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(var_gbm = rownames(.)) %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Imp_gbm\" = \"Overall\")\n)  %>% kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_LR2 <- full_join(\n  varImp(ranger_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %>% \n    mutate_if(is.numeric, round, 2) %>% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %>% \n    arrange(desc(rel.inf)) %>% \n    rename(\"Importance\" = \"rel.inf\") %>% \n    select(variable, model, Importance)) %>% \n  full_join(varImp(xgb_red)$importance %>% \n    round(2) %>% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %>% \n    arrange(desc(Overall)) %>% \n    rename(\"Importance\" = \"Overall\")) %>%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n```\n\n\n:::\n:::\n\n:::\n\n### Variable Importances across models\n::: panel-tabset\n## Jaccard 1\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importances across models:\nggplot(data = importances_long_J1, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n## Jaccard 2\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importances across models:\nggplot(data = importances_long_J2, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Log Ratio 1\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importances across models:\nggplot(data = importances_long_LR1, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Log Ratio 2\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importances across models:\nggplot(data = importances_long_LR2, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n```\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n\n\n\n# Resamples across models:\n:::panel-tabset\n## Jaccard 1\n\n::: {.cell}\n\n```{.r .cell-code}\n## Jaccard 1\n# Create resamples for each model\nresamps_J1 <- resamples(list(\n  ranger = models_J1[[1]],\n  xgb = models_J1[[2]],\n  gbm = models_J1[[3]]\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the resampled error rates for each model\ndotplot_resamps_J1 <- dotplot(resamps_J1)\n\n# Summarize the resamples\nsummary_resamps_J1 <- summary(resamps_J1)\n```\n:::\n\n\n## Jaccard 2\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create resamples for each model\nresamps_J2 <- resamples(list(\n  ranger = models_J2[[1]],\n  xgb = models_J2[[2]],\n  gbm = models_J2[[3]]\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the resampled error rates for each model\ndotplot_resamps_J2 <- dotplot(resamps_J2)\n\n# Summarize the resamples\nsummary_resamps_J2 <- summary(resamps_J2)\n```\n:::\n\n\n## Log Ratio 1\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create resamples for each model\nresamps_LR1 <- resamples(list(\n  ranger = models_LR1[[1]],\n  xgb = models_LR1[[2]],\n  gbm = models_LR1[[3]]\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the resampled error rates for each model\ndotplot_resamps_LR1 <- dotplot(resamps_LR1)\n\n# Summarize the resamples\nsummary_resamps_LR1 <- summary(resamps_LR1)\n```\n:::\n\n\n## Log Ratio 2\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create resamples for each model\nresamps_LR2 <- resamples(list(\n  ranger = models_LR2[[1]],\n  xgb = models_LR2[[2]],\n  gbm = models_LR2[[3]]\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the resampled error rates for each model\ndotplot_resamps_LR2 <- dotplot(resamps_LR2)\n\n# Summarize the resamples\nsummary_resamps_LR2 <- summary(resamps_LR2)\n```\n:::\n\n:::\n\n### Summary across all analyses\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nresamples_all <- list(\n  Jaccard1 = list(\n    resamps_J1 = resamps_J1,\n    Dotplot = dotplot_resamps_J1, # store the dotplot object\n    Summary = summary_resamps_J1  # store the summary object\n  ),\n  Jaccard2 = list(\n    resamps_J2 = resamps_J2,\n    Dotplot = dotplot_resamps_J2, # store the dotplot object\n    Summary = summary_resamps_J2  # store the summary object\n  ),\n  LogRatio1 = list(\n    resamps_LR1 = resamps_LR1,\n    Dotplot = dotplot_resamps_LR1, # store the dotplot object\n    Summary = summary_resamps_LR1  # store the summary object\n  ),\n  LogRatio2 = list(\n    resamps_LR2 = resamps_LR2,\n    Dotplot = dotplot_resamps_LR2, # store the dotplot object\n    Summary = summary_resamps_LR2  # store the summary object\n))\n\n\nresamples_all\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Jaccard1\n$Jaccard1$resamps_J1\n\nCall:\nresamples.default(x = list(ranger = models_J1[[1]], xgb = models_J1[[2]], gbm\n = models_J1[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_J1)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n             Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger 0.06829633 0.07169567 0.07270608 0.07364971 0.07571423 0.08141634    0\nxgb    0.06740328 0.06906778 0.07147917 0.07206656 0.07467866 0.07734897    0\ngbm    0.06421988 0.07039892 0.07168599 0.07268969 0.07604308 0.08220707    0\n\nRMSE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.09892923 0.1027651 0.1091351 0.1111970 0.1156768 0.1297203    0\nxgb    0.09876680 0.1020442 0.1048924 0.1096201 0.1182042 0.1315152    0\ngbm    0.09474868 0.1000715 0.1039498 0.1099151 0.1165069 0.1362249    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.7893557 0.8342709 0.8531349 0.8455695 0.8671522 0.8806199    0\nxgb    0.7890662 0.8286518 0.8648936 0.8498558 0.8663667 0.8816143    0\ngbm    0.7764380 0.8305198 0.8693117 0.8491608 0.8734892 0.8896350    0\n\n\n\n$Jaccard2\n$Jaccard2$resamps_J2\n\nCall:\nresamples.default(x = list(ranger = models_J2[[1]], xgb = models_J2[[2]], gbm\n = models_J2[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard2$Dotplot\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n$Jaccard2$Summary\n\nCall:\nsummary.resamples(object = resamps_J2)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n             Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger 0.07893936 0.08223173 0.08529537 0.08621790 0.09025959 0.09783803    0\nxgb    0.07988486 0.08273490 0.08305801 0.08511744 0.08506362 0.09860895    0\ngbm    0.08314703 0.08589038 0.08743914 0.08969457 0.09451449 0.09872513    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.1120621 0.1194030 0.1240289 0.1242679 0.1279603 0.1371445    0\nxgb    0.1128614 0.1203841 0.1248338 0.1249692 0.1282270 0.1401247    0\ngbm    0.1126345 0.1258033 0.1285686 0.1280873 0.1322026 0.1407843    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.7606501 0.7986940 0.8126404 0.8094102 0.8245148 0.8464101    0\nxgb    0.7513672 0.7968261 0.8078829 0.8072804 0.8241337 0.8430043    0\ngbm    0.7492397 0.7839718 0.7984033 0.7971016 0.8026715 0.8436424    0\n\n\n\n$LogRatio1\n$LogRatio1$resamps_LR1\n\nCall:\nresamples.default(x = list(ranger = models_LR1[[1]], xgb = models_LR1[[2]],\n gbm = models_LR1[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$LogRatio1$Dotplot\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-17-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n$LogRatio1$Summary\n\nCall:\nsummary.resamples(object = resamps_LR1)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.2411987 0.2938274 0.3141578 0.3116032 0.3339331 0.3556762    0\nxgb    0.2634826 0.3192141 0.3432916 0.3368182 0.3574410 0.3886517    0\ngbm    0.2585481 0.3085024 0.3305215 0.3289230 0.3575965 0.3743060    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.3796529 0.4865685 0.5404298 0.5203579 0.5678237 0.5964049    0\nxgb    0.4086214 0.5065945 0.5581161 0.5424288 0.5898028 0.6202438    0\ngbm    0.4009512 0.5214004 0.5607243 0.5410392 0.5908667 0.6138864    0\n\nRsquared \n             Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.11731364 0.13113028 0.1769187 0.1905113 0.2220548 0.3739204    0\nxgb    0.06317943 0.13825059 0.1600146 0.1538734 0.1786388 0.2297801    0\ngbm    0.04681140 0.08501375 0.1411423 0.1358820 0.1728236 0.2524480    0\n\n\n\n$LogRatio2\n$LogRatio2$resamps_LR2\n\nCall:\nresamples.default(x = list(ranger = models_LR2[[1]], xgb = models_LR2[[2]],\n gbm = models_LR2[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$LogRatio2$Dotplot\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03_HyperparameterTuning_files/figure-html/unnamed-chunk-17-4.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n$LogRatio2$Summary\n\nCall:\nsummary.resamples(object = resamps_LR2)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.2999839 0.3105708 0.3341102 0.3301628 0.3439960 0.3657595    0\nxgb    0.3143081 0.3184640 0.3387676 0.3436322 0.3686984 0.3853569    0\ngbm    0.3192607 0.3259466 0.3369070 0.3441194 0.3538542 0.3928447    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.4715465 0.5081076 0.5262481 0.5335750 0.5591634 0.6160089    0\nxgb    0.4886618 0.5131532 0.5393618 0.5452046 0.5733619 0.6105273    0\ngbm    0.4788404 0.5051905 0.5341836 0.5378094 0.5526089 0.6341441    0\n\nRsquared \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.07138851 0.1580110 0.2000510 0.1858403 0.2274409 0.2706381    0\nxgb    0.06998937 0.1186448 0.1439607 0.1597737 0.1855734 0.2791619    0\ngbm    0.05721234 0.1155320 0.1598668 0.1752265 0.2456909 0.3076351    0\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# save.image(\"data/RData/03_reduced_hyper_para_tuning_all_models.RData\")\n```\n:::\n",
    "supporting": [
      "03_HyperparameterTuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}