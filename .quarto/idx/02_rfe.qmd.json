{"title":"Script 2 - Recursive Feature Selection","markdown":{"yaml":{"title":"Script 2 - Recursive Feature Selection","author":[{"name":"MSc. Friederike Johanna Rosa Wölke","orcid":"0000-0001-9034-4883","url":"https://friedarosa.github.io","email":"wolke@fzp.czu.cz","corresponding":true}],"date":"2023-05-29"},"headingText":"Recursive feature elimination","containsRefs":false,"markdown":"\n\n\nI will use recursive feature elimination to reduce the dimensionality of the data by removing variables that do not lead to an increase of model performance when included.\nI will apply this method, because most of my predictors were calculated from the same data and are thus not independent.\nAlthough checking for high correlations in one of the previous steps, any correlations between predictor variables may confuse the model during variation partitioning, as correlations make it impossible to discern which variable explains how much of the variation.\nThis also reduces the probability of overfitting the model to the data as redundant features with correlated noise signals are removed.\n\nThe method being used relies on the `randomForest` package to recursively eliminate one predictor after another from the model, calculate the variable importance, rank these, average the importance across resamples and comparing the fit across models with different subsets of the set of predictors.\nThe workflow is set in the `caret` helper function `rfFuncs()`.\n\n::: panel-tabset\n## Source custom functions\n\n```{r}\n#| label: load-packages\n#| message: FALSE\n#| warning: FALSE\nrm(list = ls())\n\nsource(\"src/functions.R\")\n\n```\n\n## MachineLearning packages\n\n```{r}\n#| label: load-ML-packages\n#| message: FALSE\n#| error: FALSE\n\npckgs <- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"ggcorrplot\", \n           \"caret\",  \"recipes\",   \"caretEnsemble\", \n           \"randomForest\", \n           \"gridExtra\", \"kableExtra\", \"tidyr\")\n\ninstall_and_load(pckgs)\n```\n\n## Load RData to reduce computing time\n\n```{r}\n#| label: load-RData\n#| message: FALSE\n#| error: FALSE\n\n# Load workspace to save computing time:\n## it has: varPart from ranger models\n## recursive feature selection results\n\nload(\"data/RData/02_rfe_full_vs_reduced.RData\")\n```\n:::\n\n### Predictor importance / Recursive Feature Selection\n\nWe will set up a loop that runs through the four response variables that I am investigating.\nThe models from which the variable importance is calculated are run for 5000 trees each across 10 resamples.\nAgain I will be using 10-fold repeated cross-validation with 3 repeats to evaluate the performance of the models.\n\n```{r}\n#| eval: false\n#| label: rfe-loop\n\n\nindex_list <- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list <- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\n\nsaved_profiles <- replicate(4, list())\n# names(saved_profiles) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nsave_imp <- replicate(4, list())\n# names(save_imp) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\n\nresponse_list <- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nfor(j in seq_along(1:4)){\n  ## Loop through differet datasets/Analyses\n  indices <- index_list[[j]]\n  dat_train <- dat_train_list[[j]]\n  response <- response_list[[j]] \n  saved_profiles[[j]] <- replicate(4, list())\n  save_imp[[j]] <- replicate(4, list())\n  \n  ## Recursive feature selection:\n  set.seed(42)\n  ctrl <- rfeControl(\n    functions = rfFuncs,\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    returnResamp = \"all\", # we need all resamples\n    verbose = FALSE,\n    index = indices,\n    saveDetails = TRUE)\n\n  ctrl$functions$rank <- rank #adjust rank function\n\n  ## Set variables for recursive feature elimination\n  subsets <- c(1:50) # number of predictors in each run\n  x <- dat_train %>% select(!all_of(response))\n  y <- dat_train %>% pull(response)\n  \n  ## First run:\n  set.seed(42)\n  rfProfile <- rfe(x, y, ntree = 5000, sizes = subsets, rfeControl = ctrl)\n  rfProfile\n    \n  # Most important predictors:\n  imp <- as.data.frame(rfProfile$fit$importance) %>%\n                   round(3) %>%\n                   select(`%IncMSE`) %>% \n                   mutate(var = row.names(.)) %>%\n                   arrange(desc(`%IncMSE`))    \n    \n  saved_profiles[[j]] <- rfProfile\n  save_imp[[j]] <- imp\n}\n\nsaveRDS(saved_profiles, file = \"data/02_rfe_saved_profiles_5000.rds\")\nsave.image(file = \"data/RData/02_rfe_5000.RData\")\n```\n\n```{r}\n#| label: rfe-results-eval\n#| fig.height: 4\n#| fig.width: 4\n#| eval: true\n#| warning: false\n\nsaved_profiles[[1]]$bestSubset\nsaved_profiles[[2]]$bestSubset\nsaved_profiles[[3]]$bestSubset\nsaved_profiles[[4]]$bestSubset\n\n\n\nresults <- replicate(4, list())\n    for (i in seq_along(1:4)){\n        resamp_res <- saved_profiles[[i]]\n        res <- slice_min(resamp_res$results, RMSE)\n        results[[i]] <- res\n    }\n\nnames(results) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nrfe_res <- do.call(rbind, results)\nrfe_res$dd <- rownames(rfe_res)\n\n\n# Bar plot: Nr. Vars selected for each analysis \nggplot(data = rfe_res, aes(x = dd, y = Variables)) +\n    geom_col(fill = \"lightgrey\") +\n    geom_point(data = rfe_res %>% group_by(dd) %>% summarize(mean_Variables = mean(Variables)), \n    aes(x = dd, y = mean_Variables), color = \"red\") +\n    theme_classic()+\n    labs(title = \"Number of variables selected per analysis\", y = \"Number of variables selected\", x = \"Analysis\")\n\n\n# Add mean importance across resamples to results\nsaved_profiles[[1]]$variables <- saved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall)) %>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[2]]$variables <- saved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[3]]$variables <- saved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[4]]$variables <- saved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n```\n\n::: panel-tabset\n## Jaccard\n\nThe following plots show, that the data that can be used to predict Jaccard can be reduced enormously without losing predictive performance.\n\nFor *`Jaccard1`* the\n\n-   reduced model yields: RMSE = 0.1116 and R² = 0.8450,\n\n-   while the full model yields: RMSE = 0.1118 and R² = 0.8455.\n\nshowing even a slightly reduced RMSE compared to the full model.\n\nFor Jaccard 2 the\n\n-   reduced model yields: RMSE = 0.1248 and R² = 0.8077,\n\n-   while the full model yields: RMSE = 0.1273 and R² = 0.8022.\n\n```{r}\n#| label: rfe-results-boxplot-j\n#| fig.height: 8\n#| fig.width: 10\n#| eval: true\n#| warning: false\n\n# On best models (best hyper parameters)\nsaved_profiles[[1]]$fit \nsaved_profiles[[1]]$fit %>% varImp()\nplot(saved_profiles[[1]])\n\n\nsaved_profiles[[2]]$fit\nsaved_profiles[[2]]$fit %>% varImp()\nplot(saved_profiles[[2]])\n\n## Plot the importances\n\ngrid.arrange(ncol=2,\nsaved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[1]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 1\", x = \"Importance\", y = \"Variable\"),\n\n\nsaved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[2]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n\n## Log Ratio\n\nFor `log ratio of AOO,` we can see that we need more predictors than for `Jaccard` to predict it from the data.\nAs expected before, the model performance is generally low (both R² = 0.147) and the models try to include more information to discern the relationship between predictors and the response that does not capture a big signal from temporal change.\n\n```{r}\n#| label: rfe-results-boxplot-lr\n#| fig.height: 8\n#| fig.width: 10\n#| eval: true\n#| warning: false\n\n# On best models (best hyper parameters)\nsaved_profiles[[3]]$fit \nsaved_profiles[[3]]$fit %>% varImp()\nplot(saved_profiles[[3]])\n\n\nsaved_profiles[[4]]$fit\nsaved_profiles[[4]]$fit %>% varImp()\nplot(saved_profiles[[4]])\n\n\ngrid.arrange(ncol=2,\nsaved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[3]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 1\", x = \"Importance\", y = \"Variable\"),\n\nsaved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[4]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n:::\n\n```{r}\nImp_list <- replicate(4, list())\nImp_list[[1]] <- saved_profiles[[1]]$variables %>% \n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\") %>%\n  mutate(model = \"J1\")\nImp_list[[2]] <- saved_profiles[[2]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"J2\")\n\nImp_list[[3]] <- saved_profiles[[3]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR1\")\n\nImp_list[[4]] <- saved_profiles[[4]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR2\")\n\n# Included vars:\nJ1_vars <- saved_profiles[[1]]$fit$importance\nJ2_vars <- saved_profiles[[2]]$fit$importance\nLR1_vars <- saved_profiles[[3]]$fit$importance\nLR2_vars <- saved_profiles[[4]]$fit$importance\n\nImp_df <- do.call(rbind, Imp_list)\nwide <- Imp_df %>%\n  tidyr::pivot_wider(names_from = c(model), \n                     values_from = imp, names_sep = \"_\", \n                     values_fn = mean) %>% \n  arrange(desc(J1)) %>% \n  group_by(var) %>%  \n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\")) %>%\n  mutate(Include_J1 = case_when(var %in% c(row.names(J1_vars)) ~ 1, \n         .default = 0), \n         Include_J2 = case_when(var %in% c(row.names(J2_vars)) ~ 1, \n         .default = 0),\n         Include_LR1 = case_when(var %in% c(row.names(LR1_vars)) ~ 1, \n         .default = 0),\n         Include_LR2 = case_when(var %in% c(row.names(LR2_vars)) ~ 1, \n         .default = 0))\n\nwide %>% write.csv(\"data/csv/02_all_var_imp_5000.csv\")\nwide %>% kableExtra::kable()\n```\n\n\n# Compare reduced and full ranger models\nSince the rfe function works with the randomForest package, we will check if we get similarly better results with ranger and the reduced model.\n::: panel-tabset\n## Jaccard 1\n```{r}\n#| label: compare-ranger-results-J1\n#| eval: false\n\nJ1_vars <- wide %>% filter(Include_J1 == 1) %>% pull(var)\n\nresponse <- \"Jaccard\"\n\nindices <- indices_J1\n\ndat_train <- dat_train_J1\n\n\n# Define training control ==========================================================\n  trainControl <- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"all\",\n    verboseIter = FALSE,\n    index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_full <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\ntictoc::toc()\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_reduced <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train %>% select(response, all_of(J1_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\n  \ntictoc::toc()\nJ1_full$finalModel\nJ1_full$results\nJ1_reduced$finalModel\nJ1_reduced$results\n\n```\n\n## Jaccard 2\n```{r}\n#| label: compare-ranger-results-J2\n#| eval: false\n\nJ2_vars <- wide %>% filter(Include_J2 == 1) %>% pull(var)\nresponse <- \"Jaccard\"\nindices <- indices_J2\ndat_train <- dat_train_J2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(J2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nJ2_full$finalModel\nJ2_full$results\nJ2_reduced$finalModel\nJ2_reduced$results\n\n```\n\n## Log Ratio 1\n```{r}\n#| label: compare-ranger-results-LR1\n#| eval: false\n\nLR1_vars <- wide %>% filter(Include_LR1 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR1\ndat_train <- dat_train_LR1\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR1_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n\ntictoc::toc()\nLR1_full$finalModel\nLR1_full$results\nLR1_reduced$finalModel\nLR1_reduced$results\n\n```\n\n\n## Log Ratio 2\n```{r}\n#| label: compare-ranger-results-LR2\n#| eval: false\n\nLR2_vars <- wide %>% filter(Include_LR2 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR2\ndat_train <- dat_train_LR2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n  \n  \n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \ntictoc::toc()\n  \nLR2_full$finalModel\nLR2_full$results\nLR2_reduced$finalModel\nLR2_reduced$results\n\n\n```\n:::\n\n\n```{r}\n# save.image(\"data/RData/02_rfe_full_vs_reduced.RData\")\n# selected_predictors <- list(J1_vars, J2_vars, LR1_vars, LR2_vars)\n# saveRDS(selected_predictors, \"data/rds/selected_predictors_list.rds\")\n```\n","srcMarkdownNoYaml":"\n\n# Recursive feature elimination\n\nI will use recursive feature elimination to reduce the dimensionality of the data by removing variables that do not lead to an increase of model performance when included.\nI will apply this method, because most of my predictors were calculated from the same data and are thus not independent.\nAlthough checking for high correlations in one of the previous steps, any correlations between predictor variables may confuse the model during variation partitioning, as correlations make it impossible to discern which variable explains how much of the variation.\nThis also reduces the probability of overfitting the model to the data as redundant features with correlated noise signals are removed.\n\nThe method being used relies on the `randomForest` package to recursively eliminate one predictor after another from the model, calculate the variable importance, rank these, average the importance across resamples and comparing the fit across models with different subsets of the set of predictors.\nThe workflow is set in the `caret` helper function `rfFuncs()`.\n\n::: panel-tabset\n## Source custom functions\n\n```{r}\n#| label: load-packages\n#| message: FALSE\n#| warning: FALSE\nrm(list = ls())\n\nsource(\"src/functions.R\")\n\n```\n\n## MachineLearning packages\n\n```{r}\n#| label: load-ML-packages\n#| message: FALSE\n#| error: FALSE\n\npckgs <- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"ggcorrplot\", \n           \"caret\",  \"recipes\",   \"caretEnsemble\", \n           \"randomForest\", \n           \"gridExtra\", \"kableExtra\", \"tidyr\")\n\ninstall_and_load(pckgs)\n```\n\n## Load RData to reduce computing time\n\n```{r}\n#| label: load-RData\n#| message: FALSE\n#| error: FALSE\n\n# Load workspace to save computing time:\n## it has: varPart from ranger models\n## recursive feature selection results\n\nload(\"data/RData/02_rfe_full_vs_reduced.RData\")\n```\n:::\n\n### Predictor importance / Recursive Feature Selection\n\nWe will set up a loop that runs through the four response variables that I am investigating.\nThe models from which the variable importance is calculated are run for 5000 trees each across 10 resamples.\nAgain I will be using 10-fold repeated cross-validation with 3 repeats to evaluate the performance of the models.\n\n```{r}\n#| eval: false\n#| label: rfe-loop\n\n\nindex_list <- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list <- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\n\nsaved_profiles <- replicate(4, list())\n# names(saved_profiles) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nsave_imp <- replicate(4, list())\n# names(save_imp) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\n\nresponse_list <- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nfor(j in seq_along(1:4)){\n  ## Loop through differet datasets/Analyses\n  indices <- index_list[[j]]\n  dat_train <- dat_train_list[[j]]\n  response <- response_list[[j]] \n  saved_profiles[[j]] <- replicate(4, list())\n  save_imp[[j]] <- replicate(4, list())\n  \n  ## Recursive feature selection:\n  set.seed(42)\n  ctrl <- rfeControl(\n    functions = rfFuncs,\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    returnResamp = \"all\", # we need all resamples\n    verbose = FALSE,\n    index = indices,\n    saveDetails = TRUE)\n\n  ctrl$functions$rank <- rank #adjust rank function\n\n  ## Set variables for recursive feature elimination\n  subsets <- c(1:50) # number of predictors in each run\n  x <- dat_train %>% select(!all_of(response))\n  y <- dat_train %>% pull(response)\n  \n  ## First run:\n  set.seed(42)\n  rfProfile <- rfe(x, y, ntree = 5000, sizes = subsets, rfeControl = ctrl)\n  rfProfile\n    \n  # Most important predictors:\n  imp <- as.data.frame(rfProfile$fit$importance) %>%\n                   round(3) %>%\n                   select(`%IncMSE`) %>% \n                   mutate(var = row.names(.)) %>%\n                   arrange(desc(`%IncMSE`))    \n    \n  saved_profiles[[j]] <- rfProfile\n  save_imp[[j]] <- imp\n}\n\nsaveRDS(saved_profiles, file = \"data/02_rfe_saved_profiles_5000.rds\")\nsave.image(file = \"data/RData/02_rfe_5000.RData\")\n```\n\n```{r}\n#| label: rfe-results-eval\n#| fig.height: 4\n#| fig.width: 4\n#| eval: true\n#| warning: false\n\nsaved_profiles[[1]]$bestSubset\nsaved_profiles[[2]]$bestSubset\nsaved_profiles[[3]]$bestSubset\nsaved_profiles[[4]]$bestSubset\n\n\n\nresults <- replicate(4, list())\n    for (i in seq_along(1:4)){\n        resamp_res <- saved_profiles[[i]]\n        res <- slice_min(resamp_res$results, RMSE)\n        results[[i]] <- res\n    }\n\nnames(results) <- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\nrfe_res <- do.call(rbind, results)\nrfe_res$dd <- rownames(rfe_res)\n\n\n# Bar plot: Nr. Vars selected for each analysis \nggplot(data = rfe_res, aes(x = dd, y = Variables)) +\n    geom_col(fill = \"lightgrey\") +\n    geom_point(data = rfe_res %>% group_by(dd) %>% summarize(mean_Variables = mean(Variables)), \n    aes(x = dd, y = mean_Variables), color = \"red\") +\n    theme_classic()+\n    labs(title = \"Number of variables selected per analysis\", y = \"Number of variables selected\", x = \"Analysis\")\n\n\n# Add mean importance across resamples to results\nsaved_profiles[[1]]$variables <- saved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall)) %>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[2]]$variables <- saved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[3]]$variables <- saved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n\nsaved_profiles[[4]]$variables <- saved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  mutate(Overall_mean_resamp = mean(Overall))%>%\n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\"))\n```\n\n::: panel-tabset\n## Jaccard\n\nThe following plots show, that the data that can be used to predict Jaccard can be reduced enormously without losing predictive performance.\n\nFor *`Jaccard1`* the\n\n-   reduced model yields: RMSE = 0.1116 and R² = 0.8450,\n\n-   while the full model yields: RMSE = 0.1118 and R² = 0.8455.\n\nshowing even a slightly reduced RMSE compared to the full model.\n\nFor Jaccard 2 the\n\n-   reduced model yields: RMSE = 0.1248 and R² = 0.8077,\n\n-   while the full model yields: RMSE = 0.1273 and R² = 0.8022.\n\n```{r}\n#| label: rfe-results-boxplot-j\n#| fig.height: 8\n#| fig.width: 10\n#| eval: true\n#| warning: false\n\n# On best models (best hyper parameters)\nsaved_profiles[[1]]$fit \nsaved_profiles[[1]]$fit %>% varImp()\nplot(saved_profiles[[1]])\n\n\nsaved_profiles[[2]]$fit\nsaved_profiles[[2]]$fit %>% varImp()\nplot(saved_profiles[[2]])\n\n## Plot the importances\n\ngrid.arrange(ncol=2,\nsaved_profiles[[1]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[1]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 1\", x = \"Importance\", y = \"Variable\"),\n\n\nsaved_profiles[[2]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[2]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: Jaccard tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n\n## Log Ratio\n\nFor `log ratio of AOO,` we can see that we need more predictors than for `Jaccard` to predict it from the data.\nAs expected before, the model performance is generally low (both R² = 0.147) and the models try to include more information to discern the relationship between predictors and the response that does not capture a big signal from temporal change.\n\n```{r}\n#| label: rfe-results-boxplot-lr\n#| fig.height: 8\n#| fig.width: 10\n#| eval: true\n#| warning: false\n\n# On best models (best hyper parameters)\nsaved_profiles[[3]]$fit \nsaved_profiles[[3]]$fit %>% varImp()\nplot(saved_profiles[[3]])\n\n\nsaved_profiles[[4]]$fit\nsaved_profiles[[4]]$fit %>% varImp()\nplot(saved_profiles[[4]])\n\n\ngrid.arrange(ncol=2,\nsaved_profiles[[3]]$variables %>% filter(Variables == 39) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[3]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ), show.legend = FALSE)+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 1\", x = \"Importance\", y = \"Variable\"),\n\nsaved_profiles[[4]]$variables %>% filter(Variables == 38) %>%\n  group_by(var) %>%\n  ggplot()+\n  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = (39.5-saved_profiles[[4]]$bestSubset)), fill = \"lightgray\", alpha = 0.9) +\n  geom_boxplot(aes(y = reorder(var, Overall), x = Overall, fill = hypo ))+\n  geom_point(aes(x = Overall_mean_resamp, y = var), col = \"red\", alpha = 0.4)+\n  theme_classic()+\n  xlim(0,100)+\n  scale_fill_manual(values = c(\"#e66101\", \"#fdb863\", \"#b2abd2\", \"#5e3c99\")) +\n  labs(title = \"Variables by Importance: log ratio tp = 2\", x = \"Importance\", y = \"Variable\")\n)\n```\n:::\n\n```{r}\nImp_list <- replicate(4, list())\nImp_list[[1]] <- saved_profiles[[1]]$variables %>% \n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\") %>%\n  mutate(model = \"J1\")\nImp_list[[2]] <- saved_profiles[[2]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"J2\")\n\nImp_list[[3]] <- saved_profiles[[3]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR1\")\n\nImp_list[[4]] <- saved_profiles[[4]]$variables %>%\n  select(Overall_mean_resamp, var) %>%\n  rename(\"imp\" = \"Overall_mean_resamp\")%>%\n  mutate(model = \"LR2\")\n\n# Included vars:\nJ1_vars <- saved_profiles[[1]]$fit$importance\nJ2_vars <- saved_profiles[[2]]$fit$importance\nLR1_vars <- saved_profiles[[3]]$fit$importance\nLR2_vars <- saved_profiles[[4]]$fit$importance\n\nImp_df <- do.call(rbind, Imp_list)\nwide <- Imp_df %>%\n  tidyr::pivot_wider(names_from = c(model), \n                     values_from = imp, names_sep = \"_\", \n                     values_fn = mean) %>% \n  arrange(desc(J1)) %>% \n  group_by(var) %>%  \n  mutate(hypo = case_when(var %in% H1_vars ~ \"H1\",\n                          var %in% H2_vars ~ \"H2\",\n                          var %in% H3_vars ~ \"H3\",\n                          var %in% H4_vars ~ \"H4\")) %>%\n  mutate(Include_J1 = case_when(var %in% c(row.names(J1_vars)) ~ 1, \n         .default = 0), \n         Include_J2 = case_when(var %in% c(row.names(J2_vars)) ~ 1, \n         .default = 0),\n         Include_LR1 = case_when(var %in% c(row.names(LR1_vars)) ~ 1, \n         .default = 0),\n         Include_LR2 = case_when(var %in% c(row.names(LR2_vars)) ~ 1, \n         .default = 0))\n\nwide %>% write.csv(\"data/csv/02_all_var_imp_5000.csv\")\nwide %>% kableExtra::kable()\n```\n\n\n# Compare reduced and full ranger models\nSince the rfe function works with the randomForest package, we will check if we get similarly better results with ranger and the reduced model.\n::: panel-tabset\n## Jaccard 1\n```{r}\n#| label: compare-ranger-results-J1\n#| eval: false\n\nJ1_vars <- wide %>% filter(Include_J1 == 1) %>% pull(var)\n\nresponse <- \"Jaccard\"\n\nindices <- indices_J1\n\ndat_train <- dat_train_J1\n\n\n# Define training control ==========================================================\n  trainControl <- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"all\",\n    verboseIter = FALSE,\n    index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_full <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\ntictoc::toc()\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\n  J1_reduced <- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train %>% select(response, all_of(J1_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\n  \ntictoc::toc()\nJ1_full$finalModel\nJ1_full$results\nJ1_reduced$finalModel\nJ1_reduced$results\n\n```\n\n## Jaccard 2\n```{r}\n#| label: compare-ranger-results-J2\n#| eval: false\n\nJ2_vars <- wide %>% filter(Include_J2 == 1) %>% pull(var)\nresponse <- \"Jaccard\"\nindices <- indices_J2\ndat_train <- dat_train_J2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(J2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nJ2_full$finalModel\nJ2_full$results\nJ2_reduced$finalModel\nJ2_reduced$results\n\n```\n\n## Log Ratio 1\n```{r}\n#| label: compare-ranger-results-LR1\n#| eval: false\n\nLR1_vars <- wide %>% filter(Include_LR1 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR1\ndat_train <- dat_train_LR1\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR1_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n\ntictoc::toc()\nLR1_full$finalModel\nLR1_full$results\nLR1_reduced$finalModel\nLR1_reduced$results\n\n```\n\n\n## Log Ratio 2\n```{r}\n#| label: compare-ranger-results-LR2\n#| eval: false\n\nLR2_vars <- wide %>% filter(Include_LR2 == 1) %>% pull(var)\nresponse <- \"log_R2_1\"\nindices <- indices_LR2\ndat_train <- dat_train_LR2\n\n\n# Define training control ==========================================================\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_full <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n  \n  \n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_reduced <- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %>% select(response, all_of(LR2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \ntictoc::toc()\n  \nLR2_full$finalModel\nLR2_full$results\nLR2_reduced$finalModel\nLR2_reduced$results\n\n\n```\n:::\n\n\n```{r}\n# save.image(\"data/RData/02_rfe_full_vs_reduced.RData\")\n# selected_predictors <- list(J1_vars, J2_vars, LR1_vars, LR2_vars)\n# saveRDS(selected_predictors, \"data/rds/selected_predictors_list.rds\")\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"quarto::html_document":{"self_contained":true,"toc":true,"number_sections":true,"keep_md":true}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"02_rfe.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","bibliography":["references.bib"],"comments":{"hypothesis":true},"theme":"united","monofont":"Cascadia Code","title":"Script 2 - Recursive Feature Selection","author":[{"name":"MSc. Friederike Johanna Rosa Wölke","orcid":"0000-0001-9034-4883","url":"https://friedarosa.github.io","email":"wolke@fzp.czu.cz","corresponding":true}],"date":"2023-05-29"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}