[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assignment Statistics KVHEM CZU SS 2024",
    "section": "",
    "text": "Assignment for PhD Statistics class 2024",
    "crumbs": [
      "Assignment for PhD Statistics class 2024"
    ]
  },
  {
    "objectID": "index.html#topic-of-credit-report-statistical-methods---prague-07.05.2024",
    "href": "index.html#topic-of-credit-report-statistical-methods---prague-07.05.2024",
    "title": "Assignment Statistics KVHEM CZU SS 2024",
    "section": "Topic of Credit Report – Statistical Methods - Prague, 07.05.2024",
    "text": "Topic of Credit Report – Statistical Methods - Prague, 07.05.2024\nTitle of report: Towards predicting temporal biodiversity change from static patterns\nName: MSc. Friederike Wölke\nSupervisor: Dr. Petr Keil\nThesis title: Universal imprints of temporal change in static spatial patterns of biodiversity\nExpected methods applied:\n\nMachine learning (random forest regression from ranger package; using the caret suite of tools for hyperparameter tuning)\nvisualization of results (ggplot2, partial plots, BAM chart).\n\nAbstract:\nThe world is undergoing significant environmental transformations, impacting biodiversity and ecosystem functions. Since obtaining temporal replication of biodiversity data is challenging due to cost and monitoring limitations, I aim at predicting temporal trends in species occupancy without requiring temporally replicated data.\nBiodiversity kinetics leave characteristics imprint in the spatial patterns that we can see from geo-referenced presence/absence data because the underlying processes such as extinction and colonization happen across space.\nI aim at predicting the log ratio of temporal change in occupancy (i.e., the sum of area occupied by a species) between two sampling periods from a set of predictor variables that are either related to H1) species traits and ecology, H2)  geometric features of the species range from a single sampling period, H3) biodiversity equilibrium dynamics via spatial diversity patterns,  or H4) to the characteristics of the study region – all of which may equally contribute and act in concert to explaining the temporal process that is underlying the spatial pattern.\nFor this I use high-quality, spatially continuous atlas data from four breeding bird atlases from temperate zones across the globe. Atlas data comes with spatial grids that enable easy up- and down scaling of the data.\nHere, I assess universal imprints of temporal change in breeding birds in Czech Republic, Japan, New York State and the whole of Europe across two aggregated sampling periods that took place pre-2000 and post-2000. Since data for two sampling periods are available, I will additionally test whether imprints in the spatial aggregation of species can better predict past or future biodiversity change.\nFor this, I collated 60 predictor variables that vary across sampling periods - each belonging to one of the hypotheses mentioned above. I will use random forest regression to determine the capability of static patterns to predict temporal change, identify the most important predictor variables and compare observed versus predicted results.\nIf my model can predict temporal change from static patterns, this method will be a useful tool for estimating temporal change in areas and for species where repeated monitoring might not be feasible. If the models are only partially able to predict temporal trends, the important predictors may still yield insights into how temporal processes are acting across space. Additionally disentangling whether imprints of biodiversity kinetics are better at explaining past versus future change may help to understand the temporal dimensions of the imprints.",
    "crumbs": [
      "Assignment for PhD Statistics class 2024"
    ]
  },
  {
    "objectID": "index.html#adjustments-to-the-topic",
    "href": "index.html#adjustments-to-the-topic",
    "title": "Assignment Statistics KVHEM CZU SS 2024",
    "section": "Adjustments to the topic",
    "text": "Adjustments to the topic\nSince handing in the topic of the report, I found that the response variable that I chose did not capture much of a signal in the data which led to very poor predictive power and model performances. The distribution of this variable was predominantly centered around zero with few extreme cases of extreme increase or decrease, while predictor variables varied heavily. Ths suggested that there is just not enough signal in the log ratio for predictions. I investigated why this was the case. Here follows the explanation:\nLog ratio of the area of occupancy (log ratio AOO) was calculated as follows:\nlog times the product of the sum of all areas occupied in sampling period 2 (AOO 2) divided by the sum of all areas occupied in sampling period 1 (AOO1 ), i.e.,:\n\\[log Ratio_{1,2} = log(AOO_{tp2}/AOO_{tp1})\\]\nI found that most species did not experience much change in AOO between both sampling periods, but a lot of site turnover happened. This means that, although log ratio of AOO is zero, species moved around a lot. Some even switched from a north Europe-based distribution to a south Europe-based distribution (e.g. Vanellus vanellus, the Northern Lapwing).\n\nThese changes were not detected at all by the metric that I chose to describe temporal change. Thus I decided to include another metric of temporal change which better captures the spatial change that species undergo. I calculated the Jaccard index of similarity for each species across their occupied sites between two sampling periods. Jaccard was calculated as follows:\n\\[\nJ(X,Y) = |X∩Y| / |X∪Y|\n\\]\nwhere \\(X\\) are the names of the cells that are occupied in period 1 and \\(Y\\) is the names of the cells that are occupied in period 2. This results in a measure of similarity between sites that ranges from 1 (= complete similarity in sites) to 0 (= no similar sites). This may give an idea of how much spatial change happened for each species across sampling periods.\nIn the following report I will still report the results for log ratio AOO, but these will have secondary importance and results for different response variables will be printed in separate tabs for the same analysis.",
    "crumbs": [
      "Assignment for PhD Statistics class 2024"
    ]
  },
  {
    "objectID": "01_DataPrep.html",
    "href": "01_DataPrep.html",
    "title": "1  Script 1 - Data preparation",
    "section": "",
    "text": "2 The data\nSince the raw data is not open, I am providing the (reduced) predictor table that I calculated from it and other external data. For additional information on how the raw data was handled to produce this set of predictors, please visit the corresponding github repository (https://github.com/FriedaRosa/BEAST_General_Procedures/tree/main/Project_Frieda/StaticPredictors).\nThe data has bird species in rows (verbatim_name) and their predictor data across different datasets (dataset) in columns.\nThe column log_R2_1 is the log ratio of AOO (area of occupancy) between two sampling periods (indicated with tp = 1 or tp = 2) and was the inital response for my temporal change models.\nTelfer_1_2 is another measure of temporal change, though it is relative for each species in a dataset in comparison to the average other species. Telfer will not be further investigated in this assignment. I have assessed its correlation with log ratio before and they correlate well.\nJaccard is the Jaccard index of similarity and indicates how similar ( 0 - 1) two species ranges are across different sampling periods.\nRaw data:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Script 1 - Data preparation</span>"
    ]
  },
  {
    "objectID": "01_DataPrep.html#hypotheses",
    "href": "01_DataPrep.html#hypotheses",
    "title": "1  Script 1 - Data preparation",
    "section": "3.1 Hypotheses",
    "text": "3.1 Hypotheses\nIn the course of this analysis I will investigate how much variation in temporal change metrics can be explained by different hypothesis. This is an important first step of my dissertation project, as it helps me to guide my future investigations towards specific topics.\nIn the following I will go a bit into the detail of the specific hypotheses.\n\nHypothesis 1: Species TraitsHypothesis 2: Species range geometryHypothesis 3: Diversity MetricsHypothesis 4: Atlas geometry\n\n\nFor this analysis I hypothesize that species-characteristics contribute to the temporal change trends of a species through proxies of dispersal ability, adaptive potential and competitive strength. These factors may play a role in determining whether a species is able to persist in a certain area (cell) or whether it will move away (and how far it might move away), or whether it may be able to adapt to new circumstances via their phylogenetic heritage in old vs. new lineages (i.e., their evolutionary distinctness).\n\nH1_vars &lt;- c(\n    \"sd_PC1\", \"sd_PC2\", # Climatic Niche Breadth\n    \"GlobRangeSize_m2\", \"IUCN\", \"Mass\", \"Habitat\", \"Habitat.Density\",\n    \"Migration\", \"Trophic.Level\", \"Trophic.Niche\", \"Primary.Lifestyle\",\n    \"FP\", # Phylogenetic Distinctness\n    \"Hand.Wing.Index\") # Measure of dispersal ability\n\n\n\nThe range of a species in a study region is the product of population-scale colonization and extinction processes that add to the birth-death dynamics of a population. Since it is the product of these processes that determine temporal change for species, I hypothesize that they might contain signals of these underlying processes and thus making it possible to infer temporal change from these spatial characteristics of the species range. Variables that start with “rel_” were calculated as species measures relative to the study region, making it comparable between different study regions.\n\nH2_vars &lt;- c(\n    \"AOO\", \"rel_occ_Ncells\", \"mean_prob_cooccur\", \"D_AOO_a\", \n    \"moran\", \"x_intercept\", \"sp_centr_lon\", \"sp_centr_lat\",\n    \"lengthMinRect\", \"widthMinRect\", \"elonMinRect\", \"bearingMinRect\",\n    \"circ\", \"bearing\", \"Southernness\", \"Westernness\",\n    \"rel_maxDist\", \"rel_ewDist\", \"rel_nsDist\", \"rel_elonRatio\",\n    \"rel_relCirc\", \"rel_circNorm\", \"rel_lin\", \"Dist_centroid_to_COG\",\n    \"maxDist_toBorder_border\", \"maxDist_toBorder_centr\",\n    \"minDist_toBorder_centr\")\n\n\n\nSince this is a spatial analysis on species level, calculating spatial diversity metrics, such as Gamma Diversity (i.e., species richness of the study region), Alpha diversity (i.e., species richness of a single cell) and Beta diversity (i.e., the product of Gamma and Alpha indicating species turnover between sites), was more complicated than initially expected.\nThe idea was, that the species richness of a single cell may influence how many species can colonize it in addition to those that are already there, since there are limited resources per cell. This is easily calculated for sites across all species, but not so much for species across sites. Thus I chose to calculate the mean alpha and beta diversities for each species based on the species richness in cells that are occupied by this species and their mean beta diversity.\nNow, this rather indicates a species potential to compete or avoid competition ecologically when occupying the same space with other species. A species with a high mean alpha diversity, is a species that can survive in competition with many other species, while one that has a low mean alpha diversity, might struggle in such situations or is better adapted to more specialized environments. Equally, species that have a high mean beta diversity are found in cells that are generally poor in species richness, which may explain their specialization compared to other species in the data. This changes a bit the interpretation of this hypothesis but it still makes sense to include it for the reason I mentioned before: A species potential to co-occur with many other species may still contribute to inferring spatial change of species.\n\nH3_vars &lt;- c(\"GammaSR\", \"AlphaSR_sp\", \"BetaSR_sp\")\n\n\n\nThe last hypothesis concerns how much characteristics of the study region influence a species temporal change dynamics. For example, investigating a landlocked country such as Czech Republic, where borders are artificially introduced to the data, but do not capture ecological units that act as barriers, may lead to results that may not be explainable with biological information. In addition, the shape of the study region may contribute to explaining temporal change dynamics of species. Elongated study areas, such as Japan, may lead to different dynamics than those that are not elongated (e.g., New York State), just because there are more possibilities how to occupy a square than there is to occupy an elongated rectangle. Of course, the size of the study region may also explain how much change can actually happen, where more change can happen in larger study regions.\n\nH4_vars &lt;- c(\n    \"dataset\", \"mean_area\", \"Total_area_samp\", \"Total_Ncells_samp\",\n    \"mean_cell_length\", \"atlas_lengthMinRect\", \"atlas_widthMinRect\",\n    \"atlas_elonMinRect\", \"atlas_circ\", \"atlas_bearingMinRect\",\n    \"atlas_bearing\", \"AtlasCOG_long\", \"AtlasCOG_lat\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Script 1 - Data preparation</span>"
    ]
  },
  {
    "objectID": "01_DataPrep.html#create-data-subsets",
    "href": "01_DataPrep.html#create-data-subsets",
    "title": "1  Script 1 - Data preparation",
    "section": "3.2 Create data subsets",
    "text": "3.2 Create data subsets\nIn the following part we will create 4 different datasets from our data table. We will be assessing whether change in occupied area (log Ratio AOO) or change in sites (Jaccard) can be better predicted per species, and whether past (tp = 1) or future (tp = 2) change can be better predicted.\nNote: Some predictor columns have NAs that result from either very rare species or highly cosmopolitan species (thus resulting in division by 0 during computation). With knowledge of how I computed the predictors, I manually set some rows with NAs to 0 or 1 within the process_data() function that I wrote.\nSpatial autocorrelation (Moran’s I) cannot be calculated for species occupying 100% of the available area in a study region, thus resulting in NA. These species are removed completely from the model as there is no way to impute this value.\nThe resulting data contains NAs in rows for 3 species which recently split from their sister clades and for which no data was available. We will use knn-nearest neighbor imputation (k = 5) to being able to predict risks for these species as well. Since it’s only 3 species, I could have easily removed these species from the model without performance decreases, but my overarching goal was the risk assessment for each species in the data. Thus losing species was not acceptable for me here and I continued with imputation of these few cells that contained NAs. Also, I’ll exclude some predictors which are overlapping with some others within the process_data() function.\n\n3.2.1 1. Site Turnover (Jaccard):\n\nfile_path &lt;- \"data/AllPredictors.rds\"\n\n\nJaccard 1. Sampling periodJaccard 2. Sampling periodDistribution\n\n\n\nresponse &lt;- \"Jaccard\"\nvars &lt;- c(H1_vars, H2_vars, H3_vars, H4_vars)\ntp_value &lt;- 1\n\n# Function to process the data\ndat_J1 &lt;- process_data(file_path, tp_value, response, vars)\n\n# Check NAs\nsummarize_NA(dat_J1)\n\n\n\n\nVariable\nNA_Count\n\n\n\n\nsd_PC1\n2\n\n\nsd_PC2\n2\n\n\nGlobRangeSize_m2\n2\n\n\nIUCN\n3\n\n\nMass\n3\n\n\nHabitat\n3\n\n\nHabitat.Density\n3\n\n\nMigration\n3\n\n\nTrophic.Level\n3\n\n\nTrophic.Niche\n3\n\n\nPrimary.Lifestyle\n3\n\n\nHand.Wing.Index\n3\n\n\nmean_prob_cooccur\n3\n\n\n\n\n\n\n\n\nresponse &lt;- \"Jaccard\"\nvars &lt;- c(H1_vars, H2_vars, H3_vars, H4_vars)\ntp_value &lt;- 2\n\n# Function to process the data\ndat_J2 &lt;- process_data(file_path, tp_value, response, vars)\n\n# Check NAs\nsummarize_NA(dat_J2)\n\n\n\n\nVariable\nNA_Count\n\n\n\n\nsd_PC1\n2\n\n\nsd_PC2\n2\n\n\nGlobRangeSize_m2\n2\n\n\nIUCN\n3\n\n\nMass\n3\n\n\nHabitat\n3\n\n\nHabitat.Density\n3\n\n\nMigration\n3\n\n\nTrophic.Level\n3\n\n\nTrophic.Niche\n3\n\n\nPrimary.Lifestyle\n3\n\n\nHand.Wing.Index\n3\n\n\nmean_prob_cooccur\n3\n\n\n\n\n\n\n\nThe distribution shows that our data spans the full range of Jaccard 0 to 1 and (besides Europe), the variable seems to be distributed uniformly. This can be advantageous for prediction modeling as most events are equally likely and data partitioning will most probably not bias the training data towards a certain pattern.\n\n# Plot response distribution\ndat_J1 %&gt;%\n    select(Jaccard, dataset) %&gt;%\n    melt(id.vars = \"dataset\") %&gt;%\n    ggplot(aes(x = value, fill = dataset)) +\n    geom_histogram(bins = 30, color = \"black\") +\n    facet_wrap(~variable, scales = \"free_x\") +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#009E73\", \"#D55E00\")) +\n    theme_bw() +\n    labs(\n        title = \"Species-level Jaccard index of site-similarity\",\n        x = \"Jaccard\",\n        y = \"Frequency\"\n    ) +\n    facet_wrap(dataset ~ .)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 2. Area change (log Ratio):\n\nLog Ratio 1. Sampling periodLog Ratio 2. Sampling periodDistribution\n\n\n\nresponse &lt;- \"log_R2_1\"\nvars &lt;- c(H1_vars, H2_vars, H3_vars, H4_vars)\ntp_value &lt;- 1\n\n# Function to process the data\ndat_LR1 &lt;- process_data(file_path, tp_value, response, vars)\n\n# Check NAs\nsummarize_NA(dat_LR1)\n\n\n\n\nVariable\nNA_Count\n\n\n\n\nsd_PC1\n2\n\n\nsd_PC2\n2\n\n\nGlobRangeSize_m2\n2\n\n\nIUCN\n3\n\n\nMass\n3\n\n\nHabitat\n3\n\n\nHabitat.Density\n3\n\n\nMigration\n3\n\n\nTrophic.Level\n3\n\n\nTrophic.Niche\n3\n\n\nPrimary.Lifestyle\n3\n\n\nHand.Wing.Index\n3\n\n\nmean_prob_cooccur\n3\n\n\n\n\n\n\n\n\nresponse &lt;- \"log_R2_1\"\nvars &lt;- c(H1_vars, H2_vars, H3_vars, H4_vars)\ntp_value &lt;- 2\n\n# Function to process the data\ndat_LR2 &lt;- process_data(file_path, tp_value, response, vars)\n\n# Check NAs\nsummarize_NA(dat_LR2)\n\n\n\n\nVariable\nNA_Count\n\n\n\n\nsd_PC1\n2\n\n\nsd_PC2\n2\n\n\nGlobRangeSize_m2\n2\n\n\nIUCN\n3\n\n\nMass\n3\n\n\nHabitat\n3\n\n\nHabitat.Density\n3\n\n\nMigration\n3\n\n\nTrophic.Level\n3\n\n\nTrophic.Niche\n3\n\n\nPrimary.Lifestyle\n3\n\n\nHand.Wing.Index\n3\n\n\nmean_prob_cooccur\n3\n\n\n\n\n\n\n\nWe can see that the response variable log Ratio has little variation and is mainly distributed around 0. This may be a sign of weak signal of temporal change in this variable, and thus lead to low predictive performance.\n\n# Plot response distribution\ndat_LR1 %&gt;%\n    select(log_R2_1, dataset) %&gt;%\n    melt(id.vars = \"dataset\") %&gt;%\n    ggplot(aes(x = value, fill = dataset)) +\n    geom_histogram(bins = 30, color = \"black\") +\n    facet_wrap(~variable, scales = \"free_x\") +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#009E73\", \"#D55E00\")) +\n    theme_bw() +\n    labs(\n        title = \"Species-level change in AOO\",\n        x = \"Log Ratio\",\n        y = \"Frequency\"\n    ) +\n    facet_wrap(dataset ~ .)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Relationships between variables\nThe feature plots show how the variables for each hypothesis are related to each other and to the response variable. We can see that some relationships follow distinct patterns which suggests a correlation between variables. We will check this more specifically below and remove any variables that are correlated more than a certain threshold.\n\nFeature plot: H1Feature plot: H2Feature plot: H3Feature plot: H4\n\n\n\nfeaturePlot(x = dat_J1 %&gt;% select(dataset, all_of(H1_vars)),\n    y = dat_J1$Jaccard,\n    group = dat_J1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species traits (H1) - Jaccard 1\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J2 %&gt;% select(dataset, all_of(H1_vars)),\n    y = dat_J2$Jaccard,\n    group = dat_J2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species traits (H1) - Jaccard 2\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR1 %&gt;% select(dataset, all_of(H1_vars)),\n    y = dat_LR1$log_R2_1,\n    group = dat_LR1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species traits (H1) - Log ratio 1\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR2 %&gt;% select(dataset, all_of(H1_vars)),\n    y = dat_LR2$log_R2_1,\n    group = dat_LR2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species traits (H1) - log Ratio 2\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J1 %&gt;% select(dataset, all_of(H2_vars)),\n    y = dat_J1$Jaccard,\n    group = dat_J1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species geometry (H2) - Jaccard 1\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J2 %&gt;% select(dataset, all_of(H2_vars)),\n    y = dat_J2$Jaccard,\n    group = dat_J2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species geometry (H2) - Jaccard 2\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR1 %&gt;% select(dataset, all_of(H2_vars)),\n    y = dat_LR1$log_R2_1,\n    group = dat_LR1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species geometry (H2) - log Ratio 1\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR2 %&gt;% select(dataset, all_of(H2_vars)),\n    y = dat_LR2$log_R2_1,\n    group = dat_LR2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.2,\n    xlab = \"Scatterplot Matrix of species geometry (H2) - log Ratio 2\",\n    par.settings =\n        list(fontsize = list(text = 4)))\n\n\n\n\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J1 %&gt;% select(dataset, all_of(H3_vars)),\n    y = dat_J1$Jaccard,\n    group = dat_J1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of diversity metrics (H3) - Jaccard 1\",\n    par.settings =\n        list(fontsize = list(text = 6)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J2 %&gt;% select(dataset, all_of(H3_vars)),\n    y = dat_J2$Jaccard,\n    group = dat_J2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of diversity metrics (H3) - Jaccard 2\",\n    par.settings =\n        list(fontsize = list(text = 6)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR1 %&gt;% select(dataset, all_of(H3_vars)),\n    y = dat_LR1$log_R2_1,\n    group = dat_LR1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of diversity metrics (H3) - log Ratio 1\",\n    par.settings =\n        list(fontsize = list(text = 6)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR2 %&gt;% select(dataset, all_of(H3_vars)),\n    y = dat_LR2$log_R2_1,\n    group = dat_LR2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.3,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of diversity metrics (H3) - log Ratio 1\",\n    par.settings =\n        list(fontsize = list(text = 6)))\n\n\n\n\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J1 %&gt;% select(dataset, all_of(H4_vars)),\n    y = dat_J1$Jaccard,\n    group = dat_J1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.6,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of atlas specifics (H4) - Jaccard 2\",\n    par.settings =\n        list(fontsize = list(text = 5)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_J2 %&gt;% select(dataset, all_of(H4_vars)),\n    y = dat_J2$Jaccard,\n    group = dat_J2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.6,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of atlas specifics (H4) - Jaccard 2\",\n    par.settings =\n        list(fontsize = list(text = 5)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR1 %&gt;% select(dataset, all_of(H4_vars)),\n    y = dat_LR1$log_R2_1,\n    group = dat_LR1$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.6,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of atlas specifics (H4) - log Ratio 1\",\n    par.settings =\n        list(fontsize = list(text = 5)))\n\n\n\n\n\n\n\nfeaturePlot(x = dat_LR2 %&gt;% select(dataset, all_of(H4_vars)),\n    y = dat_LR2$log_R2_1,\n    group = dat_LR2$dataset,\n    plot = \"pairs\",\n    pch = 16,\n    alpha = 0.6,\n    cex = 0.5,\n    xlab = \"Scatterplot Matrix of atlas specifics (H4) - log Ratio 2\",\n    par.settings =\n        list(fontsize = list(text = 5)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 Correlation Matrix\nNext, we will check how correlated the predictors are. Those will be automatically excluded using recipes below in: Model fitting &gt; Data pre-processing\n\nSampling periods 1: 17 correlated variables\nSampling periods 2: 18 correlated variables\n\n\nPredictor correlations for Sampling Period 1Predictor correlations of Sampling period 2\n\n\n\ncor_df &lt;- dat_J1 %&gt;% select(-Jaccard) # does not matter which tp = 1 data we look at\np.mat &lt;- model.matrix(~ 0 + ., data = cor_df) %&gt;%\n    cor_pmat()\n\ncorrelation_matrix &lt;- cor_df %&gt;%\n    select_if(is.numeric) %&gt;%\n    cor(use = \"pairwise.complete.obs\")\ncorrelation_matrix %&gt;%\n    ggcorrplot(\n        hc.order = TRUE,\n        lab = TRUE,\n        lab_size = 3,\n        p.mat = p.mat,\n        insig = \"blank\"\n    )\n\n\n\n\n\n\n\n# We will set the threshold for excluding correlations = 0.85\n# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity\n\ncor_vars &lt;- findCorrelation(correlation_matrix,\n    cutoff = .85,\n    names = TRUE,\n    exact = TRUE,\n    verbose = FALSE)\n\n# cor_vars # 17 variables seemed to be highly correlated. We will exclude\n\n\n\n\ncor_df &lt;- dat_J2 %&gt;% select(-Jaccard)\np.mat &lt;- model.matrix(~ 0 + ., data = cor_df) %&gt;%\n    cor_pmat()\n\ncorrelation_matrix &lt;- cor_df %&gt;%\n    select_if(is.numeric) %&gt;%\n    cor(use = \"pairwise.complete.obs\")\ncorrelation_matrix %&gt;%\n    ggcorrplot(\n        hc.order = TRUE,\n        lab = TRUE,\n        lab_size = 3,\n        p.mat = p.mat,\n        insig = \"blank\"\n    )\n\n\n\n\n\n\n\n# We will set the threshold for excluding correlations = 0.85\n# this is a bit arbitrary, trying to find a good trade-off between loss of predictor variables and collinearity\n\ncor_vars &lt;- findCorrelation(correlation_matrix,\n    cutoff = .85,\n    names = TRUE,\n    exact = TRUE,\n    verbose = FALSE)\n\n# cor_vars # 18 variables seemed to be highly correlated. We will exclude",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Script 1 - Data preparation</span>"
    ]
  },
  {
    "objectID": "01_DataPrep.html#data-pre-processing",
    "href": "01_DataPrep.html#data-pre-processing",
    "title": "1  Script 1 - Data preparation",
    "section": "4.1 Data pre-processing:",
    "text": "4.1 Data pre-processing:\n\nFirst we have to check if there are (near) zero variance variables in the predictors. These can be removed since they will not explain a lot generally.\n\n\n# Step 1. Near Zero Vars\nrbind(\nnearZeroVar(dat_J1, saveMetrics = T) %&gt;% filter(nzv == T),\nnearZeroVar(dat_J2, saveMetrics = T) %&gt;% filter(nzv == T),\nnearZeroVar(dat_LR1, saveMetrics = T) %&gt;% filter(nzv == T),\nnearZeroVar(dat_LR2, saveMetrics = T) %&gt;% filter(nzv == T)) %&gt;% \n  kableExtra::kable()\n\n\n\n\n\nfreqRatio\npercentUnique\nzeroVar\nnzv\n\n\n\n\nIUCN\n19.4375\n0.4854369\nFALSE\nTRUE\n\n\nIUCN1\n19.3750\n0.4868549\nFALSE\nTRUE\n\n\nIUCN2\n19.4375\n0.4854369\nFALSE\nTRUE\n\n\nIUCN3\n19.3750\n0.4868549\nFALSE\nTRUE\n\n\n\n\n# only IUCN, but this is an important predictor (!) we will keep it.\n\n\nSecond, we will exclude all correlated variables with pearson’s pairwise correlations coefficients r &gt; 0.85.\nThird, we will impute NA values based on knn-imputation with 5 neighbors (default). We will do both steps (2 & 3) at once using recipes.\nWe could have included the Near Zero Variable check in the recipe as well, however I wanted to have more control about which variables should be included. In this case, the near zero variance predictor IUCN status should be included into the model that assesses the risk of a species to undergo change.\n\n\nJaccard - sampling period 1Jaccard - sampling period 2Log Ratio - sampling period 1Log Ratio - sampling period 2\n\n\n\n# Step 2 & 3: imputing missing values & removing highly correlated variables\nrecipe_pp_J1 &lt;- recipe(Jaccard ~ .,\n    data = dat_J1) %&gt;%\n    step_corr(all_numeric_predictors(), threshold = .85) %&gt;%\n    step_impute_knn(all_predictors())\n\n# Estimate recipe on data:\nrecipe_pp_prepped_J1 &lt;- prep(recipe_pp_J1, dat_J1)\n\n# Removed columns:\nrecipe_pp_prepped_J1$steps[[1]]$removals\n\n [1] \"rel_ewDist\"              \"rel_nsDist\"             \n [3] \"rel_circNorm\"            \"maxDist_toBorder_border\"\n [5] \"maxDist_toBorder_centr\"  \"GammaSR\"                \n [7] \"mean_area\"               \"Total_area_samp\"        \n [9] \"mean_cell_length\"        \"atlas_lengthMinRect\"    \n[11] \"atlas_elonMinRect\"       \"atlas_bearing\"          \n[13] \"AtlasCOG_long\"           \"AtlasCOG_lat\"           \n[15] \"lengthMinRect\"           \"Total_Ncells_samp\"      \n[17] \"atlas_circ\"             \n\n# apply the recipe to the data:\ndat_J1_v2 &lt;- bake(recipe_pp_prepped_J1, dat_J1)\n\n\n\n\n# Step 2 & 3: imputing missing values & removing highly correlated variables\nrecipe_pp_J2 &lt;- recipe(Jaccard ~ .,\n    data = dat_J2) %&gt;%\n    step_corr(all_numeric_predictors(), threshold = .85) %&gt;%\n    step_impute_knn(all_predictors())\n\n# Estimate recipe on data:\nrecipe_pp_prepped_J2 &lt;- prep(recipe_pp_J2, dat_J2)\n\n# Removed columns:\nrecipe_pp_prepped_J2$steps[[1]]$removals\n\n [1] \"mean_prob_cooccur\"       \"rel_ewDist\"             \n [3] \"rel_nsDist\"              \"rel_circNorm\"           \n [5] \"maxDist_toBorder_border\" \"maxDist_toBorder_centr\" \n [7] \"GammaSR\"                 \"mean_area\"              \n [9] \"Total_area_samp\"         \"mean_cell_length\"       \n[11] \"atlas_lengthMinRect\"     \"atlas_elonMinRect\"      \n[13] \"atlas_bearing\"           \"AtlasCOG_long\"          \n[15] \"AtlasCOG_lat\"            \"lengthMinRect\"          \n[17] \"Total_Ncells_samp\"       \"atlas_circ\"             \n\n# apply the recipe to the data:\ndat_J2_v2 &lt;- bake(recipe_pp_prepped_J2, dat_J2)\n\n\n\n\n# Step 2 & 3: imputing missing values & removing highly correlated variables\nrecipe_pp_LR1 &lt;- recipe(log_R2_1 ~ .,\n    data = dat_LR1) %&gt;%\n    step_corr(all_numeric_predictors(), threshold = .85) %&gt;%\n    step_impute_knn(all_predictors())\n\n# Estimate recipe on data:\nrecipe_pp_prepped_LR1 &lt;- prep(recipe_pp_LR1, dat_LR1)\n\n# Removed columns:\nrecipe_pp_prepped_LR1$steps[[1]]$removals\n\n [1] \"rel_ewDist\"              \"rel_nsDist\"             \n [3] \"rel_circNorm\"            \"maxDist_toBorder_border\"\n [5] \"maxDist_toBorder_centr\"  \"GammaSR\"                \n [7] \"mean_area\"               \"Total_area_samp\"        \n [9] \"mean_cell_length\"        \"atlas_lengthMinRect\"    \n[11] \"atlas_elonMinRect\"       \"atlas_bearing\"          \n[13] \"AtlasCOG_long\"           \"AtlasCOG_lat\"           \n[15] \"lengthMinRect\"           \"Total_Ncells_samp\"      \n[17] \"atlas_circ\"             \n\n# apply the recipe to the data:\ndat_LR1_v2 &lt;- bake(recipe_pp_prepped_LR1, dat_LR1)\n\n\n\n\n# Step 2 & 3: imputing missing values & removing highly correlated variables\nrecipe_pp_LR2 &lt;- recipe(log_R2_1 ~ .,\n    data = dat_LR2) %&gt;%\n    step_corr(all_numeric_predictors(), threshold = .85) %&gt;%\n    step_impute_knn(all_predictors())\n\n# Estimate recipe on data:\nrecipe_pp_prepped_LR2 &lt;- prep(recipe_pp_LR2, dat_LR2)\n\n# Removed columns:\nrecipe_pp_prepped_LR2$steps[[1]]$removals\n\n [1] \"mean_prob_cooccur\"       \"rel_ewDist\"             \n [3] \"rel_nsDist\"              \"rel_circNorm\"           \n [5] \"maxDist_toBorder_border\" \"maxDist_toBorder_centr\" \n [7] \"GammaSR\"                 \"mean_area\"              \n [9] \"Total_area_samp\"         \"mean_cell_length\"       \n[11] \"atlas_lengthMinRect\"     \"atlas_elonMinRect\"      \n[13] \"atlas_bearing\"           \"AtlasCOG_long\"          \n[15] \"AtlasCOG_lat\"            \"lengthMinRect\"          \n[17] \"Total_Ncells_samp\"       \"atlas_circ\"             \n\n# apply the recipe to the data:\ndat_LR2_v2 &lt;- bake(recipe_pp_prepped_LR2, dat_LR2)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Script 1 - Data preparation</span>"
    ]
  },
  {
    "objectID": "01_DataPrep.html#training-validation-sets",
    "href": "01_DataPrep.html#training-validation-sets",
    "title": "1  Script 1 - Data preparation",
    "section": "4.2 Training & Validation sets:",
    "text": "4.2 Training & Validation sets:\nNow we will split the data into training, testing and validation sets. We will do an initial split to exclude some data completely from the training set. This subset will be used in the end to evaluate predictive performance on data that was never used to train the model.\nThen we will create a list of indices for 10 resamples of splits of the training data for internal validation of the models.\n\nJaccard - Sampling Period 1Jaccard - Sampling Period 2Log Ratio - Sampling Period 1Log Ratio - Sampling Period 2\n\n\n\nset.seed(42)\n# Initial split to training and validation set (for final evaluation, keep dat_test completely out of the training sets)\nindex_J1 &lt;- createDataPartition(dat_J1_v2$Jaccard, p = 0.8, 1, list = FALSE)\n\ndat_train_J1 &lt;- dat_J1_v2[index_J1, ]\ndat_test_J1 &lt;- dat_J1_v2[-index_J1, ]\n\n# Cross-validation resampling indices \nindices_J1 &lt;- createDataPartition(dat_train_J1$Jaccard, p = 0.8, 10) # 10 resamples\n\n\n\n\nset.seed(42)\n# Initial split to training and validation set (for final evaluation, keep dat_test completely out of the training sets)\nindex_J2 &lt;- createDataPartition(dat_J2_v2$Jaccard, p = 0.8, 1, list = FALSE)\n\ndat_train_J2 &lt;- dat_J2_v2[index_J2, ]\ndat_test_J2 &lt;- dat_J2_v2[-index_J2, ]\n\n# Cross-validation resampling indices \nindices_J2 &lt;- createDataPartition(dat_train_J2$Jaccard, p = 0.8, 10) # 10 resamples\n\n\n\n\nset.seed(42)\n# Initial split to training and validation set (for final evaluation, keep dat_test completely out of the training sets)\nindex_LR1 &lt;- createDataPartition(dat_LR1_v2$log_R2_1, p = 0.8, 1, list = FALSE)\n\ndat_train_LR1 &lt;- dat_LR1_v2[index_LR1, ]\ndat_test_LR1 &lt;- dat_LR1_v2[-index_LR1, ]\n\n# Cross-validation resampling indices \nindices_LR1 &lt;- createDataPartition(dat_train_LR1$log_R2_1, p = 0.8, 10) # 10 resamples\n\n\n\n\nset.seed(42)\n# Initial split to training and validation set (for final evaluation, keep dat_test completely out of the training sets)\nindex_LR2 &lt;- createDataPartition(dat_LR2_v2$log_R2_1, p = 0.8, 1, list = FALSE)\n\ndat_train_LR2 &lt;- dat_LR2_v2[index_LR2, ]\ndat_test_LR2 &lt;- dat_LR2_v2[-index_LR2, ]\n\n# Cross-validation resampling indices \nindices_LR2 &lt;- createDataPartition(dat_train_LR2$log_R2_1, p = 0.8, 10) # 10 resamples\n\n\n\n\n\nsave.image(\"data/RData/01_Data_prep.RData\")\n\n\n\n\n\nBirdLife International. 2020. “BirdLife International Datazone.” http://datazone.birdlife.org/.\n\n\nCouncil, European Bird Census. 2022. “European Breeding Bird Atlas 2 Website.” http://ebba2.info.\n\n\nHagemeijer, W. J. M., and M. J. Blair. 1997. The EBCC Atlas of European Breeding Birds: Their Distribution and Abundance. London: T. & A.D. Poyser.\n\n\nHagemeyer, W., M. Blair, and W. Loos. 2016. “EBCC Atlas of European Breeding Birds.” https://doi.org/10.15468/adtfvf.\n\n\nKarger, Dirk N., Olaf Conrad, Jürgen Böhner, Thomas Kawohl, Holger Kreft, Rodrigo W. Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017a. “Climatologies at High Resolution for the Earth Land Surface Areas.” Scientific Data 4: 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\n———. 2017b. “Data from: Climatologies at High Resolution for the Earth’s Land Surface Areas.” Dryad Digital Repository. https://doi.org/10.5061/dryad.kd1d4.\n\n\nKeller, V., S. Herrando, P. Voříšek, M. Franch, M. Kipson, P. Milanesi, D. Martí, et al. 2020. European Breeding Bird Atlas 2: Distribution, Abundance and Change. Barcelona: European Bird Census Council & Lynx Edicions.\n\n\nMinistry of the Environment, Natural Environment Bureau, Biodiversity Center. 2004. Bird Breeding Distribution Survey Report. Fujiyoshida City: Ministry of the Environment, Natural Environment Bureau, Biodiversity Center.\n\n\nNew York State Department of Environmental Conservation. 1980-1985. “New York State Breeding Bird Atlas.” https://www.dec.ny.gov/animals/7312.html.\n\n\n———. 2000-2005. “New York State Breeding Bird Atlas 2000.” https://www.dec.ny.gov/animals/7312.html.\n\n\nŠťastný, Karel, Vladimír Bejček, and Karel Hudec. 1997. Atlas Hnízdního Rozšíření Ptáků v České Republice 1985-1989. Jinočany: Nakladatelství a vydavatelství H&H.\n\n\n———. 2006. Atlas Hnízdního Rozšíření Ptáků v České Republice: 2001-2003. Praha: Aventinum.\n\n\nTobias, Joseph A., Catherine Sheard, Alex L. Pigot, Adam J. M. Devenish, Jingyi Yang, Ferran Sayol, Montague H. C. Neate‐Clegg, et al. 2022. “AVONET: Morphological, Ecological and Geographical Data for All Birds.” Edited by T. Coulson. Ecology Letters 25 (3): 581–97. https://doi.org/10.1111/ele.13898.\n\n\nUeda, Mutsuyuki, and Shingo Uemura. 2021. National Bird Breeding Distribution Survey Report Let’s Describe the Current Status of Japanese Birds 2016-2021. Fuchu City: Bird Breeding Distribution Research Committee.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Script 1 - Data preparation</span>"
    ]
  },
  {
    "objectID": "02_rfe.html",
    "href": "02_rfe.html",
    "title": "2  Script 2 - Recursive Feature Selection",
    "section": "",
    "text": "3 Recursive feature elimination\nI will use recursive feature elimination to reduce the dimensionality of the data by removing variables that do not lead to an increase of model performance when included. I will apply this method, because most of my predictors were calculated from the same data and are thus not independent. Although checking for high correlations in one of the previous steps, any correlations between predictor variables may confuse the model during variation partitioning, as correlations make it impossible to discern which variable explains how much of the variation. This also reduces the probability of overfitting the model to the data as redundant features with correlated noise signals are removed.\nThe method being used relies on the randomForest package to recursively eliminate one predictor after another from the model, calculate the variable importance, rank these, average the importance across resamples and comparing the fit across models with different subsets of the set of predictors. The workflow is set in the caret helper function rfFuncs().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Script 2 - Recursive Feature Selection</span>"
    ]
  },
  {
    "objectID": "02_rfe.html#jaccard-2",
    "href": "02_rfe.html#jaccard-2",
    "title": "2  Script 2 - Recursive Feature Selection",
    "section": "4.1 Jaccard 2",
    "text": "4.1 Jaccard 2\n\nJ2_vars &lt;- wide %&gt;% filter(Include_J2 == 1) %&gt;% pull(var)\nresponse &lt;- \"Jaccard\"\nindices &lt;- indices_J2\ndat_train &lt;- dat_train_J2\n\n\n# Define training control ==========================================================\ntrainControl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n  ## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_full &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nset.seed(42)\ntictoc::tic(\"ranger\")\nJ2_reduced &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %&gt;% select(response, all_of(J2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n\nJ2_full$finalModel\nJ2_full$results\nJ2_reduced$finalModel\nJ2_reduced$results",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Script 2 - Recursive Feature Selection</span>"
    ]
  },
  {
    "objectID": "02_rfe.html#log-ratio-1",
    "href": "02_rfe.html#log-ratio-1",
    "title": "2  Script 2 - Recursive Feature Selection",
    "section": "4.2 Log Ratio 1",
    "text": "4.2 Log Ratio 1\n\nLR1_vars &lt;- wide %&gt;% filter(Include_LR1 == 1) %&gt;% pull(var)\nresponse &lt;- \"log_R2_1\"\nindices &lt;- indices_LR1\ndat_train &lt;- dat_train_LR1\n\n\n# Define training control ==========================================================\ntrainControl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_full &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR1_reduced &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %&gt;% select(response, all_of(LR1_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n\ntictoc::toc()\nLR1_full$finalModel\nLR1_full$results\nLR1_reduced$finalModel\nLR1_reduced$results",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Script 2 - Recursive Feature Selection</span>"
    ]
  },
  {
    "objectID": "02_rfe.html#log-ratio-2",
    "href": "02_rfe.html#log-ratio-2",
    "title": "2  Script 2 - Recursive Feature Selection",
    "section": "4.3 Log Ratio 2",
    "text": "4.3 Log Ratio 2\n\nLR2_vars &lt;- wide %&gt;% filter(Include_LR2 == 1) %&gt;% pull(var)\nresponse &lt;- \"log_R2_1\"\nindices &lt;- indices_LR2\ndat_train &lt;- dat_train_LR2\n\n\n# Define training control ==========================================================\ntrainControl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"all\",\n  verboseIter = FALSE,\n  index = indices)\n\n## Train ranger model ==========================================================\nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_full &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train,\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\ntictoc::toc()\n  \n  \n  \nset.seed(42)\ntictoc::tic(\"ranger\")\nLR2_reduced &lt;- train(\n  as.formula(paste(response, \"~ .\")),\n  data = dat_train %&gt;% select(response, all_of(LR2_vars)),\n  method = \"ranger\",\n  trControl = trainControl,\n  importance = \"permutation\",\n  scale.permutation.importance = TRUE,\n  num.trees = 5000,\n  respect.unordered.factors = TRUE,\n  oob.error = TRUE,\n  tuneLength = 20)\n  \ntictoc::toc()\n  \nLR2_full$finalModel\nLR2_full$results\nLR2_reduced$finalModel\nLR2_reduced$results\n\n:::\n\n# save.image(\"data/RData/02_rfe_full_vs_reduced.RData\")\n# selected_predictors &lt;- list(J1_vars, J2_vars, LR1_vars, LR2_vars)\n# saveRDS(selected_predictors, \"data/rds/selected_predictors_list.rds\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Script 2 - Recursive Feature Selection</span>"
    ]
  },
  {
    "objectID": "03_HyperparameterTuning.html",
    "href": "03_HyperparameterTuning.html",
    "title": "3  Script 3 - Hyperparameter Tuning",
    "section": "",
    "text": "Source custom functionsMachineLearning packagesLoad RData/RDS objects to reduce computing time\n\n\n\nrm(list = ls())\nsource(\"src/functions.R\")\n\n\n\n\npckgs &lt;- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"caret\", \"caretEnsemble\", \n           \"randomForest\", \"ranger\", \"gbm\", \"xgboost\", \n           \"gridExtra\", \"kableExtra\")\n\n\ninstall_and_load(pckgs)\n\n\n\n\n# Load workspace to save computing time:\nload(\"data/RData/03_reduced_hyper_para_tuning_all_models.RData\")\n\n# make list of models for evaluation ========\nsaved_models &lt;- list(Jaccard1 = models_J1, \n                   Jaccard2 = models_J2, \n                   LogRatio1 = models_LR1, \n                   LogRatio2 = models_LR2)\n\n# saveRDS(saved_models, \"data/models/reducedModels/03_List_all_reduced_models.rds\")\n\n# List of variables to keep in each model\nreduced_predictors &lt;- readRDS(\"data/rds/selected_predictors_list.rds\")\n\n\nH1_vars &lt;- c(\n    \"sd_PC1\", \"sd_PC2\", # Climatic Niche Breadth\n    \"GlobRangeSize_m2\", \"IUCN\", \"Mass\", \"Habitat\", \"Habitat.Density\",\n    \"Migration\", \"Trophic.Level\", \"Trophic.Niche\", \"Primary.Lifestyle\",\n    \"FP\", # Phylogenetic Distinctness\n    \"Hand.Wing.Index\") # Measure of dispersal ability\nH2_vars &lt;- c(\n    \"AOO\", \"rel_occ_Ncells\", \"mean_prob_cooccur\", \"D_AOO_a\", \n    \"moran\", \"x_intercept\", \"sp_centr_lon\", \"sp_centr_lat\",\n    \"lengthMinRect\", \"widthMinRect\", \"elonMinRect\", \"bearingMinRect\",\n    \"circ\", \"bearing\", \"Southernness\", \"Westernness\",\n    \"rel_maxDist\", \"rel_ewDist\", \"rel_nsDist\", \"rel_elonRatio\",\n    \"rel_relCirc\", \"rel_circNorm\", \"rel_lin\", \"Dist_centroid_to_COG\",\n    \"maxDist_toBorder_border\", \"maxDist_toBorder_centr\",\n    \"minDist_toBorder_centr\")\nH3_vars &lt;- c(\"GammaSR\", \"AlphaSR_sp\", \"BetaSR_sp\")\nH4_vars &lt;- c(\n    \"dataset\", \"mean_area\", \"Total_area_samp\", \"Total_Ncells_samp\",\n    \"mean_cell_length\", \"atlas_lengthMinRect\", \"atlas_widthMinRect\",\n    \"atlas_elonMinRect\", \"atlas_circ\", \"atlas_bearingMinRect\",\n    \"atlas_bearing\", \"AtlasCOG_long\", \"AtlasCOG_lat\")\n\n\n\n\n\n3.0.1 Individual models\n\n3.0.1.1 Hyperparameter tuning\n\n\nCode\nindex_list &lt;- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list &lt;- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\n\nsaved_models &lt;- replicate(4, list())\nnames(saved_models) &lt;- c(\"J1\", \"J2\", \"LR1\", \"LR2\")\n\nresponse_list &lt;- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nfor(j in c(1:4)){\n  ## Loop through differet datasets/Analyses\n  indices &lt;- index_list[[j]]\n  response &lt;- response_list[[j]] \n  predictors &lt;- rownames(reduced_predictors[[j]])\n  \n  # Subset the data\n  dat_train &lt;- dat_train_list[[j]] %&gt;% \n                    select(any_of(c(response,predictors)))\n\n  \n  # Define training control ==========================================================\n  trainControl &lt;- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"all\",\n    verboseIter = FALSE,\n    index = indices)\n\n  ## Train ranger model ==========================================================\n  set.seed(42)\n  tictoc::tic(\"ranger\")\n  rangerModel_t &lt;- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 20)\n  \n  saveRDS(rangerModel_t, paste0(\"data/models/reducedModels/03_reduced_rangerModel_\", response, j, \"all.rds\"))\n  tictoc::toc()\n  \n  # rangerModel_t &lt;- readRDS( paste0(\"data/models/reducedModels/rangerModel_\", response, j, \"all.rds\"))\n\n  ### Model results:\n  p_rangerModel &lt;- plot(rangerModel_t)\n  p_rangerModel\n  rangerModel_t$finalModel\n\n  ## Train xgbTree model ==========================================================\n  xgb_grid &lt;- expand.grid(\n    nrounds = c(1000),\n    eta = c(0.1, 0.3),\n    max_depth = c(2,3, 5),\n    gamma = c(0, 0.01, 0.1),\n    colsample_bytree = 0.6,\n    min_child_weight = 1,\n    subsample =1)\n  \n  tictoc::tic(\"xgb\")\n  set.seed(42)\n  xgbModel_t &lt;- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"xgbTree\",\n    trControl = trainControl,\n    tuneGrid = xgb_grid)\n  saveRDS(xgbModel_t, paste0(\"data/models/reducedModels/03_reduced_xgbModel_all_\", response, j, \"TLCUSTOM.rds\"))\n  tictoc::toc()\n  \n  # xgbModel_t &lt;- readRDS(paste0(\"data/models/reducedModels/xgbModel_all_\", response, j, \"TLCUSTOM.rds\"))\n\n  ### Model results:\n  p_xgbModel &lt;- plot(xgbModel_t)\n  p_xgbModel\n  slice_min(xgbModel_t$results, RMSE)\n  slice_max(xgbModel_t$results, Rsquared)\n\n  \n  ## Train gbm model ==========================================================\n  set.seed(42)\n  tictoc::tic(\"gbm\")\n  gbmModel_t &lt;- train(\n    as.formula(paste(response, \"~ .\")),\n    data = dat_train,\n    method = \"gbm\",\n    trControl = trainControl,\n    tuneLength= 20,\n    verbose = FALSE)\n  saveRDS(gbmModel_t, paste0(\"data/models/reducedModels/03_reduced_gbmModel_\", response, j, \"all.rds\"))\n  tictoc::toc()\n\n  ### Model results:\n  summary.gbm(gbmModel_t$finalModel)\n  p_gbmModel &lt;- plot(gbmModel_t)\n  p_gbmModel\n  gbmModel_t$finalModel\n  slice_min(gbmModel_t$results, RMSE)\n  slice_max(gbmModel_t$results, Rsquared)\n  \n  saved_models[[j]] &lt;- list(rangerModel_t, xgbModel_t, gbmModel_t)\n}\n\n# saveRDS(saved_models, \"data/models/reducedModels/03_List_all_reduced_models.rds\")\n# save.image(\"data/RData/03_reduced_hyper_para_tuning.RData\")\n\n\n\n\n3.0.1.2 Model Evaluation\nHere we will extract the model performance, best hyperparameters and variable importances from each model and compare them.\n\nJaccard 1Jaccard 2Log Ratio 1Log Ratio 2\n\n\n\n# reduced predictor models:\n\nranger_red &lt;- saved_models[[1]][[1]]\nxgb_red &lt;- saved_models[[1]][[2]]\ngbm_red &lt;- saved_models[[1]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      826 \nNumber of independent variables:  36 \nMtry:                             12 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01255428 \nR squared (OOB):                  0.842953 \n\nranger_red$results\n\n   mtry min.node.size  splitrule      RMSE  Rsquared        MAE      RMSESD\n1     2             5   variance 0.1254667 0.8251798 0.09432341 0.007866969\n2     2             5 extratrees 0.1433463 0.8069389 0.11376738 0.007243472\n3     3             5   variance 0.1162028 0.8378857 0.08264697 0.008493842\n4     3             5 extratrees 0.1239683 0.8257686 0.09142955 0.008905706\n5     5             5   variance 0.1127714 0.8431616 0.07726843 0.009281521\n6     5             5 extratrees 0.1158077 0.8377883 0.08107427 0.009436044\n7     7             5   variance 0.1120340 0.8441887 0.07558047 0.009731207\n8     7             5 extratrees 0.1139346 0.8406778 0.07817203 0.009799280\n9     9             5   variance 0.1114101 0.8454293 0.07451086 0.010060776\n10    9             5 extratrees 0.1131202 0.8419704 0.07666131 0.009829115\n11   10             5   variance 0.1113424 0.8454794 0.07416096 0.010251817\n12   10             5 extratrees 0.1127374 0.8427572 0.07612782 0.009882439\n13   12             5   variance 0.1111970 0.8455695 0.07364971 0.010845162\n14   12             5 extratrees 0.1125581 0.8428030 0.07552565 0.010047851\n15   14             5   variance 0.1112450 0.8452995 0.07329932 0.011082041\n16   14             5 extratrees 0.1123174 0.8431734 0.07502352 0.010198308\n17   16             5   variance 0.1114307 0.8446849 0.07299860 0.011160060\n18   16             5 extratrees 0.1122693 0.8431345 0.07470521 0.010309894\n19   18             5   variance 0.1113786 0.8447690 0.07278867 0.011339312\n20   18             5 extratrees 0.1122118 0.8431386 0.07441526 0.010351843\n21   19             5   variance 0.1116185 0.8440440 0.07272567 0.011550396\n22   19             5 extratrees 0.1120336 0.8435511 0.07421197 0.010503697\n23   21             5   variance 0.1117216 0.8437035 0.07259284 0.011623650\n24   21             5 extratrees 0.1122036 0.8429443 0.07407919 0.010530430\n25   23             5   variance 0.1118380 0.8432769 0.07255752 0.011993990\n26   23             5 extratrees 0.1120763 0.8432083 0.07382346 0.010593439\n27   25             5   variance 0.1120119 0.8428018 0.07248863 0.012029803\n28   25             5 extratrees 0.1121203 0.8430174 0.07370156 0.010599123\n29   27             5   variance 0.1121443 0.8423662 0.07247942 0.012261101\n30   27             5 extratrees 0.1121384 0.8429046 0.07356852 0.010666715\n31   28             5   variance 0.1122884 0.8419467 0.07247270 0.012323806\n32   28             5 extratrees 0.1120747 0.8430957 0.07348672 0.010652228\n33   30             5   variance 0.1125005 0.8413136 0.07253342 0.012482113\n34   30             5 extratrees 0.1121711 0.8427283 0.07340361 0.010788097\n35   32             5   variance 0.1126637 0.8408520 0.07256008 0.012454792\n36   32             5 extratrees 0.1123006 0.8423463 0.07337539 0.010748973\n37   34             5   variance 0.1129129 0.8401382 0.07268725 0.012689290\n38   34             5 extratrees 0.1122970 0.8422977 0.07328178 0.010836723\n39   36             5   variance 0.1131537 0.8393838 0.07281461 0.012812062\n40   36             5 extratrees 0.1122648 0.8423172 0.07314710 0.010951207\n   RsquaredSD       MAESD\n1  0.03055050 0.003974652\n2  0.03397330 0.004744941\n3  0.02879728 0.003775940\n4  0.03217047 0.005025291\n5  0.02906748 0.003851168\n6  0.03001990 0.004825038\n7  0.02956819 0.003892900\n8  0.03032316 0.004813523\n9  0.02989661 0.003836167\n10 0.02988290 0.004745772\n11 0.03028848 0.003914733\n12 0.02981438 0.004777269\n13 0.03184243 0.004084179\n14 0.03022429 0.004946745\n15 0.03248639 0.004079712\n16 0.03040113 0.004842249\n17 0.03259931 0.004031980\n18 0.03053069 0.004881325\n19 0.03300013 0.004127697\n20 0.03063005 0.004957603\n21 0.03370734 0.004120164\n22 0.03089294 0.005019679\n23 0.03388870 0.004159916\n24 0.03107748 0.005036566\n25 0.03502004 0.004190612\n26 0.03108341 0.005021662\n27 0.03496946 0.004085399\n28 0.03100310 0.005003647\n29 0.03573363 0.004072790\n30 0.03118679 0.004985610\n31 0.03593341 0.004161666\n32 0.03112147 0.004939953\n33 0.03632342 0.004105860\n34 0.03150875 0.005115531\n35 0.03633219 0.004116852\n36 0.03143579 0.005014301\n37 0.03703642 0.004177651\n38 0.03159180 0.005004110\n39 0.03744262 0.004201171\n40 0.03191157 0.004981865\n\nplot(ranger_red)\n\n\n\n\n\n\n\n## XGB ======\nplot(xgb_red)\n\n\n\n\n\n\n\nxgb_red\n\neXtreme Gradient Boosting \n\n826 samples\n 19 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE       \n  0.1  2          0.00   0.1118070  0.8446102  0.07369056\n  0.1  2          0.01   0.1102063  0.8478990  0.07321260\n  0.1  2          0.10   0.1194187  0.8236726  0.08266637\n  0.1  3          0.00   0.1097821  0.8490233  0.07146877\n  0.1  3          0.01   0.1105528  0.8472857  0.07279250\n  0.1  3          0.10   0.1137796  0.8394713  0.07826346\n  0.1  5          0.00   0.1096201  0.8498558  0.07206656\n  0.1  5          0.01   0.1101902  0.8484793  0.07238673\n  0.1  5          0.10   0.1130442  0.8417327  0.07777401\n  0.3  2          0.00   0.1141524  0.8374329  0.07623701\n  0.3  2          0.01   0.1133456  0.8401277  0.07609019\n  0.3  2          0.10   0.1199386  0.8209386  0.08380543\n  0.3  3          0.00   0.1133470  0.8409218  0.07640170\n  0.3  3          0.01   0.1126855  0.8421184  0.07477419\n  0.3  3          0.10   0.1159277  0.8326335  0.07964922\n  0.3  5          0.00   0.1144612  0.8368645  0.07534337\n  0.3  5          0.01   0.1129137  0.8408449  0.07452666\n  0.3  5          0.10   0.1179038  0.8257194  0.07989957\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample\n = 1.\n\n## GBM =====\ngbm_red$finalModel\n\nA gradient boosted model with gaussian loss function.\n300 iterations were performed.\nThere were 36 predictors of which 31 had non-zero influence.\n\ngbm_red\n\nStochastic Gradient Boosting \n\n826 samples\n 19 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE       \n   1                   50     0.1453374  0.7558990  0.11270542\n   1                  100     0.1289298  0.7968217  0.09436793\n   1                  150     0.1227746  0.8130044  0.08692107\n   1                  200     0.1211241  0.8176810  0.08455328\n   1                  250     0.1186376  0.8243122  0.08194397\n   1                  300     0.1175363  0.8275557  0.08088948\n   1                  350     0.1168408  0.8295444  0.08009465\n   1                  400     0.1166267  0.8302205  0.07976559\n   1                  450     0.1165562  0.8302532  0.07977221\n   1                  500     0.1163796  0.8305591  0.07940275\n   1                  550     0.1164298  0.8307084  0.07930831\n   1                  600     0.1166490  0.8301586  0.07916662\n   1                  650     0.1166229  0.8302922  0.07903669\n   1                  700     0.1164948  0.8306774  0.07876904\n   1                  750     0.1162696  0.8312881  0.07880899\n   1                  800     0.1162676  0.8315504  0.07858171\n   1                  850     0.1160615  0.8319353  0.07834910\n   1                  900     0.1163859  0.8311009  0.07839108\n   1                  950     0.1164178  0.8313684  0.07838938\n   1                 1000     0.1164582  0.8313685  0.07819886\n   2                   50     0.1247676  0.8109618  0.09000949\n   2                  100     0.1169673  0.8298555  0.08096768\n   2                  150     0.1155937  0.8330150  0.07900767\n   2                  200     0.1148729  0.8351281  0.07805390\n   2                  250     0.1137115  0.8385979  0.07690696\n   2                  300     0.1139379  0.8378319  0.07673802\n   2                  350     0.1138836  0.8381328  0.07656764\n   2                  400     0.1137209  0.8386412  0.07648715\n   2                  450     0.1139224  0.8382901  0.07645012\n   2                  500     0.1139598  0.8382803  0.07648368\n   2                  550     0.1142135  0.8378188  0.07650678\n   2                  600     0.1141230  0.8380017  0.07648719\n   2                  650     0.1142878  0.8377684  0.07660950\n   2                  700     0.1140709  0.8383336  0.07658041\n   2                  750     0.1139938  0.8385548  0.07644036\n   2                  800     0.1139313  0.8385619  0.07645051\n   2                  850     0.1137421  0.8391433  0.07649776\n   2                  900     0.1137695  0.8390701  0.07654395\n   2                  950     0.1137925  0.8388728  0.07649111\n   2                 1000     0.1140195  0.8385503  0.07666889\n   3                   50     0.1189724  0.8250200  0.08342629\n   3                  100     0.1140092  0.8376412  0.07759369\n   3                  150     0.1128369  0.8411181  0.07582167\n   3                  200     0.1125331  0.8417550  0.07544162\n   3                  250     0.1128486  0.8409744  0.07557854\n   3                  300     0.1122436  0.8426731  0.07525752\n   3                  350     0.1128212  0.8412139  0.07552856\n   3                  400     0.1127797  0.8412309  0.07538995\n   3                  450     0.1129368  0.8408223  0.07554079\n   3                  500     0.1129223  0.8407906  0.07549643\n   3                  550     0.1129996  0.8408311  0.07553183\n   3                  600     0.1131481  0.8404655  0.07567619\n   3                  650     0.1132707  0.8402120  0.07581446\n   3                  700     0.1131610  0.8405827  0.07568361\n   3                  750     0.1129443  0.8412505  0.07561206\n   3                  800     0.1130614  0.8409369  0.07566536\n   3                  850     0.1130207  0.8409924  0.07569383\n   3                  900     0.1131045  0.8407793  0.07578185\n   3                  950     0.1131567  0.8407358  0.07578853\n   3                 1000     0.1132802  0.8402919  0.07602095\n   4                   50     0.1154835  0.8341507  0.07837077\n   4                  100     0.1126572  0.8415089  0.07488959\n   4                  150     0.1116744  0.8445624  0.07379697\n   4                  200     0.1114789  0.8451936  0.07360589\n   4                  250     0.1113986  0.8452521  0.07353219\n   4                  300     0.1110988  0.8458559  0.07376527\n   4                  350     0.1108353  0.8468867  0.07378405\n   4                  400     0.1112453  0.8458057  0.07411103\n   4                  450     0.1114257  0.8455253  0.07424983\n   4                  500     0.1115432  0.8452468  0.07440587\n   4                  550     0.1117424  0.8447426  0.07464034\n   4                  600     0.1116771  0.8450425  0.07458276\n   4                  650     0.1121899  0.8436529  0.07485827\n   4                  700     0.1123517  0.8433128  0.07498572\n   4                  750     0.1122322  0.8436231  0.07488180\n   4                  800     0.1124768  0.8429594  0.07496479\n   4                  850     0.1126783  0.8424707  0.07504317\n   4                  900     0.1128018  0.8421813  0.07516802\n   4                  950     0.1128704  0.8420455  0.07523006\n   4                 1000     0.1128398  0.8421247  0.07532680\n   5                   50     0.1147182  0.8357200  0.07744488\n   5                  100     0.1127834  0.8409669  0.07443970\n   5                  150     0.1127285  0.8411105  0.07432455\n   5                  200     0.1129253  0.8408366  0.07433093\n   5                  250     0.1126308  0.8417328  0.07430120\n   5                  300     0.1124856  0.8421890  0.07400270\n   5                  350     0.1125105  0.8421371  0.07408824\n   5                  400     0.1125924  0.8417729  0.07419449\n   5                  450     0.1126305  0.8418316  0.07424855\n   5                  500     0.1126437  0.8418528  0.07434069\n   5                  550     0.1128031  0.8413877  0.07443978\n   5                  600     0.1129749  0.8410222  0.07464416\n   5                  650     0.1130505  0.8407691  0.07481935\n   5                  700     0.1131943  0.8403461  0.07488762\n   5                  750     0.1132781  0.8401328  0.07502383\n   5                  800     0.1132754  0.8400697  0.07504798\n   5                  850     0.1132677  0.8400960  0.07501872\n   5                  900     0.1134174  0.8397231  0.07510891\n   5                  950     0.1135016  0.8395041  0.07514660\n   5                 1000     0.1136219  0.8392270  0.07523409\n   6                   50     0.1134738  0.8387884  0.07630523\n   6                  100     0.1112866  0.8449952  0.07437753\n   6                  150     0.1116498  0.8441935  0.07451806\n   6                  200     0.1114086  0.8450053  0.07433774\n   6                  250     0.1116904  0.8446003  0.07450528\n   6                  300     0.1116405  0.8447067  0.07460745\n   6                  350     0.1120175  0.8437966  0.07477339\n   6                  400     0.1119667  0.8440214  0.07476552\n   6                  450     0.1121760  0.8434861  0.07499075\n   6                  500     0.1122244  0.8433772  0.07514079\n   6                  550     0.1122506  0.8433603  0.07526066\n   6                  600     0.1122688  0.8433015  0.07528187\n   6                  650     0.1123367  0.8431433  0.07535241\n   6                  700     0.1124893  0.8427805  0.07549024\n   6                  750     0.1124589  0.8428081  0.07553631\n   6                  800     0.1125934  0.8424594  0.07564791\n   6                  850     0.1126293  0.8423557  0.07570012\n   6                  900     0.1127100  0.8421271  0.07570814\n   6                  950     0.1127572  0.8419953  0.07577021\n   6                 1000     0.1127874  0.8419329  0.07580536\n   7                   50     0.1125015  0.8422207  0.07506903\n   7                  100     0.1111397  0.8459996  0.07323311\n   7                  150     0.1111699  0.8460714  0.07309644\n   7                  200     0.1116112  0.8448118  0.07374291\n   7                  250     0.1113905  0.8454904  0.07355970\n   7                  300     0.1115168  0.8449676  0.07370424\n   7                  350     0.1115537  0.8449419  0.07386732\n   7                  400     0.1116880  0.8445208  0.07404607\n   7                  450     0.1118584  0.8441245  0.07424290\n   7                  500     0.1118135  0.8443609  0.07427428\n   7                  550     0.1118715  0.8441660  0.07432583\n   7                  600     0.1120839  0.8436000  0.07449454\n   7                  650     0.1122996  0.8430489  0.07466681\n   7                  700     0.1124466  0.8426628  0.07477854\n   7                  750     0.1124656  0.8426620  0.07484187\n   7                  800     0.1125503  0.8424901  0.07492728\n   7                  850     0.1126420  0.8422935  0.07505970\n   7                  900     0.1127063  0.8421388  0.07511347\n   7                  950     0.1127420  0.8420142  0.07514704\n   7                 1000     0.1127848  0.8419209  0.07519591\n   8                   50     0.1134442  0.8390744  0.07517079\n   8                  100     0.1113542  0.8453119  0.07365406\n   8                  150     0.1111771  0.8462489  0.07358146\n   8                  200     0.1117109  0.8448927  0.07391530\n   8                  250     0.1113348  0.8460525  0.07379929\n   8                  300     0.1114117  0.8460919  0.07380980\n   8                  350     0.1113601  0.8461744  0.07371058\n   8                  400     0.1115123  0.8458106  0.07371669\n   8                  450     0.1115202  0.8457846  0.07375731\n   8                  500     0.1115910  0.8456670  0.07389795\n   8                  550     0.1117072  0.8454298  0.07398178\n   8                  600     0.1117628  0.8452755  0.07405285\n   8                  650     0.1119484  0.8448660  0.07414512\n   8                  700     0.1119255  0.8449389  0.07418292\n   8                  750     0.1118843  0.8450571  0.07421084\n   8                  800     0.1120240  0.8447453  0.07431664\n   8                  850     0.1121129  0.8445211  0.07437079\n   8                  900     0.1121847  0.8443480  0.07447542\n   8                  950     0.1122337  0.8442344  0.07448805\n   8                 1000     0.1122457  0.8442135  0.07452872\n   9                   50     0.1121301  0.8430789  0.07446649\n   9                  100     0.1106031  0.8471191  0.07303017\n   9                  150     0.1113339  0.8455348  0.07312638\n   9                  200     0.1116384  0.8447969  0.07336973\n   9                  250     0.1117298  0.8445624  0.07370333\n   9                  300     0.1114822  0.8453314  0.07377300\n   9                  350     0.1115669  0.8451223  0.07384065\n   9                  400     0.1117214  0.8447831  0.07402333\n   9                  450     0.1117696  0.8446269  0.07406763\n   9                  500     0.1119380  0.8442322  0.07426571\n   9                  550     0.1120356  0.8440328  0.07439376\n   9                  600     0.1121426  0.8437248  0.07449799\n   9                  650     0.1121849  0.8436342  0.07456437\n   9                  700     0.1122436  0.8434723  0.07460770\n   9                  750     0.1123410  0.8432376  0.07471884\n   9                  800     0.1123425  0.8432636  0.07475188\n   9                  850     0.1124113  0.8430882  0.07481762\n   9                  900     0.1124623  0.8429731  0.07483556\n   9                  950     0.1124724  0.8429332  0.07485113\n   9                 1000     0.1124804  0.8429258  0.07486100\n  10                   50     0.1125329  0.8419627  0.07423121\n  10                  100     0.1109795  0.8463898  0.07302537\n  10                  150     0.1107044  0.8469210  0.07302168\n  10                  200     0.1103046  0.8479983  0.07276967\n  10                  250     0.1101989  0.8483237  0.07275326\n  10                  300     0.1099151  0.8491608  0.07268969\n  10                  350     0.1101296  0.8487303  0.07284724\n  10                  400     0.1102292  0.8484347  0.07288301\n  10                  450     0.1102154  0.8484738  0.07294780\n  10                  500     0.1102776  0.8483017  0.07292208\n  10                  550     0.1102967  0.8482732  0.07297980\n  10                  600     0.1103522  0.8481305  0.07304615\n  10                  650     0.1103438  0.8481346  0.07311375\n  10                  700     0.1103962  0.8480408  0.07318450\n  10                  750     0.1105087  0.8477763  0.07327629\n  10                  800     0.1104959  0.8478212  0.07328697\n  10                  850     0.1105327  0.8477226  0.07332758\n  10                  900     0.1105954  0.8475670  0.07337949\n  10                  950     0.1105441  0.8477055  0.07336182\n  10                 1000     0.1105557  0.8476579  0.07337891\n  11                   50     0.1125197  0.8414815  0.07368924\n  11                  100     0.1110939  0.8455271  0.07274269\n  11                  150     0.1112469  0.8452484  0.07289503\n  11                  200     0.1114980  0.8447607  0.07304767\n  11                  250     0.1117493  0.8442900  0.07323927\n  11                  300     0.1120365  0.8434408  0.07349975\n  11                  350     0.1120915  0.8433073  0.07359549\n  11                  400     0.1122045  0.8430514  0.07361647\n  11                  450     0.1121791  0.8430437  0.07369199\n  11                  500     0.1123262  0.8427237  0.07380589\n  11                  550     0.1123061  0.8428324  0.07386247\n  11                  600     0.1124338  0.8425433  0.07397561\n  11                  650     0.1124787  0.8424397  0.07404825\n  11                  700     0.1125376  0.8422566  0.07410800\n  11                  750     0.1125699  0.8422021  0.07416902\n  11                  800     0.1126495  0.8419872  0.07421064\n  11                  850     0.1126935  0.8419189  0.07421886\n  11                  900     0.1127041  0.8418878  0.07422131\n  11                  950     0.1127020  0.8418864  0.07423385\n  11                 1000     0.1127409  0.8418014  0.07428164\n  12                   50     0.1138047  0.8385718  0.07434719\n  12                  100     0.1122265  0.8431903  0.07282290\n  12                  150     0.1122766  0.8429706  0.07286829\n  12                  200     0.1126828  0.8419639  0.07353919\n  12                  250     0.1128720  0.8414933  0.07380936\n  12                  300     0.1125650  0.8423696  0.07390821\n  12                  350     0.1126444  0.8422611  0.07400306\n  12                  400     0.1126037  0.8424018  0.07413544\n  12                  450     0.1126501  0.8422381  0.07424561\n  12                  500     0.1127401  0.8420509  0.07435252\n  12                  550     0.1127314  0.8420666  0.07445691\n  12                  600     0.1127963  0.8419111  0.07454040\n  12                  650     0.1127702  0.8420194  0.07456929\n  12                  700     0.1127896  0.8419794  0.07461111\n  12                  750     0.1128090  0.8419230  0.07463127\n  12                  800     0.1127870  0.8419980  0.07462415\n  12                  850     0.1128296  0.8419052  0.07467478\n  12                  900     0.1128188  0.8419309  0.07467296\n  12                  950     0.1128546  0.8418360  0.07471009\n  12                 1000     0.1128452  0.8418740  0.07472713\n  13                   50     0.1142697  0.8377481  0.07420000\n  13                  100     0.1126579  0.8428432  0.07331850\n  13                  150     0.1130368  0.8420538  0.07380871\n  13                  200     0.1126539  0.8432476  0.07377520\n  13                  250     0.1127526  0.8429539  0.07390364\n  13                  300     0.1129154  0.8426248  0.07411403\n  13                  350     0.1128190  0.8428538  0.07405083\n  13                  400     0.1129205  0.8426879  0.07415125\n  13                  450     0.1128204  0.8428946  0.07418998\n  13                  500     0.1129501  0.8425824  0.07431134\n  13                  550     0.1129461  0.8425239  0.07437962\n  13                  600     0.1129409  0.8425408  0.07443808\n  13                  650     0.1129371  0.8425598  0.07443888\n  13                  700     0.1129351  0.8425863  0.07442023\n  13                  750     0.1130065  0.8424000  0.07443876\n  13                  800     0.1130398  0.8423019  0.07448059\n  13                  850     0.1130509  0.8423064  0.07447182\n  13                  900     0.1130906  0.8421975  0.07450368\n  13                  950     0.1130594  0.8422888  0.07448156\n  13                 1000     0.1130608  0.8422905  0.07448304\n  14                   50     0.1126369  0.8420395  0.07354179\n  14                  100     0.1128960  0.8416422  0.07348016\n  14                  150     0.1123394  0.8433392  0.07318046\n  14                  200     0.1122478  0.8435616  0.07327454\n  14                  250     0.1124516  0.8429707  0.07360471\n  14                  300     0.1127593  0.8421768  0.07385269\n  14                  350     0.1129961  0.8415917  0.07413896\n  14                  400     0.1130062  0.8416126  0.07416940\n  14                  450     0.1130618  0.8414101  0.07432499\n  14                  500     0.1129875  0.8416486  0.07432808\n  14                  550     0.1130492  0.8414819  0.07437773\n  14                  600     0.1130555  0.8414152  0.07443019\n  14                  650     0.1131076  0.8413375  0.07451082\n  14                  700     0.1131065  0.8413504  0.07453191\n  14                  750     0.1130964  0.8413805  0.07455071\n  14                  800     0.1130887  0.8414151  0.07454242\n  14                  850     0.1131360  0.8413176  0.07458052\n  14                  900     0.1131351  0.8413214  0.07459388\n  14                  950     0.1130822  0.8414517  0.07456862\n  14                 1000     0.1131068  0.8413938  0.07459236\n  15                   50     0.1131305  0.8403465  0.07349639\n  15                  100     0.1123321  0.8431827  0.07287891\n  15                  150     0.1118004  0.8446679  0.07282148\n  15                  200     0.1120324  0.8439525  0.07332044\n  15                  250     0.1119005  0.8443481  0.07349359\n  15                  300     0.1116865  0.8449179  0.07357513\n  15                  350     0.1119159  0.8442780  0.07376522\n  15                  400     0.1117553  0.8447825  0.07373375\n  15                  450     0.1117660  0.8448326  0.07389888\n  15                  500     0.1115844  0.8452712  0.07385394\n  15                  550     0.1118022  0.8447157  0.07395732\n  15                  600     0.1118108  0.8447591  0.07399147\n  15                  650     0.1118719  0.8445354  0.07403653\n  15                  700     0.1118839  0.8445297  0.07404660\n  15                  750     0.1118374  0.8446646  0.07403235\n  15                  800     0.1118474  0.8446509  0.07406386\n  15                  850     0.1118260  0.8447102  0.07403923\n  15                  900     0.1117808  0.8448196  0.07401107\n  15                  950     0.1117840  0.8448252  0.07402911\n  15                 1000     0.1117652  0.8448745  0.07401993\n  16                   50     0.1141391  0.8375297  0.07406268\n  16                  100     0.1121129  0.8435288  0.07292953\n  16                  150     0.1121899  0.8435102  0.07338521\n  16                  200     0.1119181  0.8442792  0.07344087\n  16                  250     0.1122253  0.8435785  0.07393778\n  16                  300     0.1122620  0.8434988  0.07417501\n  16                  350     0.1126126  0.8426088  0.07453239\n  16                  400     0.1124789  0.8429325  0.07454633\n  16                  450     0.1125580  0.8426936  0.07469981\n  16                  500     0.1125500  0.8426901  0.07472204\n  16                  550     0.1125751  0.8426911  0.07474228\n  16                  600     0.1125733  0.8426905  0.07477102\n  16                  650     0.1126125  0.8425720  0.07482068\n  16                  700     0.1125686  0.8427024  0.07481747\n  16                  750     0.1126200  0.8425544  0.07483699\n  16                  800     0.1125920  0.8426440  0.07484748\n  16                  850     0.1125509  0.8427445  0.07483909\n  16                  900     0.1125916  0.8426379  0.07487934\n  16                  950     0.1126015  0.8426075  0.07488019\n  16                 1000     0.1125854  0.8426562  0.07487126\n  17                   50     0.1133941  0.8398378  0.07294471\n  17                  100     0.1126347  0.8423077  0.07290355\n  17                  150     0.1137885  0.8392116  0.07402004\n  17                  200     0.1139283  0.8387646  0.07444590\n  17                  250     0.1140582  0.8385842  0.07466001\n  17                  300     0.1141314  0.8384539  0.07476493\n  17                  350     0.1140994  0.8385512  0.07480841\n  17                  400     0.1142548  0.8381093  0.07490541\n  17                  450     0.1141576  0.8383890  0.07490032\n  17                  500     0.1142423  0.8381209  0.07494723\n  17                  550     0.1141920  0.8383328  0.07496546\n  17                  600     0.1141366  0.8384829  0.07491644\n  17                  650     0.1142033  0.8383281  0.07498024\n  17                  700     0.1141768  0.8384235  0.07496827\n  17                  750     0.1142274  0.8383156  0.07502256\n  17                  800     0.1142559  0.8382422  0.07503381\n  17                  850     0.1142619  0.8382555  0.07505778\n  17                  900     0.1143011  0.8381534  0.07507514\n  17                  950     0.1143168  0.8381244  0.07508052\n  17                 1000     0.1143637  0.8380077  0.07509405\n  18                   50     0.1133819  0.8398295  0.07350387\n  18                  100     0.1136137  0.8396218  0.07364100\n  18                  150     0.1130604  0.8411272  0.07365593\n  18                  200     0.1134891  0.8401435  0.07386500\n  18                  250     0.1132350  0.8408405  0.07394343\n  18                  300     0.1133004  0.8406767  0.07406930\n  18                  350     0.1134083  0.8404483  0.07420219\n  18                  400     0.1135125  0.8401787  0.07425763\n  18                  450     0.1133526  0.8406305  0.07421093\n  18                  500     0.1133832  0.8405383  0.07430299\n  18                  550     0.1134025  0.8405343  0.07431838\n  18                  600     0.1134555  0.8403742  0.07435045\n  18                  650     0.1134723  0.8403659  0.07432233\n  18                  700     0.1134564  0.8403773  0.07431042\n  18                  750     0.1134600  0.8403946  0.07434606\n  18                  800     0.1134416  0.8404452  0.07431402\n  18                  850     0.1134453  0.8404420  0.07433191\n  18                  900     0.1134832  0.8403340  0.07433309\n  18                  950     0.1134722  0.8403798  0.07432612\n  18                 1000     0.1134970  0.8403208  0.07432418\n  19                   50     0.1128946  0.8401795  0.07299429\n  19                  100     0.1132095  0.8399601  0.07333296\n  19                  150     0.1130428  0.8403917  0.07356504\n  19                  200     0.1133440  0.8394295  0.07399058\n  19                  250     0.1131280  0.8400183  0.07423971\n  19                  300     0.1136914  0.8386044  0.07454144\n  19                  350     0.1135952  0.8388618  0.07466089\n  19                  400     0.1135656  0.8389527  0.07481595\n  19                  450     0.1136120  0.8388584  0.07485000\n  19                  500     0.1134450  0.8393081  0.07478708\n  19                  550     0.1134875  0.8392416  0.07487141\n  19                  600     0.1135867  0.8390107  0.07495247\n  19                  650     0.1135482  0.8391352  0.07495277\n  19                  700     0.1136328  0.8388962  0.07499157\n  19                  750     0.1135513  0.8391350  0.07498136\n  19                  800     0.1135918  0.8390274  0.07501198\n  19                  850     0.1136301  0.8389561  0.07503862\n  19                  900     0.1136142  0.8390024  0.07502570\n  19                  950     0.1136217  0.8389901  0.07504057\n  19                 1000     0.1136317  0.8389646  0.07505510\n  20                   50     0.1133590  0.8398770  0.07329153\n  20                  100     0.1133497  0.8404356  0.07362630\n  20                  150     0.1133742  0.8406328  0.07369263\n  20                  200     0.1134622  0.8403462  0.07386509\n  20                  250     0.1135787  0.8399785  0.07398267\n  20                  300     0.1134219  0.8404992  0.07388051\n  20                  350     0.1136676  0.8398728  0.07406826\n  20                  400     0.1135035  0.8403044  0.07408810\n  20                  450     0.1135800  0.8401002  0.07409903\n  20                  500     0.1134690  0.8403542  0.07406907\n  20                  550     0.1133482  0.8406803  0.07403952\n  20                  600     0.1133517  0.8406998  0.07407751\n  20                  650     0.1133925  0.8406453  0.07409429\n  20                  700     0.1133773  0.8406790  0.07410704\n  20                  750     0.1133733  0.8406936  0.07411591\n  20                  800     0.1133378  0.8408165  0.07414581\n  20                  850     0.1133412  0.8408601  0.07415678\n  20                  900     0.1132905  0.8409679  0.07411478\n  20                  950     0.1133220  0.8408902  0.07411604\n  20                 1000     0.1133335  0.8408656  0.07411751\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 300, interaction.depth =\n 10, shrinkage = 0.1 and n.minobsinnode = 10.\n\nmodels_J1 &lt;- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n## Importance frame (wide format) ===\nimportances_wide &lt;- cbind(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_ranger = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  \n  varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_gbm\" = \"Overall\"))  %&gt;% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_J1 &lt;- full_join(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Importance\" = \"rel.inf\") %&gt;% \n    select(variable, model, Importance)) %&gt;% \n  full_join(varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\")) %&gt;%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n\n\n\n\n\n# Jaccard 2\nranger_red &lt;- saved_models[[2]][[1]]\nxgb_red &lt;- saved_models[[2]][[2]]\ngbm_red &lt;- saved_models[[2]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      823 \nNumber of independent variables:  15 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01544412 \nR squared (OOB):                  0.8057051 \n\nranger_red$results\n\n   mtry min.node.size  splitrule      RMSE  Rsquared        MAE      RMSESD\n1     2             5   variance 0.1260406 0.8056930 0.08926665 0.007312223\n2     2             5 extratrees 0.1289977 0.7973707 0.09216982 0.007723962\n3     3             5   variance 0.1249849 0.8078292 0.08748564 0.007485030\n4     3             5 extratrees 0.1276692 0.8002262 0.09018152 0.007813283\n5     4             5   variance 0.1244352 0.8091078 0.08658932 0.007405179\n6     4             5 extratrees 0.1267373 0.8026290 0.08905400 0.007921564\n7     5             5   variance 0.1242679 0.8094102 0.08621790 0.007355492\n8     5             5 extratrees 0.1263655 0.8034284 0.08842375 0.008066713\n9     6             5   variance 0.1243970 0.8088235 0.08615208 0.007526901\n10    6             5 extratrees 0.1261296 0.8039488 0.08794364 0.008209857\n11    7             5   variance 0.1244752 0.8084954 0.08608531 0.007552705\n12    7             5 extratrees 0.1259829 0.8042480 0.08771178 0.008187244\n13    8             5   variance 0.1245474 0.8081827 0.08612129 0.007602701\n14    8             5 extratrees 0.1257252 0.8049353 0.08736545 0.008345148\n15    9             5   variance 0.1247794 0.8074774 0.08621205 0.007444275\n16    9             5 extratrees 0.1256311 0.8051973 0.08712087 0.008266322\n17   10             5   variance 0.1249122 0.8069866 0.08619433 0.007518964\n18   10             5 extratrees 0.1257293 0.8047977 0.08709033 0.008474355\n19   11             5   variance 0.1252386 0.8059722 0.08641960 0.007562893\n20   11             5 extratrees 0.1256002 0.8051663 0.08693110 0.008463631\n21   12             5   variance 0.1254442 0.8052826 0.08648914 0.007540639\n22   12             5 extratrees 0.1257058 0.8047797 0.08694106 0.008491981\n23   13             5   variance 0.1256391 0.8046830 0.08653946 0.007654048\n24   13             5 extratrees 0.1256432 0.8049140 0.08682399 0.008505333\n25   14             5   variance 0.1258031 0.8041699 0.08661178 0.007615353\n26   14             5 extratrees 0.1257965 0.8043991 0.08681364 0.008568683\n27   15             5   variance 0.1260365 0.8034445 0.08676195 0.007622122\n28   15             5 extratrees 0.1256931 0.8046933 0.08675762 0.008710886\n   RsquaredSD       MAESD\n1  0.02540306 0.005493561\n2  0.02787693 0.004927717\n3  0.02529038 0.005827222\n4  0.02756015 0.005006648\n5  0.02464231 0.005894750\n6  0.02747694 0.005133563\n7  0.02420319 0.005914532\n8  0.02764641 0.005184050\n9  0.02484508 0.006043449\n10 0.02793602 0.005311346\n11 0.02490062 0.006111946\n12 0.02771323 0.005369005\n13 0.02500967 0.006134414\n14 0.02802708 0.005424229\n15 0.02465914 0.005921552\n16 0.02763188 0.005383124\n17 0.02478801 0.005980159\n18 0.02830268 0.005577527\n19 0.02516815 0.006006034\n20 0.02815352 0.005520004\n21 0.02515206 0.005983238\n22 0.02825150 0.005613019\n23 0.02553690 0.005927629\n24 0.02816930 0.005617544\n25 0.02530233 0.005946151\n26 0.02837556 0.005561156\n27 0.02542279 0.005936225\n28 0.02881966 0.005665576\n\nplot(ranger_red)\n\n\n\n\n\n\n\n## XGB ======\nplot(xgb_red)\n\n\n\n\n\n\n\nxgb_red\n\neXtreme Gradient Boosting \n\n823 samples\n 15 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 660, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE       \n  0.1  2          0.00   0.1284578  0.7979565  0.08993810\n  0.1  2          0.01   0.1260796  0.8037433  0.08842593\n  0.1  2          0.10   0.1314116  0.7878631  0.09497730\n  0.1  3          0.00   0.1284644  0.7981403  0.08934380\n  0.1  3          0.01   0.1275459  0.7995220  0.08711394\n  0.1  3          0.10   0.1293986  0.7939568  0.09083660\n  0.1  5          0.00   0.1266654  0.8025459  0.08635013\n  0.1  5          0.01   0.1249692  0.8072804  0.08511744\n  0.1  5          0.10   0.1273035  0.8005622  0.08995105\n  0.3  2          0.00   0.1338256  0.7829319  0.09464282\n  0.3  2          0.01   0.1313921  0.7884705  0.09233459\n  0.3  2          0.10   0.1329644  0.7816596  0.09544929\n  0.3  3          0.00   0.1304433  0.7926532  0.09167521\n  0.3  3          0.01   0.1284496  0.7977904  0.08988310\n  0.3  3          0.10   0.1337157  0.7791898  0.09367822\n  0.3  5          0.00   0.1329975  0.7845899  0.09044138\n  0.3  5          0.01   0.1308238  0.7899613  0.08975483\n  0.3  5          0.10   0.1289239  0.7951159  0.09133874\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.01, colsample_bytree = 0.6, min_child_weight = 1\n and subsample = 1.\n\n## GBM =====\ngbm_red$finalModel\n\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 15 predictors of which 15 had non-zero influence.\n\ngbm_red\n\nStochastic Gradient Boosting \n\n823 samples\n 15 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 660, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE       \n   1                   50     0.1584074  0.7086926  0.12246133\n   1                  100     0.1435215  0.7485171  0.10591372\n   1                  150     0.1394734  0.7599662  0.10037732\n   1                  200     0.1384146  0.7634577  0.09872444\n   1                  250     0.1373744  0.7669744  0.09733579\n   1                  300     0.1367024  0.7691750  0.09668674\n   1                  350     0.1368078  0.7688117  0.09669316\n   1                  400     0.1368899  0.7689408  0.09654413\n   1                  450     0.1360716  0.7716790  0.09593482\n   1                  500     0.1362813  0.7710574  0.09585209\n   1                  550     0.1362871  0.7711316  0.09590389\n   1                  600     0.1366642  0.7698342  0.09601912\n   1                  650     0.1367353  0.7700177  0.09584530\n   1                  700     0.1360659  0.7719509  0.09565328\n   1                  750     0.1360073  0.7722783  0.09541158\n   1                  800     0.1354786  0.7737655  0.09516774\n   1                  850     0.1353131  0.7746747  0.09487182\n   1                  900     0.1349485  0.7757720  0.09489295\n   1                  950     0.1351550  0.7750624  0.09504177\n   1                 1000     0.1351889  0.7751391  0.09462931\n   2                   50     0.1412315  0.7560463  0.10319051\n   2                  100     0.1341781  0.7778110  0.09424480\n   2                  150     0.1331899  0.7811748  0.09280546\n   2                  200     0.1320531  0.7850610  0.09153427\n   2                  250     0.1305759  0.7900238  0.09056218\n   2                  300     0.1309629  0.7888887  0.09059028\n   2                  350     0.1311929  0.7881394  0.09090977\n   2                  400     0.1305613  0.7900991  0.09058572\n   2                  450     0.1306261  0.7897671  0.09075583\n   2                  500     0.1307201  0.7898244  0.09079529\n   2                  550     0.1308576  0.7894471  0.09094342\n   2                  600     0.1306390  0.7900256  0.09097633\n   2                  650     0.1301794  0.7914600  0.09081898\n   2                  700     0.1298772  0.7926054  0.09062494\n   2                  750     0.1302201  0.7916673  0.09084508\n   2                  800     0.1301071  0.7920354  0.09101021\n   2                  850     0.1299781  0.7925779  0.09074899\n   2                  900     0.1302216  0.7918534  0.09112141\n   2                  950     0.1299370  0.7927066  0.09111246\n   2                 1000     0.1302079  0.7920151  0.09138006\n   3                   50     0.1354702  0.7737838  0.09583116\n   3                  100     0.1313452  0.7870555  0.09107133\n   3                  150     0.1307724  0.7890376  0.09040923\n   3                  200     0.1300830  0.7913672  0.09001012\n   3                  250     0.1298266  0.7923981  0.09011055\n   3                  300     0.1306013  0.7899641  0.09064603\n   3                  350     0.1305521  0.7902618  0.09042964\n   3                  400     0.1303383  0.7910292  0.09035448\n   3                  450     0.1307514  0.7898099  0.09056579\n   3                  500     0.1304741  0.7907988  0.09052507\n   3                  550     0.1304651  0.7911283  0.09039566\n   3                  600     0.1305976  0.7907643  0.09047638\n   3                  650     0.1305774  0.7907636  0.09057271\n   3                  700     0.1307723  0.7902601  0.09074473\n   3                  750     0.1310882  0.7894701  0.09089443\n   3                  800     0.1310127  0.7897196  0.09107430\n   3                  850     0.1310305  0.7898513  0.09114423\n   3                  900     0.1313108  0.7890661  0.09116317\n   3                  950     0.1313835  0.7889494  0.09124426\n   3                 1000     0.1313361  0.7892466  0.09138830\n   4                   50     0.1319902  0.7849885  0.09238716\n   4                  100     0.1290531  0.7943175  0.08889128\n   4                  150     0.1289522  0.7952563  0.08858471\n   4                  200     0.1289310  0.7954851  0.08843235\n   4                  250     0.1293091  0.7943071  0.08885981\n   4                  300     0.1296344  0.7937031  0.08890821\n   4                  350     0.1297294  0.7931637  0.08903844\n   4                  400     0.1298753  0.7929470  0.08909999\n   4                  450     0.1302956  0.7920786  0.08937468\n   4                  500     0.1300753  0.7927773  0.08936576\n   4                  550     0.1303154  0.7922350  0.08953787\n   4                  600     0.1302035  0.7926392  0.08955912\n   4                  650     0.1308081  0.7908603  0.09010048\n   4                  700     0.1308628  0.7909072  0.09010682\n   4                  750     0.1308826  0.7909193  0.09020055\n   4                  800     0.1313222  0.7897089  0.09049264\n   4                  850     0.1312623  0.7899755  0.09036911\n   4                  900     0.1316414  0.7889157  0.09063450\n   4                  950     0.1316719  0.7889149  0.09068012\n   4                 1000     0.1318779  0.7883700  0.09085566\n   5                   50     0.1312362  0.7868265  0.09155599\n   5                  100     0.1297050  0.7924267  0.08958106\n   5                  150     0.1288361  0.7950959  0.08947081\n   5                  200     0.1290651  0.7948451  0.08901226\n   5                  250     0.1298312  0.7928330  0.08967155\n   5                  300     0.1292395  0.7948574  0.08958048\n   5                  350     0.1294041  0.7945368  0.08987393\n   5                  400     0.1296468  0.7938478  0.09031521\n   5                  450     0.1298362  0.7934427  0.09042180\n   5                  500     0.1296866  0.7940207  0.09026027\n   5                  550     0.1301230  0.7928043  0.09031782\n   5                  600     0.1302336  0.7926273  0.09063277\n   5                  650     0.1303377  0.7924550  0.09052715\n   5                  700     0.1306044  0.7917702  0.09075715\n   5                  750     0.1307128  0.7915818  0.09074932\n   5                  800     0.1310031  0.7907407  0.09095152\n   5                  850     0.1311243  0.7905124  0.09106542\n   5                  900     0.1311272  0.7904726  0.09105686\n   5                  950     0.1311169  0.7906518  0.09101796\n   5                 1000     0.1311131  0.7906268  0.09097333\n   6                   50     0.1305803  0.7892275  0.09119871\n   6                  100     0.1300404  0.7913173  0.08984256\n   6                  150     0.1295650  0.7929390  0.08990302\n   6                  200     0.1300656  0.7918445  0.09002398\n   6                  250     0.1294736  0.7941055  0.08961028\n   6                  300     0.1291066  0.7953429  0.08929145\n   6                  350     0.1294622  0.7945403  0.08988877\n   6                  400     0.1299236  0.7931482  0.09007615\n   6                  450     0.1299937  0.7931098  0.09004024\n   6                  500     0.1303328  0.7922860  0.09032943\n   6                  550     0.1302410  0.7926275  0.09044957\n   6                  600     0.1304352  0.7920596  0.09052825\n   6                  650     0.1302516  0.7928019  0.09038328\n   6                  700     0.1302509  0.7928315  0.09042086\n   6                  750     0.1304720  0.7923310  0.09057977\n   6                  800     0.1303916  0.7925953  0.09054716\n   6                  850     0.1305320  0.7922091  0.09061801\n   6                  900     0.1305726  0.7921536  0.09063236\n   6                  950     0.1306469  0.7920458  0.09078435\n   6                 1000     0.1305765  0.7922752  0.09070045\n   7                   50     0.1307579  0.7888864  0.09004225\n   7                  100     0.1293564  0.7937211  0.08916414\n   7                  150     0.1303358  0.7910600  0.08975257\n   7                  200     0.1305526  0.7903710  0.09004413\n   7                  250     0.1305003  0.7908379  0.09001871\n   7                  300     0.1304427  0.7911130  0.09019726\n   7                  350     0.1306958  0.7907275  0.09039593\n   7                  400     0.1306112  0.7911365  0.09050001\n   7                  450     0.1308759  0.7906046  0.09063660\n   7                  500     0.1310556  0.7901675  0.09069579\n   7                  550     0.1311143  0.7901550  0.09068275\n   7                  600     0.1310974  0.7902719  0.09068295\n   7                  650     0.1312212  0.7900023  0.09084907\n   7                  700     0.1311993  0.7902051  0.09083951\n   7                  750     0.1312898  0.7899968  0.09093972\n   7                  800     0.1313557  0.7898352  0.09098680\n   7                  850     0.1314379  0.7895758  0.09106189\n   7                  900     0.1313992  0.7897153  0.09109252\n   7                  950     0.1315073  0.7894371  0.09115280\n   7                 1000     0.1314459  0.7896975  0.09110312\n   8                   50     0.1290474  0.7943049  0.08918481\n   8                  100     0.1288383  0.7958471  0.08789753\n   8                  150     0.1290646  0.7950620  0.08838693\n   8                  200     0.1294661  0.7939105  0.08840908\n   8                  250     0.1298856  0.7928578  0.08871989\n   8                  300     0.1298379  0.7930958  0.08866591\n   8                  350     0.1302061  0.7922050  0.08896111\n   8                  400     0.1301889  0.7924098  0.08903648\n   8                  450     0.1301035  0.7926759  0.08906734\n   8                  500     0.1302924  0.7922154  0.08923156\n   8                  550     0.1305103  0.7916771  0.08932237\n   8                  600     0.1304275  0.7919268  0.08930586\n   8                  650     0.1305183  0.7916841  0.08930643\n   8                  700     0.1305092  0.7917822  0.08932119\n   8                  750     0.1307274  0.7911322  0.08954350\n   8                  800     0.1307436  0.7911650  0.08958573\n   8                  850     0.1307089  0.7912562  0.08954295\n   8                  900     0.1306806  0.7913698  0.08952019\n   8                  950     0.1307375  0.7912389  0.08954107\n   8                 1000     0.1307916  0.7911372  0.08956973\n   9                   50     0.1280873  0.7971016  0.08969457\n   9                  100     0.1281615  0.7973094  0.08888332\n   9                  150     0.1289845  0.7954264  0.08927496\n   9                  200     0.1295955  0.7937968  0.08988226\n   9                  250     0.1297861  0.7933117  0.09002900\n   9                  300     0.1296351  0.7940924  0.09024541\n   9                  350     0.1298201  0.7937558  0.09054436\n   9                  400     0.1297383  0.7940841  0.09043395\n   9                  450     0.1298392  0.7937550  0.09066053\n   9                  500     0.1300532  0.7932424  0.09087560\n   9                  550     0.1299519  0.7935931  0.09093514\n   9                  600     0.1299662  0.7936721  0.09099519\n   9                  650     0.1300879  0.7933198  0.09109683\n   9                  700     0.1301281  0.7932973  0.09111824\n   9                  750     0.1300880  0.7934240  0.09106746\n   9                  800     0.1299914  0.7937800  0.09105585\n   9                  850     0.1300942  0.7935239  0.09113102\n   9                  900     0.1301089  0.7934939  0.09113778\n   9                  950     0.1301074  0.7935411  0.09114665\n   9                 1000     0.1301709  0.7933976  0.09119489\n  10                   50     0.1294101  0.7935070  0.08919532\n  10                  100     0.1293842  0.7941388  0.08888102\n  10                  150     0.1294039  0.7943717  0.08921797\n  10                  200     0.1290413  0.7956726  0.08899274\n  10                  250     0.1293973  0.7950923  0.08924317\n  10                  300     0.1302726  0.7925906  0.08990092\n  10                  350     0.1305757  0.7918577  0.08997181\n  10                  400     0.1307520  0.7915413  0.08993049\n  10                  450     0.1308591  0.7913457  0.08997757\n  10                  500     0.1308761  0.7912503  0.09008533\n  10                  550     0.1311018  0.7907374  0.09027126\n  10                  600     0.1311056  0.7907181  0.09027360\n  10                  650     0.1312674  0.7902953  0.09033327\n  10                  700     0.1313612  0.7900840  0.09033570\n  10                  750     0.1314093  0.7899434  0.09038508\n  10                  800     0.1314101  0.7899732  0.09041920\n  10                  850     0.1314461  0.7898900  0.09044895\n  10                  900     0.1315016  0.7897552  0.09049643\n  10                  950     0.1315404  0.7896625  0.09051017\n  10                 1000     0.1315276  0.7897137  0.09050945\n  11                   50     0.1298912  0.7912710  0.08873469\n  11                  100     0.1289184  0.7950283  0.08835305\n  11                  150     0.1300495  0.7919703  0.08929061\n  11                  200     0.1303396  0.7913036  0.08963485\n  11                  250     0.1298290  0.7932145  0.08961927\n  11                  300     0.1298668  0.7930711  0.08961964\n  11                  350     0.1300702  0.7927230  0.08989050\n  11                  400     0.1300960  0.7929156  0.08999414\n  11                  450     0.1300060  0.7932126  0.09001372\n  11                  500     0.1301325  0.7929498  0.09020387\n  11                  550     0.1302263  0.7926915  0.09024974\n  11                  600     0.1302208  0.7927429  0.09027466\n  11                  650     0.1302365  0.7927751  0.09023380\n  11                  700     0.1302353  0.7927517  0.09024268\n  11                  750     0.1301756  0.7930190  0.09019921\n  11                  800     0.1301983  0.7929603  0.09020172\n  11                  850     0.1301710  0.7930425  0.09019712\n  11                  900     0.1301582  0.7930907  0.09021776\n  11                  950     0.1301617  0.7930798  0.09022603\n  11                 1000     0.1301266  0.7932013  0.09021028\n  12                   50     0.1312779  0.7877039  0.08992192\n  12                  100     0.1306737  0.7903910  0.08938858\n  12                  150     0.1315707  0.7880055  0.09000626\n  12                  200     0.1308805  0.7903374  0.08987014\n  12                  250     0.1311043  0.7899336  0.09014911\n  12                  300     0.1313764  0.7893201  0.09048457\n  12                  350     0.1315764  0.7887876  0.09069644\n  12                  400     0.1316176  0.7887486  0.09070754\n  12                  450     0.1313914  0.7895788  0.09061801\n  12                  500     0.1313849  0.7896667  0.09062124\n  12                  550     0.1314037  0.7896541  0.09067148\n  12                  600     0.1312638  0.7901187  0.09057093\n  12                  650     0.1311911  0.7903658  0.09062612\n  12                  700     0.1311561  0.7904883  0.09067743\n  12                  750     0.1312354  0.7902894  0.09070888\n  12                  800     0.1311985  0.7904217  0.09069063\n  12                  850     0.1311593  0.7905380  0.09070009\n  12                  900     0.1311351  0.7906209  0.09070878\n  12                  950     0.1311183  0.7907100  0.09069721\n  12                 1000     0.1311325  0.7906813  0.09071575\n  13                   50     0.1290159  0.7940237  0.08859232\n  13                  100     0.1284857  0.7962826  0.08817150\n  13                  150     0.1288077  0.7959142  0.08847121\n  13                  200     0.1286499  0.7966336  0.08869121\n  13                  250     0.1284464  0.7975734  0.08858228\n  13                  300     0.1285547  0.7973087  0.08892125\n  13                  350     0.1284592  0.7977466  0.08881224\n  13                  400     0.1285081  0.7976546  0.08901085\n  13                  450     0.1285311  0.7975825  0.08902213\n  13                  500     0.1286994  0.7971841  0.08914167\n  13                  550     0.1286856  0.7972548  0.08912285\n  13                  600     0.1287239  0.7971719  0.08913940\n  13                  650     0.1286591  0.7973717  0.08912730\n  13                  700     0.1287349  0.7971281  0.08915678\n  13                  750     0.1287791  0.7970337  0.08916986\n  13                  800     0.1287876  0.7970090  0.08919717\n  13                  850     0.1287659  0.7970721  0.08919024\n  13                  900     0.1287309  0.7971844  0.08915607\n  13                  950     0.1287015  0.7972794  0.08912467\n  13                 1000     0.1287078  0.7972574  0.08914573\n  14                   50     0.1311164  0.7880510  0.08937666\n  14                  100     0.1298100  0.7928814  0.08877579\n  14                  150     0.1302723  0.7918733  0.08957876\n  14                  200     0.1297475  0.7938496  0.08958215\n  14                  250     0.1301249  0.7924913  0.09002031\n  14                  300     0.1302548  0.7922999  0.09033256\n  14                  350     0.1301869  0.7925474  0.09039868\n  14                  400     0.1304242  0.7919742  0.09050790\n  14                  450     0.1303992  0.7921621  0.09053508\n  14                  500     0.1304396  0.7920103  0.09053280\n  14                  550     0.1303686  0.7922361  0.09055737\n  14                  600     0.1303924  0.7921728  0.09055475\n  14                  650     0.1302704  0.7925936  0.09050165\n  14                  700     0.1302372  0.7927129  0.09046533\n  14                  750     0.1301894  0.7928540  0.09046913\n  14                  800     0.1302311  0.7927410  0.09049042\n  14                  850     0.1302218  0.7927599  0.09049122\n  14                  900     0.1301894  0.7928722  0.09046549\n  14                  950     0.1301115  0.7931089  0.09042556\n  14                 1000     0.1301672  0.7929461  0.09044900\n  15                   50     0.1293394  0.7932176  0.08902379\n  15                  100     0.1293676  0.7939149  0.08910845\n  15                  150     0.1289981  0.7954545  0.08899498\n  15                  200     0.1287252  0.7965825  0.08927223\n  15                  250     0.1287964  0.7966462  0.08926076\n  15                  300     0.1287307  0.7971388  0.08938199\n  15                  350     0.1289106  0.7965844  0.08960395\n  15                  400     0.1288413  0.7968719  0.08968034\n  15                  450     0.1285493  0.7978364  0.08958763\n  15                  500     0.1284915  0.7980992  0.08956695\n  15                  550     0.1285328  0.7980323  0.08955751\n  15                  600     0.1284838  0.7981427  0.08955447\n  15                  650     0.1284961  0.7981681  0.08952568\n  15                  700     0.1284951  0.7982075  0.08956355\n  15                  750     0.1284428  0.7983794  0.08953797\n  15                  800     0.1284424  0.7983892  0.08955916\n  15                  850     0.1283946  0.7985536  0.08953630\n  15                  900     0.1283703  0.7986165  0.08953146\n  15                  950     0.1283980  0.7985396  0.08954209\n  15                 1000     0.1283362  0.7987338  0.08951513\n  16                   50     0.1283689  0.7966960  0.08753356\n  16                  100     0.1305577  0.7908714  0.08804681\n  16                  150     0.1300207  0.7925528  0.08787229\n  16                  200     0.1295665  0.7945949  0.08807212\n  16                  250     0.1297753  0.7941035  0.08811500\n  16                  300     0.1299268  0.7938731  0.08850874\n  16                  350     0.1301912  0.7931743  0.08884685\n  16                  400     0.1301226  0.7934396  0.08878903\n  16                  450     0.1300845  0.7936367  0.08878791\n  16                  500     0.1301251  0.7935943  0.08876769\n  16                  550     0.1301321  0.7935444  0.08884653\n  16                  600     0.1300905  0.7937549  0.08884831\n  16                  650     0.1300608  0.7938861  0.08883626\n  16                  700     0.1299751  0.7941605  0.08882584\n  16                  750     0.1299608  0.7942125  0.08882290\n  16                  800     0.1299421  0.7942970  0.08881900\n  16                  850     0.1298566  0.7945542  0.08879357\n  16                  900     0.1298904  0.7944596  0.08879694\n  16                  950     0.1298423  0.7946215  0.08877699\n  16                 1000     0.1298347  0.7946693  0.08876285\n  17                   50     0.1299001  0.7920579  0.08804108\n  17                  100     0.1303908  0.7914290  0.08871953\n  17                  150     0.1302473  0.7922894  0.08876135\n  17                  200     0.1310583  0.7903206  0.08943456\n  17                  250     0.1317316  0.7883230  0.08964818\n  17                  300     0.1318872  0.7880527  0.08978061\n  17                  350     0.1321416  0.7874030  0.08994031\n  17                  400     0.1318971  0.7881878  0.08989739\n  17                  450     0.1317146  0.7887874  0.08982716\n  17                  500     0.1318036  0.7885520  0.08986517\n  17                  550     0.1316090  0.7891810  0.08983734\n  17                  600     0.1315521  0.7893552  0.08986816\n  17                  650     0.1314947  0.7895565  0.08982508\n  17                  700     0.1313526  0.7899884  0.08977059\n  17                  750     0.1313675  0.7899868  0.08975431\n  17                  800     0.1312992  0.7901946  0.08972015\n  17                  850     0.1312571  0.7903161  0.08970528\n  17                  900     0.1312500  0.7903422  0.08968051\n  17                  950     0.1312753  0.7902607  0.08967387\n  17                 1000     0.1312726  0.7902832  0.08969394\n  18                   50     0.1288523  0.7950338  0.08779494\n  18                  100     0.1293887  0.7942816  0.08819209\n  18                  150     0.1294105  0.7946735  0.08836900\n  18                  200     0.1291991  0.7955358  0.08860491\n  18                  250     0.1296125  0.7943926  0.08895640\n  18                  300     0.1299678  0.7934253  0.08917967\n  18                  350     0.1297991  0.7941214  0.08927997\n  18                  400     0.1299479  0.7937556  0.08940463\n  18                  450     0.1300402  0.7935270  0.08949518\n  18                  500     0.1299248  0.7938850  0.08947491\n  18                  550     0.1299464  0.7938901  0.08949894\n  18                  600     0.1299581  0.7938742  0.08955784\n  18                  650     0.1298389  0.7942222  0.08952583\n  18                  700     0.1298570  0.7942070  0.08950299\n  18                  750     0.1298009  0.7944071  0.08948176\n  18                  800     0.1298550  0.7942394  0.08952284\n  18                  850     0.1298549  0.7942623  0.08952792\n  18                  900     0.1298579  0.7942758  0.08952034\n  18                  950     0.1297876  0.7944822  0.08949148\n  18                 1000     0.1298257  0.7943640  0.08951648\n  19                   50     0.1291501  0.7935604  0.08843129\n  19                  100     0.1291973  0.7945503  0.08838429\n  19                  150     0.1296598  0.7932487  0.08895094\n  19                  200     0.1298871  0.7929046  0.08926857\n  19                  250     0.1298335  0.7932069  0.08936154\n  19                  300     0.1300422  0.7925868  0.08949688\n  19                  350     0.1296960  0.7938090  0.08953113\n  19                  400     0.1295477  0.7942420  0.08952061\n  19                  450     0.1295322  0.7943277  0.08954136\n  19                  500     0.1295516  0.7943049  0.08963444\n  19                  550     0.1295864  0.7942245  0.08966258\n  19                  600     0.1294248  0.7947457  0.08957554\n  19                  650     0.1293530  0.7949840  0.08956101\n  19                  700     0.1292330  0.7953503  0.08951184\n  19                  750     0.1292615  0.7952788  0.08949811\n  19                  800     0.1292182  0.7954010  0.08945505\n  19                  850     0.1291611  0.7955963  0.08944151\n  19                  900     0.1291467  0.7956260  0.08942938\n  19                  950     0.1291718  0.7955394  0.08944470\n  19                 1000     0.1291472  0.7956384  0.08941426\n  20                   50     0.1288098  0.7947898  0.08878282\n  20                  100     0.1288240  0.7955616  0.08882946\n  20                  150     0.1290725  0.7952653  0.08891402\n  20                  200     0.1295040  0.7940545  0.08921330\n  20                  250     0.1293822  0.7946422  0.08918458\n  20                  300     0.1294966  0.7943708  0.08922025\n  20                  350     0.1298036  0.7934646  0.08950123\n  20                  400     0.1294723  0.7945474  0.08947917\n  20                  450     0.1294167  0.7948002  0.08953956\n  20                  500     0.1292847  0.7952329  0.08949908\n  20                  550     0.1291894  0.7955359  0.08950960\n  20                  600     0.1291469  0.7956658  0.08952481\n  20                  650     0.1290690  0.7959300  0.08950182\n  20                  700     0.1290244  0.7960974  0.08950991\n  20                  750     0.1291098  0.7958526  0.08953859\n  20                  800     0.1290576  0.7960356  0.08951678\n  20                  850     0.1290655  0.7959940  0.08955958\n  20                  900     0.1290418  0.7960687  0.08955726\n  20                  950     0.1290526  0.7960397  0.08956962\n  20                 1000     0.1290149  0.7961653  0.08955349\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 9, shrinkage = 0.1 and n.minobsinnode = 10.\n\nmodels_J2 &lt;- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n\n\n## Importance frame (wide format) ===\nimportances_wide &lt;- cbind(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_ranger = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  \n  varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_gbm\" = \"Overall\"))  %&gt;% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_J2 &lt;- full_join(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\"),\n  \n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Importance\" = \"rel.inf\") %&gt;% \n    select(variable, model, Importance)) %&gt;% \n  \n  full_join(varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\")) %&gt;%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n\n\n\n\n\n## reduced:\nranger_red &lt;- saved_models[[3]][[1]]\nxgb_red &lt;- saved_models[[3]][[2]]\ngbm_red &lt;- saved_models[[3]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      826 \nNumber of independent variables:  65 \nMtry:                             28 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        extratrees \nNumber of random splits:          1 \nOOB prediction error (MSE):       0.2534179 \nR squared (OOB):                  0.2028804 \n\nranger_red$results\n\n   mtry min.node.size  splitrule      RMSE  Rsquared       MAE     RMSESD\n1     2             5   variance 0.5320747 0.1600207 0.3170141 0.07035462\n2     2             5 extratrees 0.5402015 0.1490651 0.3229499 0.07203334\n3     5             5   variance 0.5313187 0.1566525 0.3149512 0.06979858\n4     5             5 extratrees 0.5283157 0.1691224 0.3144486 0.07256819\n5     8             5   variance 0.5329553 0.1520773 0.3164223 0.06968052\n6     8             5 extratrees 0.5250628 0.1765168 0.3127621 0.07155879\n7    11             5   variance 0.5339281 0.1501889 0.3171846 0.07116278\n8    11             5 extratrees 0.5234105 0.1809372 0.3121375 0.07059536\n9    15             5   variance 0.5347444 0.1490098 0.3184222 0.07112692\n10   15             5 extratrees 0.5219943 0.1847293 0.3114732 0.06978990\n11   18             5   variance 0.5356479 0.1467067 0.3192681 0.07070322\n12   18             5 extratrees 0.5210211 0.1886790 0.3116950 0.06955238\n13   21             5   variance 0.5359480 0.1462804 0.3198207 0.07120191\n14   21             5 extratrees 0.5206855 0.1892880 0.3114805 0.06957676\n15   25             5   variance 0.5372980 0.1430518 0.3210183 0.07213182\n16   25             5 extratrees 0.5207829 0.1891704 0.3115545 0.06936890\n17   28             5   variance 0.5374314 0.1431692 0.3215648 0.07229645\n18   28             5 extratrees 0.5203579 0.1905113 0.3116032 0.06878515\n19   31             5   variance 0.5377223 0.1430242 0.3222687 0.07247417\n20   31             5 extratrees 0.5205012 0.1901582 0.3118028 0.06845785\n21   35             5   variance 0.5383492 0.1419107 0.3230841 0.07346656\n22   35             5 extratrees 0.5205606 0.1898853 0.3120008 0.06806734\n23   38             5   variance 0.5389178 0.1396532 0.3233412 0.07306683\n24   38             5 extratrees 0.5210864 0.1880556 0.3125096 0.06842888\n25   41             5   variance 0.5392749 0.1398874 0.3240600 0.07367190\n26   41             5 extratrees 0.5212167 0.1874728 0.3123936 0.06772508\n27   45             5   variance 0.5398524 0.1389578 0.3247134 0.07422217\n28   45             5 extratrees 0.5211841 0.1881407 0.3125285 0.06812320\n29   48             5   variance 0.5401299 0.1386446 0.3249213 0.07418648\n30   48             5 extratrees 0.5216228 0.1866076 0.3130290 0.06762264\n31   51             5   variance 0.5403064 0.1384746 0.3251314 0.07463733\n32   51             5 extratrees 0.5217024 0.1864329 0.3133378 0.06797208\n33   55             5   variance 0.5405742 0.1380994 0.3258049 0.07458579\n34   55             5 extratrees 0.5216594 0.1865792 0.3131444 0.06794926\n35   58             5   variance 0.5409279 0.1375885 0.3262961 0.07486114\n36   58             5 extratrees 0.5222240 0.1852020 0.3138892 0.06753329\n37   61             5   variance 0.5413735 0.1365139 0.3266171 0.07533145\n38   61             5 extratrees 0.5225788 0.1838167 0.3137455 0.06771464\n39   65             5   variance 0.5416691 0.1362006 0.3270010 0.07544421\n40   65             5 extratrees 0.5225338 0.1841976 0.3139425 0.06753457\n   RsquaredSD      MAESD\n1  0.08090549 0.03564697\n2  0.06543634 0.03343963\n3  0.07378990 0.03636950\n4  0.07240260 0.03330323\n5  0.07090402 0.03675435\n6  0.07292773 0.03345122\n7  0.07393080 0.03752717\n8  0.07443596 0.03303714\n9  0.07457408 0.03763041\n10 0.07523777 0.03349860\n11 0.07239201 0.03738859\n12 0.07703531 0.03353334\n13 0.07342991 0.03756284\n14 0.07928663 0.03366847\n15 0.07340964 0.03807907\n16 0.08042089 0.03383967\n17 0.07265267 0.03811247\n18 0.07908738 0.03368864\n19 0.07287818 0.03831340\n20 0.07982387 0.03372108\n21 0.07360244 0.03866009\n22 0.07924003 0.03378310\n23 0.07100787 0.03854148\n24 0.07838398 0.03415703\n25 0.07316551 0.03885919\n26 0.07637090 0.03364518\n27 0.07425041 0.03901070\n28 0.07881821 0.03431316\n29 0.07416434 0.03912824\n30 0.07663446 0.03412192\n31 0.07487497 0.03908243\n32 0.07788429 0.03418646\n33 0.07442515 0.03921508\n34 0.07798208 0.03426959\n35 0.07538911 0.03903597\n36 0.07797287 0.03445604\n37 0.07476799 0.03924676\n38 0.07755816 0.03420463\n39 0.07503576 0.03953798\n40 0.07726157 0.03428784\n\nplot(ranger_red)\n\n\n\n\n\n\n\n## XGB ======\nplot(xgb_red)\n\n\n\n\n\n\n\nxgb_red\n\neXtreme Gradient Boosting \n\n826 samples\n 38 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE      \n  0.1  2          0.00   0.5678779  0.1285944  0.3656795\n  0.1  2          0.01   0.5658114  0.1306091  0.3666543\n  0.1  2          0.10   0.5538973  0.1352698  0.3515384\n  0.1  3          0.00   0.5548438  0.1402920  0.3518576\n  0.1  3          0.01   0.5498611  0.1464288  0.3476347\n  0.1  3          0.10   0.5482858  0.1432473  0.3439321\n  0.1  5          0.00   0.5475174  0.1417823  0.3412254\n  0.1  5          0.01   0.5439006  0.1467551  0.3394792\n  0.1  5          0.10   0.5424288  0.1538734  0.3368182\n  0.3  2          0.00   0.5766738  0.1253950  0.3727036\n  0.3  2          0.01   0.5768103  0.1147682  0.3737651\n  0.3  2          0.10   0.5698549  0.1253031  0.3651133\n  0.3  3          0.00   0.5761356  0.1091040  0.3630615\n  0.3  3          0.01   0.5548719  0.1547236  0.3578578\n  0.3  3          0.10   0.5628699  0.1278281  0.3594727\n  0.3  5          0.00   0.5653773  0.1197111  0.3521747\n  0.3  5          0.01   0.5607586  0.1271971  0.3487048\n  0.3  5          0.10   0.5473352  0.1467605  0.3417180\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.1, colsample_bytree = 0.6, min_child_weight = 1 and\n subsample = 1.\n\n## GBM =====\ngbm_red$finalModel\n\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 65 predictors of which 31 had non-zero influence.\n\ngbm_red\n\nStochastic Gradient Boosting \n\n826 samples\n 38 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 662, 662, 662, 662, 662, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared    MAE      \n   1                   50     0.5453209  0.11276686  0.3330431\n   1                  100     0.5411494  0.13314990  0.3350526\n   1                  150     0.5427483  0.13222468  0.3396717\n   1                  200     0.5464531  0.12960125  0.3434847\n   1                  250     0.5481550  0.12903630  0.3465817\n   1                  300     0.5497724  0.12826980  0.3500051\n   1                  350     0.5494769  0.12887435  0.3515606\n   1                  400     0.5563459  0.12222251  0.3570403\n   1                  450     0.5567504  0.12061410  0.3585423\n   1                  500     0.5617986  0.11553853  0.3624929\n   1                  550     0.5637808  0.11299472  0.3635665\n   1                  600     0.5670194  0.10916300  0.3651230\n   1                  650     0.5683427  0.10716338  0.3657090\n   1                  700     0.5708814  0.10468160  0.3683994\n   1                  750     0.5723362  0.10377459  0.3691504\n   1                  800     0.5743099  0.10137179  0.3707726\n   1                  850     0.5763730  0.09855171  0.3720541\n   1                  900     0.5776585  0.10028188  0.3736275\n   1                  950     0.5789854  0.09811917  0.3747662\n   1                 1000     0.5802115  0.10014314  0.3765578\n   2                   50     0.5440630  0.12426624  0.3290501\n   2                  100     0.5499272  0.12212763  0.3388875\n   2                  150     0.5565498  0.11706322  0.3456571\n   2                  200     0.5641916  0.10996731  0.3523291\n   2                  250     0.5674625  0.10787902  0.3561385\n   2                  300     0.5726960  0.10603867  0.3594207\n   2                  350     0.5753277  0.10750726  0.3619415\n   2                  400     0.5776105  0.10460018  0.3646806\n   2                  450     0.5795096  0.10528946  0.3650658\n   2                  500     0.5819660  0.10405460  0.3669300\n   2                  550     0.5844087  0.10107821  0.3694271\n   2                  600     0.5879210  0.09875226  0.3714303\n   2                  650     0.5889186  0.09853639  0.3725080\n   2                  700     0.5915793  0.09583180  0.3742596\n   2                  750     0.5921395  0.09794384  0.3751922\n   2                  800     0.5939875  0.09595940  0.3764275\n   2                  850     0.5952447  0.09487132  0.3771570\n   2                  900     0.5944024  0.09674908  0.3770566\n   2                  950     0.5956077  0.09658741  0.3782954\n   2                 1000     0.5969042  0.09511108  0.3788724\n   3                   50     0.5410392  0.13588204  0.3289230\n   3                  100     0.5533776  0.12680546  0.3414003\n   3                  150     0.5638061  0.11740749  0.3527015\n   3                  200     0.5709343  0.10801164  0.3575787\n   3                  250     0.5735119  0.10786719  0.3602379\n   3                  300     0.5764019  0.11053800  0.3617477\n   3                  350     0.5772037  0.11302853  0.3612185\n   3                  400     0.5818488  0.10710322  0.3635196\n   3                  450     0.5815981  0.10923716  0.3638935\n   3                  500     0.5837401  0.10798408  0.3659239\n   3                  550     0.5851383  0.10917401  0.3672633\n   3                  600     0.5857292  0.10810662  0.3678001\n   3                  650     0.5872661  0.10734404  0.3692470\n   3                  700     0.5880378  0.10739893  0.3703138\n   3                  750     0.5885932  0.10747738  0.3707743\n   3                  800     0.5900401  0.10586809  0.3714613\n   3                  850     0.5905495  0.10561139  0.3718648\n   3                  900     0.5914810  0.10505618  0.3726712\n   3                  950     0.5921080  0.10457601  0.3732081\n   3                 1000     0.5928487  0.10455697  0.3741254\n   4                   50     0.5444219  0.13027568  0.3331173\n   4                  100     0.5576479  0.12247768  0.3452013\n   4                  150     0.5696806  0.10827659  0.3540370\n   4                  200     0.5743478  0.10693725  0.3571289\n   4                  250     0.5804285  0.10262226  0.3612115\n   4                  300     0.5851436  0.09928730  0.3658404\n   4                  350     0.5884409  0.09703926  0.3680604\n   4                  400     0.5892196  0.09815016  0.3687581\n   4                  450     0.5905166  0.09689026  0.3700734\n   4                  500     0.5921847  0.09602372  0.3717528\n   4                  550     0.5943531  0.09485277  0.3741240\n   4                  600     0.5946995  0.09406454  0.3748678\n   4                  650     0.5961893  0.09352154  0.3760616\n   4                  700     0.5970976  0.09274192  0.3767004\n   4                  750     0.5979700  0.09206338  0.3777760\n   4                  800     0.5986542  0.09155883  0.3779312\n   4                  850     0.5991979  0.09109155  0.3781263\n   4                  900     0.6000346  0.09034042  0.3789374\n   4                  950     0.6001051  0.09052130  0.3791347\n   4                 1000     0.6005364  0.09005158  0.3794271\n   5                   50     0.5443950  0.13118248  0.3328844\n   5                  100     0.5622369  0.10984850  0.3469523\n   5                  150     0.5717406  0.10324053  0.3542386\n   5                  200     0.5771477  0.09992301  0.3581685\n   5                  250     0.5808652  0.10021719  0.3619747\n   5                  300     0.5820350  0.10060560  0.3642813\n   5                  350     0.5850374  0.10042490  0.3661130\n   5                  400     0.5870263  0.09888269  0.3677625\n   5                  450     0.5884556  0.09943217  0.3691780\n   5                  500     0.5905174  0.09695466  0.3707680\n   5                  550     0.5909464  0.09736413  0.3718966\n   5                  600     0.5921720  0.09599165  0.3729199\n   5                  650     0.5922132  0.09634844  0.3732672\n   5                  700     0.5924012  0.09634830  0.3737686\n   5                  750     0.5929264  0.09601922  0.3741128\n   5                  800     0.5934031  0.09548468  0.3743741\n   5                  850     0.5935378  0.09535237  0.3745566\n   5                  900     0.5935863  0.09549574  0.3747803\n   5                  950     0.5940690  0.09509605  0.3749366\n   5                 1000     0.5941774  0.09503007  0.3752002\n   6                   50     0.5463812  0.13075363  0.3356204\n   6                  100     0.5635853  0.11427642  0.3489985\n   6                  150     0.5676499  0.11606195  0.3544246\n   6                  200     0.5752242  0.10982679  0.3585567\n   6                  250     0.5749509  0.11177638  0.3597475\n   6                  300     0.5773803  0.11098790  0.3619288\n   6                  350     0.5790786  0.10906313  0.3635821\n   6                  400     0.5808699  0.10811569  0.3653593\n   6                  450     0.5814797  0.10822440  0.3659935\n   6                  500     0.5824142  0.10765385  0.3661235\n   6                  550     0.5832740  0.10643209  0.3670268\n   6                  600     0.5836597  0.10588553  0.3676875\n   6                  650     0.5841380  0.10564793  0.3678690\n   6                  700     0.5849629  0.10456947  0.3687008\n   6                  750     0.5852889  0.10442105  0.3692048\n   6                  800     0.5852480  0.10467103  0.3691186\n   6                  850     0.5853728  0.10461166  0.3693694\n   6                  900     0.5855389  0.10447792  0.3695638\n   6                  950     0.5857330  0.10430570  0.3696827\n   6                 1000     0.5858491  0.10426293  0.3697533\n   7                   50     0.5478909  0.12609556  0.3337780\n   7                  100     0.5595918  0.12107848  0.3484217\n   7                  150     0.5670040  0.11610722  0.3546119\n   7                  200     0.5710763  0.11356550  0.3584137\n   7                  250     0.5738548  0.11102720  0.3603681\n   7                  300     0.5754928  0.11068284  0.3618532\n   7                  350     0.5776298  0.10894878  0.3629420\n   7                  400     0.5784484  0.10831222  0.3635879\n   7                  450     0.5795603  0.10729956  0.3644478\n   7                  500     0.5807213  0.10625747  0.3646954\n   7                  550     0.5811771  0.10591867  0.3647988\n   7                  600     0.5814677  0.10574170  0.3652090\n   7                  650     0.5818583  0.10537949  0.3652956\n   7                  700     0.5820734  0.10530370  0.3656986\n   7                  750     0.5825560  0.10473030  0.3660817\n   7                  800     0.5825511  0.10483975  0.3660106\n   7                  850     0.5827906  0.10454118  0.3662170\n   7                  900     0.5828693  0.10441064  0.3663414\n   7                  950     0.5829886  0.10431509  0.3664187\n   7                 1000     0.5830266  0.10430467  0.3664293\n   8                   50     0.5523564  0.12586920  0.3411786\n   8                  100     0.5690197  0.11122994  0.3559012\n   8                  150     0.5779470  0.10538885  0.3632349\n   8                  200     0.5813751  0.10255625  0.3657659\n   8                  250     0.5846984  0.10043011  0.3677826\n   8                  300     0.5861157  0.09972193  0.3690053\n   8                  350     0.5883513  0.09702352  0.3710379\n   8                  400     0.5895595  0.09649964  0.3719499\n   8                  450     0.5907514  0.09583195  0.3725195\n   8                  500     0.5917408  0.09487975  0.3731959\n   8                  550     0.5919380  0.09444271  0.3733007\n   8                  600     0.5924805  0.09376117  0.3734105\n   8                  650     0.5925623  0.09389444  0.3735469\n   8                  700     0.5930666  0.09347747  0.3740045\n   8                  750     0.5932415  0.09320250  0.3740608\n   8                  800     0.5935403  0.09281725  0.3742281\n   8                  850     0.5936825  0.09274038  0.3743080\n   8                  900     0.5937853  0.09281246  0.3743816\n   8                  950     0.5938222  0.09270083  0.3743452\n   8                 1000     0.5939396  0.09253259  0.3744127\n   9                   50     0.5563687  0.11530772  0.3422639\n   9                  100     0.5692173  0.10327635  0.3519663\n   9                  150     0.5779961  0.09934933  0.3601293\n   9                  200     0.5828063  0.09421369  0.3646538\n   9                  250     0.5873796  0.09061348  0.3676249\n   9                  300     0.5909195  0.08655142  0.3697890\n   9                  350     0.5926151  0.08469021  0.3711098\n   9                  400     0.5935386  0.08352374  0.3722860\n   9                  450     0.5947803  0.08216616  0.3732742\n   9                  500     0.5949191  0.08203437  0.3735599\n   9                  550     0.5951215  0.08193482  0.3737775\n   9                  600     0.5957051  0.08138625  0.3741841\n   9                  650     0.5961081  0.08112482  0.3744294\n   9                  700     0.5962783  0.08110244  0.3745588\n   9                  750     0.5964757  0.08077213  0.3748366\n   9                  800     0.5965272  0.08087575  0.3748803\n   9                  850     0.5966257  0.08081866  0.3749956\n   9                  900     0.5968004  0.08067883  0.3751649\n   9                  950     0.5968574  0.08058286  0.3752239\n   9                 1000     0.5969473  0.08052850  0.3752979\n  10                   50     0.5585276  0.11475001  0.3432607\n  10                  100     0.5739204  0.10760476  0.3561855\n  10                  150     0.5808675  0.10294254  0.3606709\n  10                  200     0.5851172  0.09827250  0.3645676\n  10                  250     0.5876309  0.09793282  0.3658774\n  10                  300     0.5895311  0.09560984  0.3670094\n  10                  350     0.5915264  0.09464707  0.3686008\n  10                  400     0.5923614  0.09398358  0.3692125\n  10                  450     0.5934603  0.09287847  0.3698782\n  10                  500     0.5939678  0.09244280  0.3702435\n  10                  550     0.5945292  0.09175479  0.3707086\n  10                  600     0.5948583  0.09187014  0.3710613\n  10                  650     0.5950586  0.09168525  0.3710830\n  10                  700     0.5951853  0.09144702  0.3711415\n  10                  750     0.5954792  0.09110628  0.3712865\n  10                  800     0.5956088  0.09097161  0.3713197\n  10                  850     0.5957780  0.09071000  0.3713616\n  10                  900     0.5960723  0.09049473  0.3715091\n  10                  950     0.5961418  0.09035351  0.3715490\n  10                 1000     0.5962568  0.09025067  0.3716236\n  11                   50     0.5639931  0.10680744  0.3459628\n  11                  100     0.5772753  0.09402876  0.3542837\n  11                  150     0.5841737  0.09101564  0.3605798\n  11                  200     0.5919958  0.08413469  0.3660693\n  11                  250     0.5949281  0.08155938  0.3684235\n  11                  300     0.5969892  0.07992388  0.3703856\n  11                  350     0.5991508  0.07831475  0.3721080\n  11                  400     0.5991172  0.07903944  0.3719415\n  11                  450     0.6000440  0.07798655  0.3723846\n  11                  500     0.6006022  0.07756201  0.3725514\n  11                  550     0.6009102  0.07719116  0.3727869\n  11                  600     0.6013365  0.07657459  0.3731125\n  11                  650     0.6014612  0.07635450  0.3731959\n  11                  700     0.6019771  0.07596137  0.3734776\n  11                  750     0.6022825  0.07568023  0.3735249\n  11                  800     0.6024224  0.07538872  0.3735851\n  11                  850     0.6026717  0.07505094  0.3736911\n  11                  900     0.6029406  0.07486509  0.3737900\n  11                  950     0.6031083  0.07477807  0.3738129\n  11                 1000     0.6032737  0.07451094  0.3738814\n  12                   50     0.5583360  0.12073746  0.3421563\n  12                  100     0.5745297  0.10755002  0.3563421\n  12                  150     0.5845424  0.10090200  0.3631475\n  12                  200     0.5906087  0.09478600  0.3672328\n  12                  250     0.5928565  0.09500117  0.3690731\n  12                  300     0.5948135  0.09253340  0.3700709\n  12                  350     0.5963781  0.09170245  0.3710345\n  12                  400     0.5975501  0.09062917  0.3716502\n  12                  450     0.5983625  0.09008512  0.3721548\n  12                  500     0.5989921  0.08961780  0.3725404\n  12                  550     0.5988918  0.08949913  0.3725463\n  12                  600     0.5992446  0.08921890  0.3726716\n  12                  650     0.5996385  0.08896780  0.3728582\n  12                  700     0.6001016  0.08852763  0.3730779\n  12                  750     0.6000472  0.08877987  0.3729958\n  12                  800     0.6003254  0.08848207  0.3731727\n  12                  850     0.6005529  0.08831577  0.3732504\n  12                  900     0.6006412  0.08829242  0.3733424\n  12                  950     0.6009422  0.08784738  0.3734805\n  12                 1000     0.6010598  0.08773740  0.3735000\n  13                   50     0.5551587  0.12016745  0.3423942\n  13                  100     0.5707515  0.11244470  0.3559943\n  13                  150     0.5785372  0.10390353  0.3624828\n  13                  200     0.5811777  0.10303882  0.3653982\n  13                  250     0.5840305  0.09926156  0.3664499\n  13                  300     0.5854905  0.09824418  0.3677243\n  13                  350     0.5880952  0.09571832  0.3692982\n  13                  400     0.5894755  0.09398528  0.3701120\n  13                  450     0.5897342  0.09435716  0.3707219\n  13                  500     0.5908199  0.09285631  0.3712537\n  13                  550     0.5915571  0.09221136  0.3715800\n  13                  600     0.5918141  0.09211228  0.3718835\n  13                  650     0.5924270  0.09122910  0.3722037\n  13                  700     0.5927368  0.09103374  0.3723603\n  13                  750     0.5930038  0.09069762  0.3725468\n  13                  800     0.5933188  0.09010738  0.3726363\n  13                  850     0.5934733  0.08995927  0.3727239\n  13                  900     0.5936346  0.08977685  0.3728276\n  13                  950     0.5938233  0.08949740  0.3728881\n  13                 1000     0.5938868  0.08949223  0.3729242\n  14                   50     0.5548040  0.11770271  0.3413240\n  14                  100     0.5673774  0.11344807  0.3538676\n  14                  150     0.5768105  0.10397719  0.3601051\n  14                  200     0.5829904  0.09848051  0.3638240\n  14                  250     0.5864827  0.09681275  0.3664618\n  14                  300     0.5890569  0.09307091  0.3679592\n  14                  350     0.5901545  0.09170456  0.3684894\n  14                  400     0.5919491  0.09040603  0.3695066\n  14                  450     0.5937192  0.08844721  0.3703378\n  14                  500     0.5946707  0.08735185  0.3709686\n  14                  550     0.5952045  0.08670526  0.3711476\n  14                  600     0.5961833  0.08590756  0.3718125\n  14                  650     0.5964806  0.08561896  0.3719604\n  14                  700     0.5968289  0.08510170  0.3720568\n  14                  750     0.5970696  0.08508368  0.3722736\n  14                  800     0.5975294  0.08447190  0.3724847\n  14                  850     0.5979976  0.08428179  0.3727531\n  14                  900     0.5980134  0.08409285  0.3727326\n  14                  950     0.5981583  0.08394068  0.3728827\n  14                 1000     0.5983978  0.08368324  0.3730040\n  15                   50     0.5528125  0.12820457  0.3400189\n  15                  100     0.5685021  0.12037565  0.3522105\n  15                  150     0.5757411  0.11378866  0.3585352\n  15                  200     0.5804401  0.10979712  0.3628368\n  15                  250     0.5835773  0.10721830  0.3650292\n  15                  300     0.5852602  0.10531309  0.3658620\n  15                  350     0.5872909  0.10313677  0.3668085\n  15                  400     0.5879709  0.10226976  0.3675585\n  15                  450     0.5891570  0.10100887  0.3682194\n  15                  500     0.5903144  0.10000720  0.3686548\n  15                  550     0.5908475  0.09963579  0.3688267\n  15                  600     0.5910235  0.09924186  0.3688363\n  15                  650     0.5915252  0.09861472  0.3691664\n  15                  700     0.5921822  0.09795344  0.3694418\n  15                  750     0.5924998  0.09765016  0.3696287\n  15                  800     0.5926619  0.09752606  0.3697795\n  15                  850     0.5930280  0.09687002  0.3699757\n  15                  900     0.5933279  0.09680841  0.3700959\n  15                  950     0.5934832  0.09636745  0.3702184\n  15                 1000     0.5935980  0.09627017  0.3703229\n  16                   50     0.5550912  0.12315054  0.3421371\n  16                  100     0.5683244  0.11382871  0.3536149\n  16                  150     0.5778940  0.10741484  0.3599092\n  16                  200     0.5823046  0.10361431  0.3623247\n  16                  250     0.5855040  0.10094928  0.3646521\n  16                  300     0.5873605  0.10003270  0.3657116\n  16                  350     0.5897280  0.09769164  0.3673586\n  16                  400     0.5908082  0.09638033  0.3677169\n  16                  450     0.5918074  0.09602485  0.3682480\n  16                  500     0.5926390  0.09521776  0.3684173\n  16                  550     0.5937057  0.09376815  0.3689461\n  16                  600     0.5941788  0.09326162  0.3691744\n  16                  650     0.5943592  0.09280916  0.3693076\n  16                  700     0.5947568  0.09225621  0.3695166\n  16                  750     0.5949403  0.09208724  0.3695637\n  16                  800     0.5952754  0.09163793  0.3697149\n  16                  850     0.5956522  0.09114995  0.3698401\n  16                  900     0.5956991  0.09109746  0.3698880\n  16                  950     0.5959031  0.09103893  0.3699368\n  16                 1000     0.5958828  0.09090244  0.3699651\n  17                   50     0.5545557  0.12166054  0.3393389\n  17                  100     0.5692239  0.11046381  0.3537977\n  17                  150     0.5770709  0.10234490  0.3588184\n  17                  200     0.5816371  0.09791505  0.3628589\n  17                  250     0.5833785  0.09829753  0.3642594\n  17                  300     0.5856648  0.09458468  0.3653939\n  17                  350     0.5867969  0.09357249  0.3661471\n  17                  400     0.5876315  0.09277722  0.3666249\n  17                  450     0.5891141  0.09084876  0.3676131\n  17                  500     0.5898168  0.09053690  0.3679031\n  17                  550     0.5904756  0.08992150  0.3681307\n  17                  600     0.5907264  0.09000211  0.3682468\n  17                  650     0.5911765  0.08959616  0.3685972\n  17                  700     0.5912397  0.08953562  0.3685547\n  17                  750     0.5917070  0.08906404  0.3687844\n  17                  800     0.5915780  0.08911650  0.3687076\n  17                  850     0.5917135  0.08895452  0.3688157\n  17                  900     0.5922291  0.08854989  0.3690603\n  17                  950     0.5923325  0.08842640  0.3691157\n  17                 1000     0.5924475  0.08826513  0.3691790\n  18                   50     0.5505142  0.12568496  0.3435025\n  18                  100     0.5673477  0.11190458  0.3553793\n  18                  150     0.5757013  0.10236183  0.3615161\n  18                  200     0.5803042  0.09939531  0.3651018\n  18                  250     0.5819240  0.09933722  0.3662938\n  18                  300     0.5839017  0.09807934  0.3677958\n  18                  350     0.5856681  0.09630623  0.3687657\n  18                  400     0.5856128  0.09652281  0.3688245\n  18                  450     0.5867742  0.09519598  0.3693696\n  18                  500     0.5879574  0.09488534  0.3698572\n  18                  550     0.5891588  0.09384420  0.3701872\n  18                  600     0.5894770  0.09314256  0.3704683\n  18                  650     0.5898895  0.09275259  0.3707015\n  18                  700     0.5904731  0.09249504  0.3709105\n  18                  750     0.5906551  0.09196746  0.3710118\n  18                  800     0.5909491  0.09158392  0.3712122\n  18                  850     0.5912767  0.09135489  0.3713378\n  18                  900     0.5915499  0.09106990  0.3714766\n  18                  950     0.5916282  0.09081519  0.3714679\n  18                 1000     0.5916689  0.09075198  0.3715015\n  19                   50     0.5568388  0.12501886  0.3440201\n  19                  100     0.5772961  0.10421439  0.3587060\n  19                  150     0.5849819  0.09890322  0.3638665\n  19                  200     0.5897435  0.09563993  0.3677138\n  19                  250     0.5931317  0.09362189  0.3692032\n  19                  300     0.5955103  0.09370107  0.3705583\n  19                  350     0.5971513  0.09233041  0.3716963\n  19                  400     0.5984851  0.09138619  0.3723932\n  19                  450     0.5992056  0.09095749  0.3728553\n  19                  500     0.6006882  0.08975024  0.3737571\n  19                  550     0.6015137  0.08909809  0.3741917\n  19                  600     0.6019959  0.08847557  0.3745817\n  19                  650     0.6024821  0.08843409  0.3748256\n  19                  700     0.6031734  0.08765087  0.3751474\n  19                  750     0.6034751  0.08731235  0.3754161\n  19                  800     0.6038583  0.08701007  0.3756354\n  19                  850     0.6041995  0.08682277  0.3757715\n  19                  900     0.6043533  0.08660278  0.3759177\n  19                  950     0.6045401  0.08639963  0.3759935\n  19                 1000     0.6048450  0.08626342  0.3761410\n  20                   50     0.5555047  0.12189357  0.3367630\n  20                  100     0.5720407  0.10797435  0.3526250\n  20                  150     0.5769324  0.10461912  0.3572071\n  20                  200     0.5837022  0.09677899  0.3611854\n  20                  250     0.5858576  0.09675706  0.3625016\n  20                  300     0.5897003  0.09442901  0.3644864\n  20                  350     0.5916716  0.09281047  0.3657568\n  20                  400     0.5928146  0.09122657  0.3664630\n  20                  450     0.5942447  0.09000727  0.3673727\n  20                  500     0.5952762  0.08937064  0.3678512\n  20                  550     0.5962510  0.08886451  0.3682580\n  20                  600     0.5972056  0.08792226  0.3688632\n  20                  650     0.5977219  0.08755390  0.3692368\n  20                  700     0.5978585  0.08721149  0.3692640\n  20                  750     0.5984538  0.08678052  0.3695976\n  20                  800     0.5983658  0.08657289  0.3695823\n  20                  850     0.5986457  0.08621178  0.3696953\n  20                  900     0.5988585  0.08604269  0.3698326\n  20                  950     0.5991466  0.08589197  0.3699546\n  20                 1000     0.5993148  0.08577172  0.3699632\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10.\n\nmodels_LR1 &lt;- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n\n\n## Importance frame (wide format) ===\nimportance_wide &lt;- cbind(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_ranger = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_gbm\" = \"Overall\"))  %&gt;% \n  kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_LR1 &lt;- full_join(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Importance\" = \"rel.inf\") %&gt;% \n    select(variable, model, Importance)) %&gt;% \n  full_join(varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\")) %&gt;%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n\n\n\n\n\n## reduced:\nranger_red &lt;- saved_models[[4]][[1]]\nxgb_red &lt;- saved_models[[4]][[2]]\ngbm_red &lt;- saved_models[[4]][[3]]\n\n## Ranger =====\nranger_red$finalModel\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = \".outcome\", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) \n\nType:                             Regression \nNumber of trees:                  5000 \nSample size:                      823 \nNumber of independent variables:  55 \nMtry:                             52 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        extratrees \nNumber of random splits:          1 \nOOB prediction error (MSE):       0.260595 \nR squared (OOB):                  0.1607592 \n\nranger_red$results\n\n   mtry min.node.size  splitrule      RMSE  Rsquared       MAE     RMSESD\n1     2             5   variance 0.5394176 0.1827108 0.3306246 0.04327289\n2     2             5 extratrees 0.5523844 0.1492828 0.3349185 0.04477997\n3     4             5   variance 0.5363103 0.1825704 0.3298550 0.04310065\n4     4             5 extratrees 0.5442179 0.1589400 0.3306828 0.04483804\n5     7             5   variance 0.5352169 0.1825738 0.3300774 0.04234869\n6     7             5 extratrees 0.5406468 0.1671234 0.3292856 0.04488829\n7    10             5   variance 0.5351361 0.1822525 0.3302695 0.04206719\n8    10             5 extratrees 0.5391743 0.1707950 0.3288732 0.04492560\n9    13             5   variance 0.5350785 0.1815348 0.3307148 0.04079842\n10   13             5 extratrees 0.5373425 0.1760238 0.3287188 0.04477033\n11   15             5   variance 0.5350091 0.1816557 0.3307483 0.04098157\n12   15             5 extratrees 0.5373931 0.1748776 0.3291373 0.04448990\n13   18             5   variance 0.5351410 0.1810147 0.3312523 0.04113763\n14   18             5 extratrees 0.5362650 0.1789442 0.3288775 0.04440944\n15   21             5   variance 0.5352901 0.1808704 0.3318494 0.04104371\n16   21             5 extratrees 0.5357438 0.1799888 0.3291477 0.04424566\n17   24             5   variance 0.5358828 0.1789333 0.3322976 0.04044356\n18   24             5 extratrees 0.5354245 0.1809650 0.3292783 0.04373709\n19   27             5   variance 0.5361697 0.1778851 0.3326148 0.04012501\n20   27             5 extratrees 0.5350873 0.1816643 0.3292259 0.04383781\n21   29             5   variance 0.5357883 0.1788138 0.3328233 0.03961067\n22   29             5 extratrees 0.5349069 0.1817657 0.3295423 0.04327749\n23   32             5   variance 0.5361444 0.1778585 0.3332305 0.03940203\n24   32             5 extratrees 0.5345948 0.1827137 0.3291906 0.04292078\n25   35             5   variance 0.5361837 0.1778279 0.3332777 0.03939331\n26   35             5 extratrees 0.5347364 0.1823905 0.3294745 0.04321173\n27   38             5   variance 0.5360743 0.1786488 0.3333957 0.03961153\n28   38             5 extratrees 0.5343217 0.1841272 0.3297860 0.04340730\n29   41             5   variance 0.5369598 0.1759463 0.3341374 0.03908748\n30   41             5 extratrees 0.5344115 0.1835345 0.3296798 0.04261963\n31   43             5   variance 0.5363456 0.1775281 0.3339625 0.03935771\n32   43             5 extratrees 0.5340870 0.1844424 0.3298840 0.04273121\n33   46             5   variance 0.5366504 0.1767557 0.3342629 0.03928290\n34   46             5 extratrees 0.5343518 0.1836526 0.3303575 0.04264377\n35   49             5   variance 0.5366978 0.1771860 0.3346697 0.03888073\n36   49             5 extratrees 0.5339151 0.1850448 0.3301933 0.04256041\n37   52             5   variance 0.5359135 0.1793983 0.3342563 0.03911702\n38   52             5 extratrees 0.5335750 0.1858403 0.3301628 0.04240879\n39   55             5   variance 0.5362471 0.1780607 0.3345988 0.03885057\n40   55             5 extratrees 0.5337027 0.1853486 0.3301865 0.04226864\n   RsquaredSD      MAESD\n1  0.07045222 0.02131811\n2  0.05953501 0.02004773\n3  0.07023492 0.02281210\n4  0.05955095 0.01969794\n5  0.07092028 0.02308931\n6  0.06235290 0.02047200\n7  0.07306720 0.02322794\n8  0.06392628 0.02089837\n9  0.07118503 0.02303430\n10 0.06591139 0.02102673\n11 0.07084756 0.02321266\n12 0.06500377 0.02102702\n13 0.07054534 0.02360130\n14 0.06815576 0.02128205\n15 0.07034732 0.02356806\n16 0.06681332 0.02169639\n17 0.06887006 0.02313728\n18 0.06743650 0.02127323\n19 0.06954124 0.02317380\n20 0.06823480 0.02166295\n21 0.06938808 0.02289528\n22 0.06717462 0.02158153\n23 0.06908141 0.02317079\n24 0.06730542 0.02132558\n25 0.06915109 0.02318020\n26 0.06799370 0.02176180\n27 0.07029267 0.02333715\n28 0.07016370 0.02218647\n29 0.06979509 0.02303449\n30 0.06860967 0.02212117\n31 0.06938452 0.02298471\n32 0.06836062 0.02216306\n33 0.06832239 0.02317263\n34 0.06949483 0.02228954\n35 0.07105095 0.02293741\n36 0.07033181 0.02231595\n37 0.07021162 0.02320422\n38 0.06829109 0.02241744\n39 0.06923502 0.02327748\n40 0.06806453 0.02226296\n\nplot(ranger_red)\n\n\n\n\n\n\n\n## XGB ======\nplot(xgb_red)\n\n\n\n\n\n\n\nxgb_red\n\neXtreme Gradient Boosting \n\n823 samples\n 33 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 659, 659, 659, 659, 659, 659, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  gamma  RMSE       Rsquared   MAE      \n  0.1  2          0.00   0.5652899  0.1370843  0.3599389\n  0.1  2          0.01   0.5672961  0.1358266  0.3639662\n  0.1  2          0.10   0.5537492  0.1468136  0.3521526\n  0.1  3          0.00   0.5546626  0.1508666  0.3553504\n  0.1  3          0.01   0.5564022  0.1489859  0.3544660\n  0.1  3          0.10   0.5486632  0.1611224  0.3496976\n  0.1  5          0.00   0.5502365  0.1535777  0.3449824\n  0.1  5          0.01   0.5491901  0.1488905  0.3497933\n  0.1  5          0.10   0.5452046  0.1597737  0.3436322\n  0.3  2          0.00   0.5818567  0.1225535  0.3773395\n  0.3  2          0.01   0.5733998  0.1315226  0.3687969\n  0.3  2          0.10   0.5629095  0.1430002  0.3608346\n  0.3  3          0.00   0.5659087  0.1453970  0.3692202\n  0.3  3          0.01   0.5717958  0.1293219  0.3664367\n  0.3  3          0.10   0.5608630  0.1385928  0.3602765\n  0.3  5          0.00   0.5592023  0.1381370  0.3563390\n  0.3  5          0.01   0.5589321  0.1422411  0.3560095\n  0.3  5          0.10   0.5624472  0.1341673  0.3527788\n\nTuning parameter 'nrounds' was held constant at a value of 1000\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nrounds = 1000, max_depth = 5, eta\n = 0.1, gamma = 0.1, colsample_bytree = 0.6, min_child_weight = 1 and\n subsample = 1.\n\n## GBM =====\ngbm_red$finalModel\n\nA gradient boosted model with gaussian loss function.\n50 iterations were performed.\nThere were 55 predictors of which 32 had non-zero influence.\n\ngbm_red\n\nStochastic Gradient Boosting \n\n823 samples\n 33 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 659, 659, 659, 659, 659, 659, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  RMSE       Rsquared   MAE      \n   1                   50     0.5435340  0.1725076  0.3401265\n   1                  100     0.5386692  0.1778030  0.3401935\n   1                  150     0.5394407  0.1746568  0.3421430\n   1                  200     0.5423092  0.1661049  0.3456837\n   1                  250     0.5429783  0.1619924  0.3483008\n   1                  300     0.5445351  0.1597740  0.3506808\n   1                  350     0.5461206  0.1592224  0.3536882\n   1                  400     0.5474695  0.1562333  0.3560610\n   1                  450     0.5492929  0.1524881  0.3585873\n   1                  500     0.5482417  0.1566053  0.3596622\n   1                  550     0.5488017  0.1556647  0.3604591\n   1                  600     0.5510617  0.1528386  0.3630761\n   1                  650     0.5510306  0.1526667  0.3643218\n   1                  700     0.5522670  0.1508417  0.3668228\n   1                  750     0.5542436  0.1480501  0.3698860\n   1                  800     0.5546132  0.1481969  0.3700358\n   1                  850     0.5569744  0.1448902  0.3718863\n   1                  900     0.5591735  0.1424688  0.3741230\n   1                  950     0.5615479  0.1388344  0.3757025\n   1                 1000     0.5609431  0.1399510  0.3745292\n   2                   50     0.5420891  0.1619915  0.3410091\n   2                  100     0.5446668  0.1521755  0.3486964\n   2                  150     0.5419179  0.1653923  0.3493598\n   2                  200     0.5475804  0.1569544  0.3549530\n   2                  250     0.5495773  0.1560494  0.3597635\n   2                  300     0.5522220  0.1534553  0.3615395\n   2                  350     0.5571143  0.1450038  0.3645586\n   2                  400     0.5586398  0.1440451  0.3660280\n   2                  450     0.5627815  0.1389216  0.3704415\n   2                  500     0.5652501  0.1359031  0.3726427\n   2                  550     0.5662805  0.1350053  0.3737641\n   2                  600     0.5689036  0.1308502  0.3769858\n   2                  650     0.5710852  0.1284044  0.3786963\n   2                  700     0.5722107  0.1271664  0.3800573\n   2                  750     0.5737294  0.1261726  0.3813939\n   2                  800     0.5747590  0.1256980  0.3822457\n   2                  850     0.5748160  0.1274190  0.3821215\n   2                  900     0.5755890  0.1267643  0.3830680\n   2                  950     0.5764352  0.1253102  0.3834816\n   2                 1000     0.5786441  0.1232617  0.3849067\n   3                   50     0.5404720  0.1657643  0.3396822\n   3                  100     0.5418197  0.1688569  0.3456548\n   3                  150     0.5445996  0.1628527  0.3489217\n   3                  200     0.5451721  0.1677858  0.3530574\n   3                  250     0.5508788  0.1590540  0.3589441\n   3                  300     0.5558333  0.1523450  0.3627655\n   3                  350     0.5587983  0.1472432  0.3655728\n   3                  400     0.5624810  0.1418606  0.3673547\n   3                  450     0.5664457  0.1359185  0.3695412\n   3                  500     0.5694455  0.1324511  0.3723059\n   3                  550     0.5708555  0.1321255  0.3737038\n   3                  600     0.5706920  0.1334693  0.3735465\n   3                  650     0.5722406  0.1318835  0.3751978\n   3                  700     0.5739100  0.1300385  0.3766772\n   3                  750     0.5755412  0.1277036  0.3778382\n   3                  800     0.5765745  0.1262573  0.3784540\n   3                  850     0.5778049  0.1246908  0.3793165\n   3                  900     0.5787741  0.1237804  0.3806852\n   3                  950     0.5787050  0.1243689  0.3808195\n   3                 1000     0.5789757  0.1246378  0.3812826\n   4                   50     0.5392465  0.1690900  0.3398406\n   4                  100     0.5437410  0.1631470  0.3481872\n   4                  150     0.5472492  0.1596970  0.3537895\n   4                  200     0.5522099  0.1517441  0.3599104\n   4                  250     0.5552790  0.1508825  0.3615048\n   4                  300     0.5570733  0.1493878  0.3639653\n   4                  350     0.5607036  0.1449125  0.3664775\n   4                  400     0.5611736  0.1461306  0.3667845\n   4                  450     0.5644857  0.1409842  0.3682662\n   4                  500     0.5657666  0.1400594  0.3695611\n   4                  550     0.5661958  0.1404224  0.3694681\n   4                  600     0.5673929  0.1393933  0.3701647\n   4                  650     0.5675766  0.1393574  0.3699474\n   4                  700     0.5685654  0.1382364  0.3707117\n   4                  750     0.5698115  0.1366572  0.3721179\n   4                  800     0.5706100  0.1361610  0.3726183\n   4                  850     0.5709354  0.1359264  0.3728772\n   4                  900     0.5715999  0.1352643  0.3731675\n   4                  950     0.5725604  0.1334728  0.3736229\n   4                 1000     0.5725949  0.1336090  0.3737679\n   5                   50     0.5517576  0.1418638  0.3492837\n   5                  100     0.5557799  0.1457220  0.3573030\n   5                  150     0.5597292  0.1443120  0.3622145\n   5                  200     0.5647009  0.1382170  0.3676612\n   5                  250     0.5666992  0.1367723  0.3702063\n   5                  300     0.5706818  0.1323741  0.3740871\n   5                  350     0.5737315  0.1288594  0.3766271\n   5                  400     0.5736835  0.1300122  0.3767334\n   5                  450     0.5748104  0.1294181  0.3777935\n   5                  500     0.5753617  0.1291891  0.3777419\n   5                  550     0.5765608  0.1273073  0.3785762\n   5                  600     0.5776865  0.1259324  0.3794708\n   5                  650     0.5783598  0.1249584  0.3796550\n   5                  700     0.5785844  0.1252472  0.3801715\n   5                  750     0.5790990  0.1248671  0.3809436\n   5                  800     0.5797999  0.1240231  0.3813883\n   5                  850     0.5799455  0.1240017  0.3815767\n   5                  900     0.5802683  0.1236322  0.3819464\n   5                  950     0.5805969  0.1230546  0.3822003\n   5                 1000     0.5806948  0.1230670  0.3823632\n   6                   50     0.5462504  0.1561230  0.3452123\n   6                  100     0.5514706  0.1491707  0.3570281\n   6                  150     0.5552894  0.1458182  0.3603644\n   6                  200     0.5600065  0.1400114  0.3642936\n   6                  250     0.5645479  0.1338760  0.3676793\n   6                  300     0.5670276  0.1304356  0.3692870\n   6                  350     0.5672593  0.1310641  0.3704915\n   6                  400     0.5696334  0.1290703  0.3723679\n   6                  450     0.5704897  0.1288122  0.3734561\n   6                  500     0.5706045  0.1290527  0.3737405\n   6                  550     0.5718385  0.1273220  0.3744323\n   6                  600     0.5727764  0.1260565  0.3751280\n   6                  650     0.5732075  0.1256415  0.3757613\n   6                  700     0.5732564  0.1258814  0.3761576\n   6                  750     0.5735114  0.1258230  0.3763305\n   6                  800     0.5732936  0.1262874  0.3762633\n   6                  850     0.5730912  0.1268145  0.3762173\n   6                  900     0.5734541  0.1263761  0.3764389\n   6                  950     0.5734033  0.1266493  0.3764823\n   6                 1000     0.5733968  0.1267272  0.3766440\n   7                   50     0.5378094  0.1752265  0.3441194\n   7                  100     0.5433111  0.1747641  0.3504528\n   7                  150     0.5479397  0.1717937  0.3556577\n   7                  200     0.5523310  0.1657099  0.3596427\n   7                  250     0.5555076  0.1613582  0.3619909\n   7                  300     0.5575027  0.1584420  0.3638399\n   7                  350     0.5591596  0.1568336  0.3651577\n   7                  400     0.5602831  0.1557860  0.3656587\n   7                  450     0.5612796  0.1544388  0.3659612\n   7                  500     0.5619919  0.1532327  0.3666425\n   7                  550     0.5630979  0.1516573  0.3676396\n   7                  600     0.5634200  0.1516095  0.3678125\n   7                  650     0.5638781  0.1510006  0.3681610\n   7                  700     0.5639960  0.1507522  0.3682634\n   7                  750     0.5640180  0.1507833  0.3684074\n   7                  800     0.5641614  0.1506221  0.3684997\n   7                  850     0.5642576  0.1505786  0.3685778\n   7                  900     0.5643929  0.1503901  0.3686600\n   7                  950     0.5644964  0.1502899  0.3687694\n   7                 1000     0.5644640  0.1503958  0.3688119\n   8                   50     0.5465278  0.1556335  0.3486363\n   8                  100     0.5536872  0.1513301  0.3596422\n   8                  150     0.5584303  0.1480592  0.3639517\n   8                  200     0.5633647  0.1405858  0.3672596\n   8                  250     0.5669173  0.1354042  0.3702147\n   8                  300     0.5693604  0.1323792  0.3716203\n   8                  350     0.5703517  0.1316742  0.3729022\n   8                  400     0.5709676  0.1323827  0.3740529\n   8                  450     0.5722419  0.1307272  0.3747663\n   8                  500     0.5732558  0.1298752  0.3752286\n   8                  550     0.5739770  0.1289958  0.3759073\n   8                  600     0.5740893  0.1291631  0.3759533\n   8                  650     0.5740911  0.1295278  0.3760576\n   8                  700     0.5745476  0.1289141  0.3764545\n   8                  750     0.5745546  0.1291928  0.3764068\n   8                  800     0.5744373  0.1295084  0.3761990\n   8                  850     0.5745369  0.1294330  0.3762800\n   8                  900     0.5745762  0.1294067  0.3763718\n   8                  950     0.5746131  0.1294200  0.3764258\n   8                 1000     0.5746798  0.1293376  0.3765098\n   9                   50     0.5397554  0.1768093  0.3470516\n   9                  100     0.5458968  0.1715482  0.3552530\n   9                  150     0.5513230  0.1659488  0.3601858\n   9                  200     0.5559849  0.1594670  0.3634127\n   9                  250     0.5582653  0.1579514  0.3654427\n   9                  300     0.5599125  0.1553335  0.3665832\n   9                  350     0.5610806  0.1543081  0.3682041\n   9                  400     0.5615299  0.1541571  0.3687943\n   9                  450     0.5616859  0.1544125  0.3688973\n   9                  500     0.5620566  0.1538690  0.3693203\n   9                  550     0.5627483  0.1527296  0.3699013\n   9                  600     0.5628900  0.1526717  0.3698823\n   9                  650     0.5631089  0.1524520  0.3700319\n   9                  700     0.5630597  0.1524914  0.3699887\n   9                  750     0.5632000  0.1524935  0.3701253\n   9                  800     0.5632287  0.1524949  0.3700930\n   9                  850     0.5632631  0.1525165  0.3700578\n   9                  900     0.5632745  0.1525503  0.3700584\n   9                  950     0.5633185  0.1524831  0.3701258\n   9                 1000     0.5632658  0.1525950  0.3700872\n  10                   50     0.5479329  0.1595555  0.3473598\n  10                  100     0.5560735  0.1498948  0.3590347\n  10                  150     0.5627648  0.1424792  0.3656957\n  10                  200     0.5670025  0.1367890  0.3694764\n  10                  250     0.5689548  0.1341975  0.3705474\n  10                  300     0.5716682  0.1305974  0.3731400\n  10                  350     0.5723019  0.1305110  0.3739690\n  10                  400     0.5730169  0.1304022  0.3749035\n  10                  450     0.5738092  0.1290531  0.3755303\n  10                  500     0.5740077  0.1292188  0.3758022\n  10                  550     0.5741381  0.1292520  0.3759381\n  10                  600     0.5742190  0.1292827  0.3759729\n  10                  650     0.5743181  0.1291403  0.3760833\n  10                  700     0.5743132  0.1291837  0.3761027\n  10                  750     0.5743356  0.1292262  0.3761364\n  10                  800     0.5744417  0.1290907  0.3762873\n  10                  850     0.5745377  0.1289574  0.3763350\n  10                  900     0.5746054  0.1288473  0.3763628\n  10                  950     0.5746386  0.1288578  0.3764010\n  10                 1000     0.5746380  0.1288680  0.3763941\n  11                   50     0.5441818  0.1606796  0.3482712\n  11                  100     0.5497133  0.1583966  0.3576095\n  11                  150     0.5560228  0.1493726  0.3621946\n  11                  200     0.5596621  0.1454334  0.3645923\n  11                  250     0.5626085  0.1423597  0.3666927\n  11                  300     0.5641731  0.1402532  0.3676519\n  11                  350     0.5652659  0.1396496  0.3690342\n  11                  400     0.5660220  0.1385469  0.3696948\n  11                  450     0.5664049  0.1384240  0.3702493\n  11                  500     0.5668304  0.1377082  0.3705263\n  11                  550     0.5671592  0.1373371  0.3707951\n  11                  600     0.5674809  0.1369477  0.3709850\n  11                  650     0.5674525  0.1370396  0.3709618\n  11                  700     0.5676150  0.1368011  0.3710817\n  11                  750     0.5677128  0.1366422  0.3711927\n  11                  800     0.5676845  0.1368104  0.3712545\n  11                  850     0.5677508  0.1367157  0.3712690\n  11                  900     0.5677400  0.1368046  0.3713302\n  11                  950     0.5677959  0.1367216  0.3713916\n  11                 1000     0.5677918  0.1367871  0.3714061\n  12                   50     0.5486969  0.1557104  0.3514358\n  12                  100     0.5552754  0.1516340  0.3599501\n  12                  150     0.5620314  0.1434373  0.3644974\n  12                  200     0.5675687  0.1349275  0.3682724\n  12                  250     0.5680101  0.1358575  0.3695771\n  12                  300     0.5683923  0.1363312  0.3702252\n  12                  350     0.5698257  0.1346120  0.3713010\n  12                  400     0.5708860  0.1329419  0.3720778\n  12                  450     0.5713183  0.1323765  0.3725445\n  12                  500     0.5716847  0.1318076  0.3726861\n  12                  550     0.5717026  0.1321116  0.3727598\n  12                  600     0.5717443  0.1321932  0.3728891\n  12                  650     0.5719468  0.1319131  0.3730767\n  12                  700     0.5718986  0.1321071  0.3731357\n  12                  750     0.5719118  0.1319763  0.3731030\n  12                  800     0.5720270  0.1318131  0.3732050\n  12                  850     0.5722275  0.1315990  0.3732938\n  12                  900     0.5722825  0.1315665  0.3733747\n  12                  950     0.5722455  0.1316775  0.3734095\n  12                 1000     0.5722097  0.1318034  0.3734055\n  13                   50     0.5402188  0.1720905  0.3485329\n  13                  100     0.5522713  0.1518652  0.3598521\n  13                  150     0.5571034  0.1490147  0.3640914\n  13                  200     0.5591038  0.1476936  0.3670438\n  13                  250     0.5613849  0.1458476  0.3687285\n  13                  300     0.5627262  0.1450748  0.3703304\n  13                  350     0.5637731  0.1439518  0.3713033\n  13                  400     0.5645554  0.1429202  0.3723207\n  13                  450     0.5651612  0.1420776  0.3728375\n  13                  500     0.5655916  0.1415256  0.3731084\n  13                  550     0.5658390  0.1413508  0.3732993\n  13                  600     0.5660754  0.1410674  0.3734960\n  13                  650     0.5661553  0.1412253  0.3736626\n  13                  700     0.5662425  0.1411439  0.3737476\n  13                  750     0.5663400  0.1410830  0.3737879\n  13                  800     0.5663475  0.1411831  0.3738168\n  13                  850     0.5664854  0.1409492  0.3739146\n  13                  900     0.5665471  0.1409752  0.3740156\n  13                  950     0.5665988  0.1409574  0.3740534\n  13                 1000     0.5666573  0.1409706  0.3740807\n  14                   50     0.5466903  0.1593075  0.3485899\n  14                  100     0.5538054  0.1531953  0.3569143\n  14                  150     0.5606127  0.1446332  0.3627351\n  14                  200     0.5640223  0.1412382  0.3657916\n  14                  250     0.5665972  0.1386975  0.3672909\n  14                  300     0.5672575  0.1386501  0.3678960\n  14                  350     0.5679869  0.1381003  0.3685011\n  14                  400     0.5688754  0.1365875  0.3692443\n  14                  450     0.5692982  0.1361094  0.3696157\n  14                  500     0.5696525  0.1356693  0.3700169\n  14                  550     0.5698437  0.1356705  0.3700207\n  14                  600     0.5701022  0.1355239  0.3702518\n  14                  650     0.5701231  0.1357210  0.3702273\n  14                  700     0.5702763  0.1356363  0.3703548\n  14                  750     0.5703191  0.1357390  0.3704201\n  14                  800     0.5704118  0.1357495  0.3705184\n  14                  850     0.5704669  0.1357367  0.3705517\n  14                  900     0.5705359  0.1357101  0.3706261\n  14                  950     0.5705255  0.1357297  0.3706455\n  14                 1000     0.5705374  0.1357559  0.3706775\n  15                   50     0.5428859  0.1723794  0.3474859\n  15                  100     0.5509054  0.1585915  0.3567359\n  15                  150     0.5584114  0.1480524  0.3611537\n  15                  200     0.5619715  0.1451323  0.3638634\n  15                  250     0.5649838  0.1416001  0.3661820\n  15                  300     0.5660400  0.1411948  0.3673904\n  15                  350     0.5669079  0.1403230  0.3683613\n  15                  400     0.5676357  0.1396901  0.3687451\n  15                  450     0.5682201  0.1390807  0.3693522\n  15                  500     0.5685675  0.1387953  0.3697387\n  15                  550     0.5689770  0.1386209  0.3700281\n  15                  600     0.5692874  0.1382838  0.3702165\n  15                  650     0.5693541  0.1384084  0.3701966\n  15                  700     0.5692911  0.1386319  0.3701522\n  15                  750     0.5693165  0.1385358  0.3701469\n  15                  800     0.5694504  0.1384136  0.3701988\n  15                  850     0.5695188  0.1383351  0.3702854\n  15                  900     0.5695686  0.1384205  0.3703544\n  15                  950     0.5695529  0.1385538  0.3704281\n  15                 1000     0.5696797  0.1383867  0.3704765\n  16                   50     0.5506425  0.1507968  0.3529644\n  16                  100     0.5616128  0.1362654  0.3634280\n  16                  150     0.5659887  0.1328752  0.3676430\n  16                  200     0.5685411  0.1305190  0.3687205\n  16                  250     0.5698773  0.1300317  0.3702114\n  16                  300     0.5713070  0.1285689  0.3717679\n  16                  350     0.5731035  0.1258093  0.3725103\n  16                  400     0.5733734  0.1263990  0.3729288\n  16                  450     0.5745182  0.1245032  0.3734173\n  16                  500     0.5747431  0.1244774  0.3738280\n  16                  550     0.5749237  0.1248925  0.3740282\n  16                  600     0.5749939  0.1248818  0.3741850\n  16                  650     0.5749882  0.1250946  0.3742661\n  16                  700     0.5751892  0.1250553  0.3744467\n  16                  750     0.5751432  0.1252821  0.3744317\n  16                  800     0.5752108  0.1253867  0.3745526\n  16                  850     0.5752817  0.1252798  0.3746187\n  16                  900     0.5753969  0.1252230  0.3746964\n  16                  950     0.5754475  0.1252255  0.3747401\n  16                 1000     0.5753680  0.1254596  0.3747167\n  17                   50     0.5536390  0.1431971  0.3543933\n  17                  100     0.5613865  0.1402918  0.3627648\n  17                  150     0.5687570  0.1316996  0.3689763\n  17                  200     0.5712300  0.1298115  0.3708426\n  17                  250     0.5740503  0.1266863  0.3735552\n  17                  300     0.5746718  0.1267227  0.3741863\n  17                  350     0.5765736  0.1245523  0.3756650\n  17                  400     0.5774495  0.1240042  0.3763274\n  17                  450     0.5783740  0.1231661  0.3768496\n  17                  500     0.5790800  0.1223745  0.3772687\n  17                  550     0.5794114  0.1220988  0.3773275\n  17                  600     0.5798206  0.1214808  0.3777110\n  17                  650     0.5800443  0.1214582  0.3778407\n  17                  700     0.5800997  0.1216585  0.3779169\n  17                  750     0.5802774  0.1214462  0.3781291\n  17                  800     0.5802877  0.1216969  0.3782201\n  17                  850     0.5803490  0.1217670  0.3782772\n  17                  900     0.5803080  0.1219123  0.3783203\n  17                  950     0.5803740  0.1219102  0.3783546\n  17                 1000     0.5806250  0.1215754  0.3785561\n  18                   50     0.5434862  0.1672202  0.3482437\n  18                  100     0.5515199  0.1590088  0.3586968\n  18                  150     0.5590352  0.1476074  0.3639811\n  18                  200     0.5605794  0.1472273  0.3664598\n  18                  250     0.5636318  0.1432375  0.3682549\n  18                  300     0.5650406  0.1412924  0.3693214\n  18                  350     0.5656539  0.1412437  0.3698649\n  18                  400     0.5665835  0.1403341  0.3706158\n  18                  450     0.5664022  0.1411758  0.3706151\n  18                  500     0.5674829  0.1395665  0.3713515\n  18                  550     0.5675251  0.1397180  0.3714956\n  18                  600     0.5679198  0.1392456  0.3717211\n  18                  650     0.5679891  0.1392878  0.3718608\n  18                  700     0.5681697  0.1391801  0.3719882\n  18                  750     0.5681135  0.1394783  0.3720042\n  18                  800     0.5680679  0.1396554  0.3719883\n  18                  850     0.5681990  0.1394064  0.3721081\n  18                  900     0.5682152  0.1396242  0.3721085\n  18                  950     0.5682717  0.1395839  0.3721704\n  18                 1000     0.5683152  0.1395887  0.3721771\n  19                   50     0.5449640  0.1648043  0.3514489\n  19                  100     0.5546267  0.1543763  0.3625809\n  19                  150     0.5606773  0.1472528  0.3682896\n  19                  200     0.5631468  0.1444462  0.3696570\n  19                  250     0.5671518  0.1396552  0.3719902\n  19                  300     0.5682363  0.1392526  0.3724019\n  19                  350     0.5687737  0.1392618  0.3726532\n  19                  400     0.5700830  0.1378876  0.3734352\n  19                  450     0.5701670  0.1378543  0.3736901\n  19                  500     0.5705627  0.1375201  0.3738553\n  19                  550     0.5709605  0.1371756  0.3740677\n  19                  600     0.5713797  0.1368640  0.3743496\n  19                  650     0.5716557  0.1367334  0.3745341\n  19                  700     0.5716855  0.1367524  0.3745393\n  19                  750     0.5718351  0.1367030  0.3746433\n  19                  800     0.5721004  0.1363798  0.3748130\n  19                  850     0.5721799  0.1362699  0.3748782\n  19                  900     0.5721948  0.1362745  0.3748838\n  19                  950     0.5721885  0.1364253  0.3749420\n  19                 1000     0.5721987  0.1364195  0.3749638\n  20                   50     0.5482480  0.1572856  0.3488554\n  20                  100     0.5556374  0.1497144  0.3599358\n  20                  150     0.5576255  0.1511267  0.3652327\n  20                  200     0.5613336  0.1474933  0.3676494\n  20                  250     0.5639905  0.1441678  0.3695924\n  20                  300     0.5656516  0.1420081  0.3709471\n  20                  350     0.5663072  0.1419029  0.3712257\n  20                  400     0.5670309  0.1418311  0.3718109\n  20                  450     0.5675614  0.1413524  0.3721823\n  20                  500     0.5679185  0.1411196  0.3722005\n  20                  550     0.5689408  0.1394624  0.3726249\n  20                  600     0.5688152  0.1402204  0.3727181\n  20                  650     0.5691985  0.1396950  0.3729195\n  20                  700     0.5695188  0.1394175  0.3731282\n  20                  750     0.5695374  0.1396411  0.3730788\n  20                  800     0.5693568  0.1400280  0.3730525\n  20                  850     0.5694771  0.1399299  0.3730905\n  20                  900     0.5696971  0.1396251  0.3732414\n  20                  950     0.5696151  0.1398476  0.3731960\n  20                 1000     0.5695910  0.1399408  0.3731869\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 7, shrinkage = 0.1 and n.minobsinnode = 10.\n\nmodels_LR2 &lt;- list(ranger = ranger_red, \n                  xgb = xgb_red,\n                  gbm = gbm_red)\n\n\n\n\n## Importance frame (wide format) ===\nimportance_wide &lt;- cbind(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_ranger = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_ranger\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Imp_gbm\" = \"rel.inf\"),\n  varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(var_gbm = rownames(.)) %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Imp_gbm\" = \"Overall\")\n)  %&gt;% kableExtra::kable()\n\n\n## Importance frame (long format for plotting) ===\nimportances_long_LR2 &lt;- full_join(\n  varImp(ranger_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"ranger\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\"),\n  summary.gbm(gbm_red$finalModel, plotit=FALSE) %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"gbm\") %&gt;% \n    arrange(desc(rel.inf)) %&gt;% \n    rename(\"Importance\" = \"rel.inf\") %&gt;% \n    select(variable, model, Importance)) %&gt;% \n  full_join(varImp(xgb_red)$importance %&gt;% \n    round(2) %&gt;% \n    mutate(variable = rownames(.),\n           model = \"xgb\") %&gt;% \n    arrange(desc(Overall)) %&gt;% \n    rename(\"Importance\" = \"Overall\")) %&gt;%\n  mutate(hypothesis = case_when(variable %in% H1_vars ~ \"H1\",\n                                variable %in% H2_vars ~ \"H2\",\n                                variable %in% H3_vars ~ \"H3\",\n                                variable %in% H4_vars ~ \"H4\",\n                                .default = \"H1\"))\n\nJoining with `by = join_by(Importance, variable, model)`\nJoining with `by = join_by(Importance, variable, model)`\n\n\n\n\n\n\n\n\n3.0.2 Variable Importances across models\n\nJaccard 1Jaccard 2Log Ratio 1Log Ratio 2\n\n\n\n# Plot variable importances across models:\nggplot(data = importances_long_J1, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n\n\n\n\n\n\n\n\n\n\n\n# Plot variable importances across models:\nggplot(data = importances_long_J2, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n\n\n\n\n\n\n\n\n\n\n\n# Plot variable importances across models:\nggplot(data = importances_long_LR1, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n\n\n\n\n\n\n\n\n\n\n\n# Plot variable importances across models:\nggplot(data = importances_long_LR2, \n       aes(y = variable, x = Importance, fill = hypothesis))+\n  geom_col()+\n  facet_grid(~model, scales = \"free\")+\n  theme_classic()+\n  scale_fill_manual(values = c(\"seagreen3\", \"mediumpurple\", \"darkorange\", \"gold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Resamples across models:\n\nJaccard 1Jaccard 2Log Ratio 1Log Ratio 2\n\n\n\n## Jaccard 1\n# Create resamples for each model\nresamps_J1 &lt;- resamples(list(\n  ranger = models_J1[[1]],\n  xgb = models_J1[[2]],\n  gbm = models_J1[[3]]\n))\n\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n\n\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n\nWarning in resamples.default(list(ranger = models_J1[[1]], xgb =\nmodels_J1[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n# Plot the resampled error rates for each model\ndotplot_resamps_J1 &lt;- dotplot(resamps_J1)\n\n# Summarize the resamples\nsummary_resamps_J1 &lt;- summary(resamps_J1)\n\n\n\n\n# Create resamples for each model\nresamps_J2 &lt;- resamples(list(\n  ranger = models_J2[[1]],\n  xgb = models_J2[[2]],\n  gbm = models_J2[[3]]\n))\n\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n\n\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n\nWarning in resamples.default(list(ranger = models_J2[[1]], xgb =\nmodels_J2[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n# Plot the resampled error rates for each model\ndotplot_resamps_J2 &lt;- dotplot(resamps_J2)\n\n# Summarize the resamples\nsummary_resamps_J2 &lt;- summary(resamps_J2)\n\n\n\n\n# Create resamples for each model\nresamps_LR1 &lt;- resamples(list(\n  ranger = models_LR1[[1]],\n  xgb = models_LR1[[2]],\n  gbm = models_LR1[[3]]\n))\n\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n\n\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n\nWarning in resamples.default(list(ranger = models_LR1[[1]], xgb =\nmodels_LR1[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n# Plot the resampled error rates for each model\ndotplot_resamps_LR1 &lt;- dotplot(resamps_LR1)\n\n# Summarize the resamples\nsummary_resamps_LR1 &lt;- summary(resamps_LR1)\n\n\n\n\n# Create resamples for each model\nresamps_LR2 &lt;- resamples(list(\n  ranger = models_LR2[[1]],\n  xgb = models_LR2[[2]],\n  gbm = models_LR2[[3]]\n))\n\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'ranger' did not have 'returnResamp=\"final\"; the optimal\ntuning parameters are used\n\n\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'xgb' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n\nWarning in resamples.default(list(ranger = models_LR2[[1]], xgb =\nmodels_LR2[[2]], : 'gbm' did not have 'returnResamp=\"final\"; the optimal tuning\nparameters are used\n\n# Plot the resampled error rates for each model\ndotplot_resamps_LR2 &lt;- dotplot(resamps_LR2)\n\n# Summarize the resamples\nsummary_resamps_LR2 &lt;- summary(resamps_LR2)\n\n\n\n\n\n4.0.1 Summary across all analyses\n\n\nCode\nresamples_all &lt;- list(\n  Jaccard1 = list(\n    resamps_J1 = resamps_J1,\n    Dotplot = dotplot_resamps_J1, # store the dotplot object\n    Summary = summary_resamps_J1  # store the summary object\n  ),\n  Jaccard2 = list(\n    resamps_J2 = resamps_J2,\n    Dotplot = dotplot_resamps_J2, # store the dotplot object\n    Summary = summary_resamps_J2  # store the summary object\n  ),\n  LogRatio1 = list(\n    resamps_LR1 = resamps_LR1,\n    Dotplot = dotplot_resamps_LR1, # store the dotplot object\n    Summary = summary_resamps_LR1  # store the summary object\n  ),\n  LogRatio2 = list(\n    resamps_LR2 = resamps_LR2,\n    Dotplot = dotplot_resamps_LR2, # store the dotplot object\n    Summary = summary_resamps_LR2  # store the summary object\n))\n\n\nresamples_all\n\n\n$Jaccard1\n$Jaccard1$resamps_J1\n\nCall:\nresamples.default(x = list(ranger = models_J1[[1]], xgb = models_J1[[2]], gbm\n = models_J1[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_J1)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n             Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger 0.06829633 0.07169567 0.07270608 0.07364971 0.07571423 0.08141634    0\nxgb    0.06740328 0.06906778 0.07147917 0.07206656 0.07467866 0.07734897    0\ngbm    0.06421988 0.07039892 0.07168599 0.07268969 0.07604308 0.08220707    0\n\nRMSE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.09892923 0.1027651 0.1091351 0.1111970 0.1156768 0.1297203    0\nxgb    0.09876680 0.1020442 0.1048924 0.1096201 0.1182042 0.1315152    0\ngbm    0.09474868 0.1000715 0.1039498 0.1099151 0.1165069 0.1362249    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.7893557 0.8342709 0.8531349 0.8455695 0.8671522 0.8806199    0\nxgb    0.7890662 0.8286518 0.8648936 0.8498558 0.8663667 0.8816143    0\ngbm    0.7764380 0.8305198 0.8693117 0.8491608 0.8734892 0.8896350    0\n\n\n\n$Jaccard2\n$Jaccard2$resamps_J2\n\nCall:\nresamples.default(x = list(ranger = models_J2[[1]], xgb = models_J2[[2]], gbm\n = models_J2[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard2$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard2$Summary\n\nCall:\nsummary.resamples(object = resamps_J2)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n             Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger 0.07893936 0.08223173 0.08529537 0.08621790 0.09025959 0.09783803    0\nxgb    0.07988486 0.08273490 0.08305801 0.08511744 0.08506362 0.09860895    0\ngbm    0.08314703 0.08589038 0.08743914 0.08969457 0.09451449 0.09872513    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.1120621 0.1194030 0.1240289 0.1242679 0.1279603 0.1371445    0\nxgb    0.1128614 0.1203841 0.1248338 0.1249692 0.1282270 0.1401247    0\ngbm    0.1126345 0.1258033 0.1285686 0.1280873 0.1322026 0.1407843    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.7606501 0.7986940 0.8126404 0.8094102 0.8245148 0.8464101    0\nxgb    0.7513672 0.7968261 0.8078829 0.8072804 0.8241337 0.8430043    0\ngbm    0.7492397 0.7839718 0.7984033 0.7971016 0.8026715 0.8436424    0\n\n\n\n$LogRatio1\n$LogRatio1$resamps_LR1\n\nCall:\nresamples.default(x = list(ranger = models_LR1[[1]], xgb = models_LR1[[2]],\n gbm = models_LR1[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$LogRatio1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$LogRatio1$Summary\n\nCall:\nsummary.resamples(object = resamps_LR1)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.2411987 0.2938274 0.3141578 0.3116032 0.3339331 0.3556762    0\nxgb    0.2634826 0.3192141 0.3432916 0.3368182 0.3574410 0.3886517    0\ngbm    0.2585481 0.3085024 0.3305215 0.3289230 0.3575965 0.3743060    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.3796529 0.4865685 0.5404298 0.5203579 0.5678237 0.5964049    0\nxgb    0.4086214 0.5065945 0.5581161 0.5424288 0.5898028 0.6202438    0\ngbm    0.4009512 0.5214004 0.5607243 0.5410392 0.5908667 0.6138864    0\n\nRsquared \n             Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.11731364 0.13113028 0.1769187 0.1905113 0.2220548 0.3739204    0\nxgb    0.06317943 0.13825059 0.1600146 0.1538734 0.1786388 0.2297801    0\ngbm    0.04681140 0.08501375 0.1411423 0.1358820 0.1728236 0.2524480    0\n\n\n\n$LogRatio2\n$LogRatio2$resamps_LR2\n\nCall:\nresamples.default(x = list(ranger = models_LR2[[1]], xgb = models_LR2[[2]],\n gbm = models_LR2[[3]]))\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$LogRatio2$Dotplot\n\n\n\n\n\n\n\n\n\n\n$LogRatio2$Summary\n\nCall:\nsummary.resamples(object = resamps_LR2)\n\nModels: ranger, xgb, gbm \nNumber of resamples: 10 \n\nMAE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.2999839 0.3105708 0.3341102 0.3301628 0.3439960 0.3657595    0\nxgb    0.3143081 0.3184640 0.3387676 0.3436322 0.3686984 0.3853569    0\ngbm    0.3192607 0.3259466 0.3369070 0.3441194 0.3538542 0.3928447    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.4715465 0.5081076 0.5262481 0.5335750 0.5591634 0.6160089    0\nxgb    0.4886618 0.5131532 0.5393618 0.5452046 0.5733619 0.6105273    0\ngbm    0.4788404 0.5051905 0.5341836 0.5378094 0.5526089 0.6341441    0\n\nRsquared \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger 0.07138851 0.1580110 0.2000510 0.1858403 0.2274409 0.2706381    0\nxgb    0.06998937 0.1186448 0.1439607 0.1597737 0.1855734 0.2791619    0\ngbm    0.05721234 0.1155320 0.1598668 0.1752265 0.2456909 0.3076351    0\n\n\nCode\n# save.image(\"data/RData/03_reduced_hyper_para_tuning_all_models.RData\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Script 3 - Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "04_FinalModels.html",
    "href": "04_FinalModels.html",
    "title": "4  Script 4 - FinalModels",
    "section": "",
    "text": "Source custom functionsMachineLearning packagesLoad RData to reduce computing time\n\n\n\nrm(list = ls())\nsource(\"src/functions.R\")\n\n\n\n\npckgs &lt;- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"ggcorrplot\", \n           \"caret\",  \"recipes\",   \"caretEnsemble\", \n           \"randomForest\", \"ranger\", \"gbm\", \"xgboost\", \n           \"vegan\", \"pdp\", \n           \"gridExtra\", \"kableExtra\")\n\ninstall_and_load(pckgs)\n\n\n\n\n# Load final workspace to save computing time:\nload(\"data/RData/04_FinalModels.RData\")\n\n\n\n\n\n4.0.1 Train all models together\n\n\n5 Jaccard 1\n::: panel-tabset ## Train model\n\ntictoc::tic(\"J1\")\n# Define model variables  (response, indices and predictors)\nresponse &lt;- \"Jaccard\" # Replace with your actual response variable name\npredictors &lt;- reduced_predictors[[1]] # Replace with your actual predictors\nindex &lt;- indices_J1\ndd &lt;- dat_train_J1 %&gt;% select(all_of(c(response, predictors)))\ndd_test &lt;- dat_test_J1 %&gt;% select(all_of(c(response, predictors)))\n\n# Define training control\ntrained_control &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"final\",\n  verboseIter = FALSE,\n  index = index # Ensure indices_J1 is defined\n)\n\n\nTrain modelSummarize individual modelsPredictive performancesEnsemble modelSummarize predictor importances\n\n\n\ntictoc::tic(\"J1\")\n\n# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd\nset.seed(42)\nmodelsList_J1 &lt;- caretList(\n  as.formula(paste(response, \"~ .\")),\n  data = dd,\n  trControl = trained_control,\n  tuneList = list(\n    ranger = caretModelSpec(\n      method = \"ranger\",\n      tuneGrid = expand.grid(\n        mtry = 12,\n        splitrule = \"variance\",\n        min.node.size = 5\n      ),\n      importance = \"permutation\",\n      num.trees = 5000\n    ),\n    gbm = caretModelSpec(\n      method = \"gbm\",\n      tuneGrid = expand.grid(\n        n.trees = 300,\n        interaction.depth = 10,\n        shrinkage = 0.1,\n        n.minobsinnode = 10\n      ),\n      verbose = FALSE\n    ),\n    xgbTree = caretModelSpec(\n      method = \"xgbTree\",\n      tuneGrid = expand.grid(\n        nrounds = 1000,\n        eta = 0.1,\n        max_depth = 5,\n        gamma = 0,\n        colsample_bytree = 0.6,\n        min_child_weight = 1,\n        subsample = 1\n      )\n    )\n  )\n)\n\n\n\n\n## Summarzize across individual models ========================================#\n\n# Create resamples from the list of models\nresamps_J1 &lt;- resamples(modelsList_J1)\n\n# Plot the resampled error rates for each model\ndotplot_resamps_J1 &lt;- dotplot(resamps_J1)\n\n# Summarize the resamples\nsummary_resamps_J1 &lt;- summary(resamps_J1)\n\n# Combine everything into a list\nresamples_all_J1 &lt;- list(\n  Jaccard1 = list(\n    resamps_J1 = resamps_J1,\n    Dotplot = dotplot_resamps_J1, # store the dotplot object\n    Summary = summary_resamps_J1  # store the summary object\n  )\n)\n\nresamples_all_J1\n\n$Jaccard1\n$Jaccard1$resamps_J1\n\nCall:\nresamples.default(x = modelsList_J1)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_J1)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \n\nMAE \n              Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger  0.06848070 0.07161636 0.07275848 0.07369927 0.07584192 0.08114329    0\ngbm     0.06193790 0.07052741 0.07172446 0.07226187 0.07497275 0.08192653    0\nxgbTree 0.06261764 0.07024619 0.07146359 0.07133180 0.07190695 0.07887175    0\n\nRMSE \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.09883248 0.1030705 0.1094454 0.1113231 0.1157472 0.1297299    0\ngbm     0.09389088 0.1015449 0.1052099 0.1103197 0.1181893 0.1372058    0\nxgbTree 0.09516518 0.1021747 0.1034087 0.1084217 0.1145142 0.1314592    0\n\nRsquared \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.7892004 0.8340418 0.8523348 0.8452701 0.8662831 0.8809218    0\ngbm     0.7733919 0.8255792 0.8668470 0.8478966 0.8737032 0.8916121    0\nxgbTree 0.7890760 0.8366012 0.8634119 0.8530995 0.8725559 0.8886361    0\n\n\n\n\n\n## Predictive Performance analysis ================================================\np_J1 &lt;- as.data.frame(\n  predict(modelsList_J1, newdata = dd_test)) %&gt;% \n  cbind(dd_test$Jaccard) %&gt;%\n  mutate(\n    error_ranger = dd_test$Jaccard-ranger,\n    error_gbm = dd_test$Jaccard-gbm,\n    error_xgb = dd_test$Jaccard-xgbTree) \n\np_J1 %&gt;%\n  summarise(mean_ranger = mean(error_ranger),\n            mean_gbm = mean(error_gbm),\n            mean_xgb = mean(error_xgb)) %&gt;% \n  kableExtra::kable() # ranger performs best\n\n\n\n\nmean_ranger\nmean_gbm\nmean_xgb\n\n\n\n\n0.0101863\n0.016859\n0.011548\n\n\n\n\n\n\n\nThe ensemble model is not better than the ranger model alone. In fact, it’s a bit worse. We will discard the ensembleModel approach therefore.\n\n# Create the ensemble model ================================================\n\n## Are they correlated? \n# yes. not the best foundation for ensembleModels...\nmodelCor(resamples(modelsList_J1))\n\n           ranger       gbm   xgbTree\nranger  1.0000000 0.6675497 0.7869473\ngbm     0.6675497 1.0000000 0.8179074\nxgbTree 0.7869473 0.8179074 1.0000000\n\nensembleModel_J1 &lt;- caretEnsemble(\n    modelsList_J1,\n    metric = \"Rsquared\",\n    trControl = trained_control)\nsummary(ensembleModel_J1)\n\nThe following models were ensembled: ranger, gbm, xgbTree \nThey were weighted: \n-0.0016 0.2234 0.3675 0.4111\nThe resulting Rsquared is: 0.8392\nThe fit for each individual model on the Rsquared is: \n  method  Rsquared RsquaredSD\n  ranger 0.8452701 0.03151442\n     gbm 0.8478966 0.03864407\n xgbTree 0.8530995 0.02992978\n\n\n\n\n\n# Summarize predictor importances ============================================\n\nvarImp(ensembleModel_J1) %&gt;% arrange(desc(overall))\n\n                                        overall       ranger          gbm\nrel_occ_Ncells                     2.248180e+01 2.110555e+01 21.706537223\nAOO                                2.002525e+01 1.613348e+01 19.434330050\nmoran                              1.105809e+01 1.026658e+01 10.077573258\nD_AOO_a                            1.102138e+01 1.625529e+01  9.226284350\nrel_relCirc                        4.398900e+00 5.956024e+00  4.990841294\nx_intercept                        4.263432e+00 5.348377e+00  4.266148165\nmean_prob_cooccur                  4.042548e+00 8.152068e+00  2.595945382\ncirc                               3.346581e+00 2.752171e+00  4.746038605\nGlobRangeSize_m2                   3.321725e+00 1.641687e+00  3.568772501\nBetaSR_sp                          2.885173e+00 1.610684e+00  3.847881842\nwidthMinRect                       2.454478e+00 1.671877e+00  2.836754971\nWesternness                        2.029216e+00 9.805118e-01  2.680586625\nsp_centr_lon                       1.686130e+00 1.628758e+00  1.357591296\nSouthernness                       1.629908e+00 9.498427e-01  2.139620060\nDist_centroid_to_COG               1.563904e+00 2.043304e+00  1.470759314\nsp_centr_lat                       1.442999e+00 1.915912e+00  1.917358681\nAlphaSR_sp                         1.441137e+00 1.132847e+00  2.143360464\nHabitatMarine                      2.275902e-01 8.439158e-02  0.251957702\nHabitatWetland                     1.653433e-01 6.179756e-02  0.180502399\nTrophic.NicheVertivore             9.790925e-02 2.053926e-02  0.121176238\nHabitatRock                        9.672362e-02 1.491517e-02  0.084563754\nTrophic.NicheOmnivore              6.556258e-02 6.207227e-03  0.108953494\nHabitatGrassland                   4.882083e-02 2.025468e-02  0.054308271\nHabitatHuman Modified              4.278182e-02 6.039479e-03  0.043946208\nTrophic.NicheGranivore             3.786718e-02 1.444022e-02  0.071499196\nHabitatForest                      3.416879e-02 5.457398e-02  0.032852854\nTrophic.NicheHerbivore terrestrial 2.428075e-02 7.273212e-02  0.000000000\nTrophic.NicheInvertivore           2.045985e-02 5.811976e-02  0.012585871\nTrophic.NicheHerbivore aquatic     1.553556e-02 8.073846e-03  0.015032065\nHabitatWoodland                    1.392963e-02 5.390698e-03  0.010580351\nHabitatShrubland                   7.501613e-03 9.869423e-03  0.005657515\nTrophic.NicheFrugivore             4.513279e-03 1.527695e-02  0.000000000\nHabitatRiverine                    3.820960e-03 0.000000e+00  0.000000000\nTrophic.NicheScavenger             2.560277e-04 1.148196e-03  0.000000000\nTrophic.NicheNectarivore           1.419716e-04 6.319197e-04  0.000000000\nHabitatDesert                      1.409070e-04 6.319197e-04  0.000000000\n                                        xgbTree\nrel_occ_Ncells                     2.392296e+01\nAOO                                2.266889e+01\nmoran                              1.236493e+01\nD_AOO_a                            9.781439e+00\nrel_relCirc                        3.023326e+00\nx_intercept                        3.671295e+00\nmean_prob_cooccur                  3.102192e+00\ncirc                               2.418494e+00\nGlobRangeSize_m2                   4.014017e+00\nBetaSR_sp                          2.717204e+00\nwidthMinRect                       2.538080e+00\nWesternness                        2.016874e+00\nsp_centr_lon                       2.011041e+00\nSouthernness                       1.543846e+00\nDist_centroid_to_COG               1.386606e+00\nsp_centr_lat                       7.618571e-01\nAlphaSR_sp                         9.808885e-01\nHabitatMarine                      2.836383e-01\nHabitatWetland                     2.080715e-01\nTrophic.NicheVertivore             1.191611e-01\nHabitatRock                        1.520610e-01\nTrophic.NicheOmnivore              5.903117e-02\nHabitatGrassland                   5.944161e-02\nHabitatHuman Modified              6.171164e-02\nTrophic.NicheGranivore             2.053222e-02\nHabitatForest                      2.425431e-02\nTrophic.NicheHerbivore terrestrial 1.965360e-02\nTrophic.NicheInvertivore           7.029942e-03\nTrophic.NicheHerbivore aquatic     2.004142e-02\nHabitatWoodland                    2.156525e-02\nHabitatShrubland                   7.863319e-03\nTrophic.NicheFrugivore             2.697880e-03\nHabitatRiverine                    9.313886e-03\nTrophic.NicheScavenger             0.000000e+00\nTrophic.NicheNectarivore           2.594904e-06\nHabitatDesert                      0.000000e+00\n\n# ranger\nimp_ranger &lt;- varImp(modelsList_J1[[1]])$importance %&gt;% \n  as.data.frame() %&gt;% \n  rename(\"imp_ranger\" = \"Overall\")\nimp_ranger$var &lt;- row.names(imp_ranger)\n\n# gbm\nimp_gbm &lt;- varImp(modelsList_J1[[2]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_gbm\" = \"Overall\")\nimp_gbm$var &lt;- row.names(imp_gbm)\nimp_merged &lt;- merge(imp_ranger, imp_gbm)\n\n# xgb\nimp_xgb &lt;- varImp(modelsList_J1[[3]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_xgb\" = \"Overall\")\nimp_xgb$var &lt;- row.names(imp_xgb)\n\nimp_merged_all_J1 &lt;- merge(imp_merged, imp_xgb) %&gt;% \n  arrange(desc(imp_ranger))\n\n# Print results\nimp_merged_all_J1 %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\nvar\nimp_ranger\nimp_gbm\nimp_xgb\n\n\n\n\nrel_occ_Ncells\n100.0000000\n100.0000000\n100.0000000\n\n\nD_AOO_a\n77.0190099\n42.5046347\n40.8872493\n\n\nAOO\n76.4418850\n89.5321527\n94.7578766\n\n\nmoran\n48.6440018\n46.4264436\n51.6864475\n\n\nmean_prob_cooccur\n38.6252330\n11.9592792\n12.9674291\n\n\nrel_relCirc\n28.2201809\n22.9923421\n12.6377599\n\n\nx_intercept\n25.3410933\n19.6537482\n15.3463255\n\n\ncirc\n13.0400334\n21.8645588\n10.1095107\n\n\nDist_centroid_to_COG\n9.6813605\n6.7756515\n5.7961319\n\n\nsp_centr_lat\n9.0777625\n8.8330933\n3.1846275\n\n\nwidthMinRect\n7.9215046\n13.0686666\n10.6093910\n\n\nGlobRangeSize_m2\n7.7784629\n16.4410033\n16.7789353\n\n\nsp_centr_lon\n7.7172001\n6.2542970\n8.4063232\n\n\nBetaSR_sp\n7.6315666\n17.7268341\n11.3581454\n\n\nAlphaSR_sp\n5.3675327\n9.8742625\n4.1001975\n\n\nWesternness\n4.6457531\n12.3492135\n8.4307034\n\n\nSouthernness\n4.5004405\n9.8570308\n6.4534085\n\n\nHabitatMarine\n0.3998549\n1.1607457\n1.1856325\n\n\nTrophic.NicheHerbivore terrestrial\n0.3446114\n0.0000000\n0.0821537\n\n\nHabitatWetland\n0.2928024\n0.8315578\n0.8697565\n\n\nTrophic.NicheInvertivore\n0.2753766\n0.0579819\n0.0293858\n\n\nHabitatForest\n0.2585764\n0.1513500\n0.1013851\n\n\nTrophic.NicheVertivore\n0.0973168\n0.5582477\n0.4981035\n\n\nHabitatGrassland\n0.0959685\n0.2501932\n0.2484710\n\n\nTrophic.NicheFrugivore\n0.0723836\n0.0000000\n0.0112774\n\n\nHabitatRock\n0.0706694\n0.3895774\n0.6356278\n\n\nTrophic.NicheGranivore\n0.0684191\n0.3293902\n0.0858264\n\n\nHabitatShrubland\n0.0467622\n0.0260636\n0.0328693\n\n\nTrophic.NicheHerbivore aquatic\n0.0382546\n0.0692513\n0.0837748\n\n\nTrophic.NicheOmnivore\n0.0294104\n0.5019386\n0.2467553\n\n\nHabitatHuman Modified\n0.0286156\n0.2024561\n0.2579599\n\n\nHabitatWoodland\n0.0255416\n0.0487427\n0.0901446\n\n\nTrophic.NicheScavenger\n0.0054403\n0.0000000\n0.0000000\n\n\nHabitatDesert\n0.0029941\n0.0000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n0.0029941\n0.0000000\n0.0000108\n\n\nHabitatRiverine\n0.0000000\n0.0000000\n0.0389328\n\n\n\n\nvarImp(ensembleModel_J1) %&gt;% \n  arrange(desc(overall)) %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\noverall\nranger\ngbm\nxgbTree\n\n\n\n\nrel_occ_Ncells\n22.4817990\n21.1055504\n21.7065372\n23.9229568\n\n\nAOO\n20.0252534\n16.1334806\n19.4343301\n22.6688859\n\n\nmoran\n11.0580908\n10.2665843\n10.0775733\n12.3649265\n\n\nD_AOO_a\n11.0213771\n16.2552860\n9.2262844\n9.7814390\n\n\nrel_relCirc\n4.3989002\n5.9560245\n4.9908413\n3.0233258\n\n\nx_intercept\n4.2634317\n5.3483772\n4.2661482\n3.6712948\n\n\nmean_prob_cooccur\n4.0425479\n8.1520680\n2.5959454\n3.1021925\n\n\ncirc\n3.3465813\n2.7521708\n4.7460386\n2.4184939\n\n\nGlobRangeSize_m2\n3.3217251\n1.6416874\n3.5687725\n4.0140175\n\n\nBetaSR_sp\n2.8851730\n1.6106841\n3.8478818\n2.7172042\n\n\nwidthMinRect\n2.4544782\n1.6718771\n2.8367550\n2.5380800\n\n\nWesternness\n2.0292158\n0.9805118\n2.6805866\n2.0168735\n\n\nsp_centr_lon\n1.6861300\n1.6287576\n1.3575913\n2.0110411\n\n\nSouthernness\n1.6299083\n0.9498427\n2.1396201\n1.5438461\n\n\nDist_centroid_to_COG\n1.5639036\n2.0433044\n1.4707593\n1.3866061\n\n\nsp_centr_lat\n1.4429992\n1.9159117\n1.9173587\n0.7618571\n\n\nAlphaSR_sp\n1.4411374\n1.1328473\n2.1433605\n0.9808885\n\n\nHabitatMarine\n0.2275902\n0.0843916\n0.2519577\n0.2836383\n\n\nHabitatWetland\n0.1653433\n0.0617976\n0.1805024\n0.2080715\n\n\nTrophic.NicheVertivore\n0.0979093\n0.0205393\n0.1211762\n0.1191611\n\n\nHabitatRock\n0.0967236\n0.0149152\n0.0845638\n0.1520610\n\n\nTrophic.NicheOmnivore\n0.0655626\n0.0062072\n0.1089535\n0.0590312\n\n\nHabitatGrassland\n0.0488208\n0.0202547\n0.0543083\n0.0594416\n\n\nHabitatHuman Modified\n0.0427818\n0.0060395\n0.0439462\n0.0617116\n\n\nTrophic.NicheGranivore\n0.0378672\n0.0144402\n0.0714992\n0.0205322\n\n\nHabitatForest\n0.0341688\n0.0545740\n0.0328529\n0.0242543\n\n\nTrophic.NicheHerbivore terrestrial\n0.0242807\n0.0727321\n0.0000000\n0.0196536\n\n\nTrophic.NicheInvertivore\n0.0204598\n0.0581198\n0.0125859\n0.0070299\n\n\nTrophic.NicheHerbivore aquatic\n0.0155356\n0.0080738\n0.0150321\n0.0200414\n\n\nHabitatWoodland\n0.0139296\n0.0053907\n0.0105804\n0.0215653\n\n\nHabitatShrubland\n0.0075016\n0.0098694\n0.0056575\n0.0078633\n\n\nTrophic.NicheFrugivore\n0.0045133\n0.0152769\n0.0000000\n0.0026979\n\n\nHabitatRiverine\n0.0038210\n0.0000000\n0.0000000\n0.0093139\n\n\nTrophic.NicheScavenger\n0.0002560\n0.0011482\n0.0000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n0.0001420\n0.0006319\n0.0000000\n0.0000026\n\n\nHabitatDesert\n0.0001409\n0.0006319\n0.0000000\n0.0000000\n\n\n\n\nrm(response, predictors, index, dd, dd_test, trained_control)\n\ntictoc::toc()\n\nJ1: 1.57 sec elapsed\n\n\n\n\n\n\n\n6 Jaccard 2\n::: panel-tabset ## Train model\n\ntictoc::tic(\"J2\")\n\n# Define model variables  (response, indices and predictors)\nresponse &lt;- \"Jaccard\" # Replace with your actual response variable name\npredictors &lt;- reduced_predictors[[2]] # Replace with your actual predictors\nindex &lt;- indices_J2\ndd &lt;- dat_train_J2 %&gt;% select(all_of(c(response, predictors)))\ndd_test &lt;- dat_test_J2 %&gt;% select(all_of(c(response, predictors)))\n\n# Define training control\ntrained_control &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"final\",\n  verboseIter = FALSE,\n  index = index # Ensure indices_J2 is defined\n)\n\n\nTrain modelSummarize individual modelsPredictive performancesEnsemble modelSummarize predictor importances\n\n\n\ntictoc::tic(\"J2\")\n\n# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd\nset.seed(42)\nmodelsList_J2 &lt;- caretList(\n  as.formula(paste(response, \"~ .\")),\n  data = dd,\n  trControl = trained_control,\n  tuneList = list(\n    ranger = caretModelSpec(\n      method = \"ranger\",\n      tuneGrid = expand.grid(\n        mtry = 5,\n        splitrule = \"variance\",\n        min.node.size = 5\n      ),\n      importance = \"permutation\",\n      num.trees = 5000\n    ),\n    gbm = caretModelSpec(\n      method = \"gbm\",\n      tuneGrid = expand.grid(\n        n.trees = 50,\n        interaction.depth = 9,\n        shrinkage = 0.1,\n        n.minobsinnode = 10\n      ),\n      verbose = FALSE\n    ),\n    xgbTree = caretModelSpec(\n      method = \"xgbTree\",\n      tuneGrid = expand.grid(\n        nrounds = 1000,\n        eta = 0.1,\n        max_depth = 5,\n        gamma = 0.01,\n        colsample_bytree = 0.6,\n        min_child_weight = 1,\n        subsample = 1\n      )\n    )\n  )\n)\n\n\n\n\n## Summarzize across individual models ========================================#\n\n# Create resamples from the list of models\nresamps_J2 &lt;- resamples(modelsList_J2)\n\n# Plot the resampled error rates for each model\ndotplot_resamps_J2 &lt;- dotplot(resamps_J2)\n\n# Summarize the resamples\nsummary_resamps_J2 &lt;- summary(resamps_J2)\n\n# Combine everything into a list\nresamples_all_J2 &lt;- list(\n  Jaccard1 = list(\n    resamps_J2 = resamps_J2,\n    Dotplot = dotplot_resamps_J2, # store the dotplot object\n    Summary = summary_resamps_J2  # store the summary object\n  )\n)\n\nresamples_all_J2\n\n$Jaccard1\n$Jaccard1$resamps_J2\n\nCall:\nresamples.default(x = modelsList_J2)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_J2)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \n\nMAE \n              Min.    1st Qu.     Median       Mean    3rd Qu.       Max. NA's\nranger  0.07893954 0.08245473 0.08522723 0.08620569 0.09027565 0.09796493    0\ngbm     0.08389465 0.08534676 0.08913508 0.08927248 0.09162794 0.09938889    0\nxgbTree 0.07828025 0.08270763 0.08399418 0.08554858 0.08843030 0.09721043    0\n\nRMSE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.1125653 0.1196879 0.1236389 0.1242638 0.1279999 0.1374080    0\ngbm     0.1160564 0.1262113 0.1295391 0.1298688 0.1337012 0.1443002    0\nxgbTree 0.1070532 0.1200788 0.1231785 0.1241742 0.1284834 0.1421728    0\n\nRsquared \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.7596017 0.7986473 0.8135708 0.8094090 0.8247881 0.8450611    0\ngbm     0.7452297 0.7801081 0.7952683 0.7913620 0.8032783 0.8335825    0\nxgbTree 0.7452082 0.8023182 0.8120472 0.8095943 0.8232890 0.8584104    0\n\n\n\n\n\n## Predictive Performance analysis ================================================\np_J2 &lt;- as.data.frame(\n  predict(modelsList_J2, newdata = dd_test)) %&gt;% \n  cbind(dd_test$Jaccard) %&gt;%\n  mutate(\n    error_ranger = dd_test$Jaccard-ranger,\n    error_gbm = dd_test$Jaccard-gbm,\n    error_xgb = dd_test$Jaccard-xgbTree) \n\np_J2 %&gt;%\n  summarise(mean_ranger = mean(error_ranger),\n            mean_gbm = mean(error_gbm),\n            mean_xgb = mean(error_xgb)) %&gt;% \n  kableExtra::kable() # ranger performs best\n\n\n\n\nmean_ranger\nmean_gbm\nmean_xgb\n\n\n\n\n0.0058978\n0.0050606\n0.0086411\n\n\n\n\n\n\n\nThe ensemble model is not better than the ranger model alone. In fact, it’s a bit worse. We will discard the ensembleModel approach therefore.\n\n# Create the ensemble model ================================================\n\n## Are they correlated? \n# yes. not the best foundation for ensembleModels...\nmodelCor(resamples(modelsList_J2))\n\n           ranger       gbm   xgbTree\nranger  1.0000000 0.7087220 0.8517862\ngbm     0.7087220 1.0000000 0.4165927\nxgbTree 0.8517862 0.4165927 1.0000000\n\nensembleModel_J2 &lt;- caretEnsemble(\n    modelsList_J2,\n    metric = \"Rsquared\",\n    trControl = trained_control)\nsummary(ensembleModel_J2)\n\nThe following models were ensembled: ranger, gbm, xgbTree \nThey were weighted: \n0.0023 0.5489 -0.0384 0.4971\nThe resulting Rsquared is: 0.7829\nThe fit for each individual model on the Rsquared is: \n  method  Rsquared RsquaredSD\n  ranger 0.8094090 0.02414369\n     gbm 0.7913620 0.02385256\n xgbTree 0.8095943 0.02903417\n\n\n\n\n\n# Summarize predictor importances ============================================\n\nvarImp(ensembleModel_J2) %&gt;% arrange(desc(overall))\n\n                        overall     ranger         gbm     xgbTree\nrel_occ_Ncells       36.2019483 30.6406770 30.71455254 42.76746759\nmoran                14.5230983 11.7475052 15.68934146 17.49812317\nAOO                  14.4724987 14.5848535 19.01366798 13.99747404\nD_AOO_a              11.1558704 15.6942177 12.99614943  6.00184528\nx_intercept           7.6943735  8.6705150  3.71501500  6.92392805\nrel_relCirc           3.3348299  5.6113834  1.42842448  0.96810732\nBetaSR_sp             3.0263428  1.9812211  3.29259704  4.15991907\ncirc                  2.2519701  2.4568247  4.36127472  1.86273389\nDist_centroid_to_COG  2.2420879  3.0571749  1.45380682  1.40288744\nsp_centr_lon          1.3496431  1.6783866  0.04228594  1.08763841\nWesternness           1.1948897  1.0428352  2.18772009  1.28607963\nsp_centr_lat          0.9334169  1.7845072  0.84979502  0.00000000\nGlobRangeSize_m2      0.8929399  0.0000000  2.33714816  1.76742422\nAlphaSR_sp            0.5836263  0.8246285  1.91822133  0.21434225\nSouthernness          0.1424642  0.2252700  0.00000000  0.06202963\n\n# ranger\nimp_ranger &lt;- varImp(modelsList_J2[[1]])$importance %&gt;% \n  as.data.frame() %&gt;% \n  rename(\"imp_ranger\" = \"Overall\")\nimp_ranger$var &lt;- row.names(imp_ranger)\n\n# gbm\nimp_gbm &lt;- varImp(modelsList_J2[[2]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_gbm\" = \"Overall\")\nimp_gbm$var &lt;- row.names(imp_gbm)\nimp_merged &lt;- merge(imp_ranger, imp_gbm)\n\n# xgb\nimp_xgb &lt;- varImp(modelsList_J2[[3]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_xgb\" = \"Overall\")\nimp_xgb$var &lt;- row.names(imp_xgb)\n\nimp_merged_all_J2 &lt;- merge(imp_merged, imp_xgb) %&gt;% \n  arrange(desc(imp_ranger))\n\n# Print results\nimp_merged_all_J2 %&gt;% \n  kableExtra::kable()\n\n\n\n\nvar\nimp_ranger\nimp_gbm\nimp_xgb\n\n\n\n\nrel_occ_Ncells\n100.0000000\n100.000000\n100.0000000\n\n\nD_AOO_a\n51.2202053\n42.312677\n14.0336700\n\n\nAOO\n47.5996451\n61.904428\n32.7292562\n\n\nmoran\n38.3395744\n51.081133\n40.9145646\n\n\nx_intercept\n28.2974002\n12.095293\n16.1897078\n\n\nrel_relCirc\n18.3135100\n4.650644\n2.2636536\n\n\nDist_centroid_to_COG\n9.9775044\n4.733283\n3.2802677\n\n\ncirc\n8.0181802\n14.199376\n4.3554926\n\n\nBetaSR_sp\n6.4659834\n10.719990\n9.7268305\n\n\nsp_centr_lat\n5.8239809\n2.766750\n0.0000000\n\n\nsp_centr_lon\n5.4776421\n0.137674\n2.5431443\n\n\nWesternness\n3.4034338\n7.122748\n3.0071447\n\n\nAlphaSR_sp\n2.6912868\n6.245317\n0.5011806\n\n\nSouthernness\n0.7351993\n0.000000\n0.1450393\n\n\nGlobRangeSize_m2\n0.0000000\n7.609254\n4.1326371\n\n\n\n\nvarImp(ensembleModel_J2) %&gt;% \n  arrange(desc(overall)) %&gt;% \n  kableExtra::kable()\n\n\n\n\n\noverall\nranger\ngbm\nxgbTree\n\n\n\n\nrel_occ_Ncells\n36.2019483\n30.6406770\n30.7145525\n42.7674676\n\n\nmoran\n14.5230983\n11.7475052\n15.6893415\n17.4981232\n\n\nAOO\n14.4724987\n14.5848535\n19.0136680\n13.9974740\n\n\nD_AOO_a\n11.1558704\n15.6942177\n12.9961494\n6.0018453\n\n\nx_intercept\n7.6943735\n8.6705150\n3.7150150\n6.9239281\n\n\nrel_relCirc\n3.3348299\n5.6113834\n1.4284245\n0.9681073\n\n\nBetaSR_sp\n3.0263428\n1.9812211\n3.2925970\n4.1599191\n\n\ncirc\n2.2519701\n2.4568247\n4.3612747\n1.8627339\n\n\nDist_centroid_to_COG\n2.2420879\n3.0571749\n1.4538068\n1.4028874\n\n\nsp_centr_lon\n1.3496431\n1.6783866\n0.0422859\n1.0876384\n\n\nWesternness\n1.1948897\n1.0428352\n2.1877201\n1.2860796\n\n\nsp_centr_lat\n0.9334169\n1.7845072\n0.8497950\n0.0000000\n\n\nGlobRangeSize_m2\n0.8929399\n0.0000000\n2.3371482\n1.7674242\n\n\nAlphaSR_sp\n0.5836263\n0.8246285\n1.9182213\n0.2143422\n\n\nSouthernness\n0.1424642\n0.2252700\n0.0000000\n0.0620296\n\n\n\n\nrm(response, predictors, index, dd, dd_test, trained_control)\n\ntictoc::toc()\n\nJ2: 1.48 sec elapsed\n\n\n\n\n\n\n\n7 Log Ratio 1\n::: panel-tabset ## Train model\n\ntictoc::tic(\"LR1\")\n# Define model variables  (response, indices and predictors)\nresponse &lt;- \"log_R2_1\" \npredictors &lt;- reduced_predictors[[3]] # Replace with your actual predictors\nindex &lt;- indices_LR1\ndd &lt;- dat_train_LR1 %&gt;% select(all_of(c(response, predictors)))\ndd_test &lt;- dat_test_LR1 %&gt;% select(all_of(c(response, predictors)))\n\n# Define training control\ntrained_control &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"final\",\n  verboseIter = FALSE,\n  index = index # Ensure indices_LR1 is defined\n)\n\n\nTrain modelSummarize individual modelsPredictive performancesEnsemble modelSummarize predictor importances\n\n\n\ntictoc::tic(\"LR1\")\n\n\n# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd\nset.seed(42)\nmodelsList_LR1 &lt;- caretList(\n  as.formula(paste(response, \"~ .\")),\n  data = dd,\n  trControl = trained_control,\n  tuneList = list(\n    ranger = caretModelSpec(\n      method = \"ranger\",\n      tuneGrid = expand.grid(\n        mtry = 28,\n        splitrule = \"extratrees\",\n        min.node.size = 5\n      ),\n      importance = \"permutation\",\n      num.trees = 5000\n    ),\n    gbm = caretModelSpec(\n      method = \"gbm\",\n      tuneGrid = expand.grid(\n        n.trees = 50,\n        interaction.depth = 3,\n        shrinkage = 0.1,\n        n.minobsinnode = 10\n      ),\n      verbose = FALSE\n    ),\n    xgbTree = caretModelSpec(\n      method = \"xgbTree\",\n      tuneGrid = expand.grid(\n        nrounds = 1000,\n        eta = 0.1,\n        max_depth = 5,\n        gamma = 0.1,\n        colsample_bytree = 0.6,\n        min_child_weight = 1,\n        subsample = 1\n      )\n    )\n  )\n)\n\n\n\n\n## Summarzize across individual models ========================================#\n\n# Create resamples from the list of models\nresamps_LR1 &lt;- resamples(modelsList_LR1)\n\n# Plot the resampled error rates for each model\ndotplot_resamps_LR1 &lt;- dotplot(resamps_LR1)\n\n# Summarize the resamples\nsummary_resamps_LR1 &lt;- summary(resamps_LR1)\n\n# Combine everything into a list\nresamples_all_LR1 &lt;- list(\n  Jaccard1 = list(\n    resamps_LR1 = resamps_LR1,\n    Dotplot = dotplot_resamps_LR1, # store the dotplot object\n    Summary = summary_resamps_LR1  # store the summary object\n  )\n)\n\nresamples_all_LR1\n\n$Jaccard1\n$Jaccard1$resamps_LR1\n\nCall:\nresamples.default(x = modelsList_LR1)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_LR1)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \n\nMAE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.2413500 0.2937891 0.3140971 0.3116381 0.3342397 0.3570233    0\ngbm     0.2710595 0.3091150 0.3297544 0.3314569 0.3629118 0.3787333    0\nxgbTree 0.2678089 0.3158956 0.3447055 0.3388593 0.3634772 0.3886261    0\n\nRMSE \n             Min.   1st Qu.    Median      Mean  3rd Qu.      Max. NA's\nranger  0.3800898 0.4863429 0.5411437 0.5204904 0.567322 0.5983651    0\ngbm     0.4270322 0.5232623 0.5484819 0.5442317 0.591902 0.6322296    0\nxgbTree 0.4088023 0.5069973 0.5762541 0.5487222 0.588816 0.6293142    0\n\nRsquared \n              Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.11579499 0.12877743 0.1775331 0.1901280 0.2192717 0.3722954    0\ngbm     0.06093550 0.09401116 0.1100171 0.1218121 0.1493415 0.1882298    0\nxgbTree 0.04937369 0.08174521 0.1328073 0.1371819 0.1948978 0.2253123    0\n\n\n\n\n\n## Predictive Performance analysis ================================================\np_LR1 &lt;- as.data.frame(\n  predict(modelsList_LR1, newdata = dd_test)) %&gt;% \n  cbind(dd_test$log_R2_1) %&gt;%\n  mutate(\n    error_ranger = dd_test$log_R2_1-ranger,\n    error_gbm = dd_test$log_R2_1-gbm,\n    error_xgb = dd_test$log_R2_1-xgbTree) \n\np_LR1 %&gt;%\n  summarise(mean_ranger = mean(error_ranger),\n            mean_gbm = mean(error_gbm),\n            mean_xgb = mean(error_xgb)) %&gt;% \n  kableExtra::kable() # ranger performs best\n\n\n\n\nmean_ranger\nmean_gbm\nmean_xgb\n\n\n\n\n0.0172756\n0.0109739\n0.0058452\n\n\n\n\n\n\n\nThe ensemble model is not better than the ranger model alone. In fact, it’s a bit worse. We will discard the ensembleModel approach therefore.\n\n# Create the ensemble model ================================================\n\n## Are they correlated? \n# yes. not the best foundation for ensembleModels...\nmodelCor(resamples(modelsList_LR1))\n\n           ranger       gbm   xgbTree\nranger  1.0000000 0.9515691 0.9546465\ngbm     0.9515691 1.0000000 0.8665197\nxgbTree 0.9546465 0.8665197 1.0000000\n\nensembleModel_LR1 &lt;- caretEnsemble(\n    modelsList_LR1,\n    metric = \"Rsquared\",\n    trControl = trained_control)\nsummary(ensembleModel_LR1)\n\nThe following models were ensembled: ranger, gbm, xgbTree \nThey were weighted: \n-0.0066 0.9729 -0.0139 0.0855\nThe resulting Rsquared is: 0.1385\nThe fit for each individual model on the Rsquared is: \n  method  Rsquared RsquaredSD\n  ranger 0.1901280 0.07840451\n     gbm 0.1218121 0.04182013\n xgbTree 0.1371819 0.06528472\n\n\n\n\n\n# Summarize predictor importances ============================================\n\nvarImp(ensembleModel_LR1) %&gt;% arrange(desc(overall))\n\n                                      overall    ranger        gbm      xgbTree\nD_AOO_a                            9.92796190 9.5125401 17.6939777 13.389344574\nAlphaSR_sp                         6.53972184 6.8849653  3.5110932  3.105305597\ndatasetBirds_atlas_EBBA            5.43632515 5.9892959  0.0000000  0.030802850\nsp_centr_lon                       5.01193330 5.0209245  2.8419212  5.262995161\ndatasetBirds_atlas_Japan           4.35874122 4.8042735  0.0000000  0.000000000\nrel_maxDist                        4.13658749 4.4315410  2.6906313  1.016571970\nrel_relCirc                        3.55247066 3.5897738  2.5687553  3.288281596\nSouthernness                       3.23045741 3.2113260  6.9748939  2.838385915\nx_intercept                        3.06405924 3.2832897  0.0000000  1.068967617\natlas_widthMinRect                 2.87253860 3.1661575  0.0000000  0.000000000\nWesternness                        2.86117464 2.7297333  5.5838645  3.913142173\nIUCNLC                             2.77972052 2.8741805  2.8029165  1.701343282\nmoran                              2.58251295 2.6188672  1.3193391  2.374623255\nrel_lin                            2.47062581 2.4453067  2.4518271  2.761723418\nrel_occ_Ncells                     2.42794896 2.1626926  4.7955967  5.060040431\nMass                               2.25245665 1.9218254  7.2411481  5.201479609\nbearing                            2.00633888 1.8148656  4.5973882  3.762679117\nsp_centr_lat                       2.00613316 1.7792523  1.8881329  4.606399427\nbearingMinRect                     1.72744509 1.7548552  0.0000000  1.696904740\nAOO                                1.66650692 1.3428689  8.1896334  4.286117740\nwidthMinRect                       1.66589869 1.6437852  3.3059001  1.650422700\ndatasetBirds_Atlas_New_York        1.66445165 1.8345849  0.0000000  0.000000000\nrel_elonRatio                      1.64358544 1.5955601  0.5034746  2.375579893\nDist_centroid_to_COG               1.62024031 1.5643364  0.6457174  2.414900853\nBetaSR_sp                          1.59513558 1.4789963  0.5894578  3.080120839\nmean_prob_cooccur                  1.44898326 1.1829119  5.9389917  3.744758428\nelonMinRect                        1.43179117 1.4384167  0.9720997  1.431269666\nminDist_toBorder_centr             1.39700803 1.2755075  3.0815177  2.504935835\natlas_bearingMinRect               1.35201521 1.4902126  0.0000000  0.000000000\nHabitat.Density3                   1.10167421 1.1632302  0.0000000  0.580786379\nHabitatGrassland                   1.02456017 1.0565355  1.4488756  0.591708231\nHand.Wing.Index                    1.00600016 0.7362298  1.8227244  3.941984932\nHabitatWetland                     0.97933179 1.0780781  0.0000000  0.015436564\ncirc                               0.94470115 0.9094419  1.2715356  1.292599300\nGlobRangeSize_m2                   0.67365596 0.5448102  0.0000000  2.249129405\nHabitatForest                      0.67298780 0.7394525  0.0000000  0.026453655\nsd_PC1                             0.60906604 0.5440202  2.4906645  1.042658246\nIUCNNT                             0.59282497 0.6197400  0.4066598  0.316946398\nsd_PC2                             0.56301281 0.3820564  0.9381582  2.560531108\nTrophic.NicheHerbivore aquatic     0.56253720 0.6196526  0.0000000  0.004377156\nMigration2                         0.54222314 0.5632685  0.0000000  0.391097592\nPrimary.LifestyleInsessorial       0.53676831 0.5847717  0.0000000  0.078072895\nTrophic.NicheInvertivore           0.49764479 0.5349728  0.0000000  0.154025120\nHabitatShrubland                   0.46430598 0.4025983  0.0000000  1.241911677\nTrophic.NicheOmnivore              0.41731930 0.3897340  0.0000000  0.799089825\nMigration3                         0.40254520 0.4416751  0.0000000  0.022941191\nHabitatHuman Modified              0.32181457 0.3512523  0.0000000  0.039325207\nHabitat.Density2                   0.32060259 0.3326944  0.0000000  0.235247461\nHabitatMarine                      0.29684975 0.3212837  0.0000000  0.067220270\nFP                                 0.28774979 0.2145901  1.0180569  1.001115260\nHabitatWoodland                    0.28528631 0.3109020  0.0000000  0.040329478\nPrimary.LifestyleAquatic           0.28125193 0.2910131  0.0000000  0.216003072\nPrimary.LifestyleGeneralist        0.26999177 0.2665376  0.4150469  0.285667548\nTrophic.NicheVertivore             0.22584250 0.2461183  0.0000000  0.031953982\nIUCNVU                             0.19730800 0.2174760  0.0000000  0.000000000\nPrimary.LifestyleTerrestrial       0.18446233 0.1907955  0.0000000  0.142451255\nHabitatRock                        0.15663099 0.1726412  0.0000000  0.000000000\nTrophic.NicheScavenger             0.12566534 0.1385103  0.0000000  0.000000000\nIUCNEN                             0.12472239 0.1374710  0.0000000  0.000000000\nTrophic.NicheFrugivore             0.12273794 0.1352837  0.0000000  0.000000000\nTrophic.NicheNectarivore           0.12261071 0.1351435  0.0000000  0.000000000\nHabitatDesert                      0.12261071 0.1351435  0.0000000  0.000000000\nTrophic.NicheGranivore             0.11858435 0.1283351  0.0000000  0.026966820\nHabitatRiverine                    0.11040863 0.1216941  0.0000000  0.000000000\nTrophic.NicheHerbivore terrestrial 0.00294067 0.0000000  0.0000000  0.036873286\n\n# ranger\nimp_ranger &lt;- varImp(modelsList_LR1[[1]])$importance %&gt;% \n  as.data.frame() %&gt;% \n  rename(\"imp_ranger\" = \"Overall\")\nimp_ranger$var &lt;- row.names(imp_ranger)\n\n# gbm\nimp_gbm &lt;- varImp(modelsList_LR1[[2]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_gbm\" = \"Overall\")\nimp_gbm$var &lt;- row.names(imp_gbm)\nimp_merged &lt;- merge(imp_ranger, imp_gbm)\n\n# xgb\nimp_xgb &lt;- varImp(modelsList_LR1[[3]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_xgb\" = \"Overall\")\nimp_xgb$var &lt;- row.names(imp_xgb)\n\nimp_merged_all_LR1 &lt;- merge(imp_merged, imp_xgb) %&gt;% \n  arrange(desc(imp_ranger))\n\n# Print results\nimp_merged_all_LR1 %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\nvar\nimp_ranger\nimp_gbm\nimp_xgb\n\n\n\n\nD_AOO_a\n100.000000\n100.000000\n100.0000000\n\n\nAlphaSR_sp\n72.377780\n19.843436\n23.1923645\n\n\ndatasetBirds_atlas_EBBA\n62.962110\n0.000000\n0.2300549\n\n\nsp_centr_lon\n52.782164\n16.061517\n39.3073397\n\n\ndatasetBirds_atlas_Japan\n50.504634\n0.000000\n0.0000000\n\n\nrel_maxDist\n46.586306\n15.206481\n7.5923953\n\n\nrel_relCirc\n37.737279\n14.517682\n24.5589437\n\n\nx_intercept\n34.515383\n0.000000\n7.9837188\n\n\nSouthernness\n33.758870\n39.419592\n21.1988414\n\n\natlas_widthMinRect\n33.284038\n0.000000\n0.0000000\n\n\nIUCNLC\n30.214648\n15.841076\n12.7066958\n\n\nWesternness\n28.696155\n31.557994\n29.2257933\n\n\nmoran\n27.530683\n7.456430\n17.7351717\n\n\nrel_lin\n25.706138\n13.856845\n20.6262779\n\n\nrel_occ_Ncells\n22.735175\n27.102988\n37.7915469\n\n\nMass\n20.203072\n40.924366\n38.8479031\n\n\ndatasetBirds_Atlas_New_York\n19.285963\n0.000000\n0.0000000\n\n\nbearing\n19.078665\n25.982785\n28.1020411\n\n\nsp_centr_lat\n18.704282\n10.671048\n34.4034721\n\n\nbearingMinRect\n18.447809\n0.000000\n12.6735460\n\n\nwidthMinRect\n17.280192\n18.683759\n12.3263890\n\n\nrel_elonRatio\n16.773229\n2.845458\n17.7423165\n\n\nDist_centroid_to_COG\n16.444991\n3.649362\n18.0359900\n\n\natlas_bearingMinRect\n15.665769\n0.000000\n0.0000000\n\n\nBetaSR_sp\n15.547859\n3.331404\n23.0042690\n\n\nelonMinRect\n15.121269\n5.493958\n10.6896171\n\n\nAOO\n14.116827\n46.284863\n32.0114082\n\n\nminDist_toBorder_centr\n13.408695\n17.415630\n18.7084276\n\n\nmean_prob_cooccur\n12.435289\n33.565045\n27.9681982\n\n\nHabitat.Density3\n12.228387\n0.000000\n4.3376759\n\n\nHabitatWetland\n11.333230\n0.000000\n0.1152899\n\n\nHabitatGrassland\n11.106766\n8.188524\n4.4192472\n\n\ncirc\n9.560453\n7.186262\n9.6539401\n\n\nHabitatForest\n7.773449\n0.000000\n0.1975724\n\n\nHand.Wing.Index\n7.739571\n10.301383\n29.4412091\n\n\nIUCNNT\n6.514979\n2.298295\n2.3671539\n\n\nTrophic.NicheHerbivore aquatic\n6.514061\n0.000000\n0.0326913\n\n\nPrimary.LifestyleInsessorial\n6.147377\n0.000000\n0.5830972\n\n\nMigration2\n5.921326\n0.000000\n2.9209614\n\n\nGlobRangeSize_m2\n5.727284\n0.000000\n16.7979052\n\n\nsd_PC1\n5.718980\n14.076340\n7.7872239\n\n\nTrophic.NicheInvertivore\n5.623869\n0.000000\n1.1503559\n\n\nMigration3\n4.643083\n0.000000\n0.1713392\n\n\nHabitatShrubland\n4.232290\n0.000000\n9.2753732\n\n\nTrophic.NicheOmnivore\n4.097055\n0.000000\n5.9681026\n\n\nsd_PC2\n4.016345\n5.302133\n19.1236479\n\n\nHabitatHuman Modified\n3.692519\n0.000000\n0.2937052\n\n\nHabitat.Density2\n3.497430\n0.000000\n1.7569752\n\n\nHabitatMarine\n3.377475\n0.000000\n0.5020430\n\n\nHabitatWoodland\n3.268339\n0.000000\n0.3012058\n\n\nPrimary.LifestyleAquatic\n3.059258\n0.000000\n1.6132460\n\n\nPrimary.LifestyleGeneralist\n2.801960\n2.345696\n2.1335439\n\n\nTrophic.NicheVertivore\n2.587304\n0.000000\n0.2386523\n\n\nIUCNVU\n2.286203\n0.000000\n0.0000000\n\n\nFP\n2.255865\n5.753691\n7.4769549\n\n\nPrimary.LifestyleTerrestrial\n2.005726\n0.000000\n1.0639151\n\n\nHabitatRock\n1.814880\n0.000000\n0.0000000\n\n\nTrophic.NicheScavenger\n1.456081\n0.000000\n0.0000000\n\n\nIUCNEN\n1.445156\n0.000000\n0.0000000\n\n\nTrophic.NicheFrugivore\n1.422162\n0.000000\n0.0000000\n\n\nHabitatDesert\n1.420688\n0.000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n1.420688\n0.000000\n0.0000000\n\n\nTrophic.NicheGranivore\n1.349115\n0.000000\n0.2014051\n\n\nHabitatRiverine\n1.279302\n0.000000\n0.0000000\n\n\nTrophic.NicheHerbivore terrestrial\n0.000000\n0.000000\n0.2753928\n\n\n\n\nvarImp(ensembleModel_LR1) %&gt;% \n  arrange(desc(overall)) %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\noverall\nranger\ngbm\nxgbTree\n\n\n\n\nD_AOO_a\n9.9279619\n9.5125401\n17.6939777\n13.3893446\n\n\nAlphaSR_sp\n6.5397218\n6.8849653\n3.5110932\n3.1053056\n\n\ndatasetBirds_atlas_EBBA\n5.4363251\n5.9892959\n0.0000000\n0.0308028\n\n\nsp_centr_lon\n5.0119333\n5.0209245\n2.8419212\n5.2629952\n\n\ndatasetBirds_atlas_Japan\n4.3587412\n4.8042735\n0.0000000\n0.0000000\n\n\nrel_maxDist\n4.1365875\n4.4315410\n2.6906313\n1.0165720\n\n\nrel_relCirc\n3.5524707\n3.5897738\n2.5687553\n3.2882816\n\n\nSouthernness\n3.2304574\n3.2113260\n6.9748939\n2.8383859\n\n\nx_intercept\n3.0640592\n3.2832897\n0.0000000\n1.0689676\n\n\natlas_widthMinRect\n2.8725386\n3.1661575\n0.0000000\n0.0000000\n\n\nWesternness\n2.8611746\n2.7297333\n5.5838645\n3.9131422\n\n\nIUCNLC\n2.7797205\n2.8741805\n2.8029165\n1.7013433\n\n\nmoran\n2.5825130\n2.6188672\n1.3193391\n2.3746233\n\n\nrel_lin\n2.4706258\n2.4453067\n2.4518271\n2.7617234\n\n\nrel_occ_Ncells\n2.4279490\n2.1626926\n4.7955967\n5.0600404\n\n\nMass\n2.2524567\n1.9218254\n7.2411481\n5.2014796\n\n\nbearing\n2.0063389\n1.8148656\n4.5973882\n3.7626791\n\n\nsp_centr_lat\n2.0061332\n1.7792523\n1.8881329\n4.6063994\n\n\nbearingMinRect\n1.7274451\n1.7548552\n0.0000000\n1.6969047\n\n\nAOO\n1.6665069\n1.3428689\n8.1896334\n4.2861177\n\n\nwidthMinRect\n1.6658987\n1.6437852\n3.3059001\n1.6504227\n\n\ndatasetBirds_Atlas_New_York\n1.6644516\n1.8345849\n0.0000000\n0.0000000\n\n\nrel_elonRatio\n1.6435854\n1.5955601\n0.5034746\n2.3755799\n\n\nDist_centroid_to_COG\n1.6202403\n1.5643364\n0.6457174\n2.4149009\n\n\nBetaSR_sp\n1.5951356\n1.4789963\n0.5894578\n3.0801208\n\n\nmean_prob_cooccur\n1.4489833\n1.1829119\n5.9389917\n3.7447584\n\n\nelonMinRect\n1.4317912\n1.4384167\n0.9720997\n1.4312697\n\n\nminDist_toBorder_centr\n1.3970080\n1.2755075\n3.0815177\n2.5049358\n\n\natlas_bearingMinRect\n1.3520152\n1.4902126\n0.0000000\n0.0000000\n\n\nHabitat.Density3\n1.1016742\n1.1632302\n0.0000000\n0.5807864\n\n\nHabitatGrassland\n1.0245602\n1.0565355\n1.4488756\n0.5917082\n\n\nHand.Wing.Index\n1.0060002\n0.7362298\n1.8227244\n3.9419849\n\n\nHabitatWetland\n0.9793318\n1.0780781\n0.0000000\n0.0154366\n\n\ncirc\n0.9447011\n0.9094419\n1.2715356\n1.2925993\n\n\nGlobRangeSize_m2\n0.6736560\n0.5448102\n0.0000000\n2.2491294\n\n\nHabitatForest\n0.6729878\n0.7394525\n0.0000000\n0.0264537\n\n\nsd_PC1\n0.6090660\n0.5440202\n2.4906645\n1.0426582\n\n\nIUCNNT\n0.5928250\n0.6197400\n0.4066598\n0.3169464\n\n\nsd_PC2\n0.5630128\n0.3820564\n0.9381582\n2.5605311\n\n\nTrophic.NicheHerbivore aquatic\n0.5625372\n0.6196526\n0.0000000\n0.0043772\n\n\nMigration2\n0.5422231\n0.5632685\n0.0000000\n0.3910976\n\n\nPrimary.LifestyleInsessorial\n0.5367683\n0.5847717\n0.0000000\n0.0780729\n\n\nTrophic.NicheInvertivore\n0.4976448\n0.5349728\n0.0000000\n0.1540251\n\n\nHabitatShrubland\n0.4643060\n0.4025983\n0.0000000\n1.2419117\n\n\nTrophic.NicheOmnivore\n0.4173193\n0.3897340\n0.0000000\n0.7990898\n\n\nMigration3\n0.4025452\n0.4416751\n0.0000000\n0.0229412\n\n\nHabitatHuman Modified\n0.3218146\n0.3512523\n0.0000000\n0.0393252\n\n\nHabitat.Density2\n0.3206026\n0.3326944\n0.0000000\n0.2352475\n\n\nHabitatMarine\n0.2968498\n0.3212837\n0.0000000\n0.0672203\n\n\nFP\n0.2877498\n0.2145901\n1.0180569\n1.0011153\n\n\nHabitatWoodland\n0.2852863\n0.3109020\n0.0000000\n0.0403295\n\n\nPrimary.LifestyleAquatic\n0.2812519\n0.2910131\n0.0000000\n0.2160031\n\n\nPrimary.LifestyleGeneralist\n0.2699918\n0.2665376\n0.4150469\n0.2856675\n\n\nTrophic.NicheVertivore\n0.2258425\n0.2461183\n0.0000000\n0.0319540\n\n\nIUCNVU\n0.1973080\n0.2174760\n0.0000000\n0.0000000\n\n\nPrimary.LifestyleTerrestrial\n0.1844623\n0.1907955\n0.0000000\n0.1424513\n\n\nHabitatRock\n0.1566310\n0.1726412\n0.0000000\n0.0000000\n\n\nTrophic.NicheScavenger\n0.1256653\n0.1385103\n0.0000000\n0.0000000\n\n\nIUCNEN\n0.1247224\n0.1374710\n0.0000000\n0.0000000\n\n\nTrophic.NicheFrugivore\n0.1227379\n0.1352837\n0.0000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n0.1226107\n0.1351435\n0.0000000\n0.0000000\n\n\nHabitatDesert\n0.1226107\n0.1351435\n0.0000000\n0.0000000\n\n\nTrophic.NicheGranivore\n0.1185844\n0.1283351\n0.0000000\n0.0269668\n\n\nHabitatRiverine\n0.1104086\n0.1216941\n0.0000000\n0.0000000\n\n\nTrophic.NicheHerbivore terrestrial\n0.0029407\n0.0000000\n0.0000000\n0.0368733\n\n\n\n\nrm(response, predictors, index, dd, dd_test, trained_control)\n\ntictoc::toc()\n\nLR1: 1.39 sec elapsed\n\n\n\n\n\n\n\n8 Log Ratio 2\n::: panel-tabset ## Train model\n\ntictoc::tic(\"LR2\")\n\n# Define model variables  (response, indices and predictors)\nresponse &lt;- \"log_R2_1\" \npredictors &lt;- reduced_predictors[[4]] # Replace with your actual predictors\nindex &lt;- indices_LR2\ndd &lt;- dat_train_LR2 %&gt;% select(all_of(c(response, predictors)))\ndd_test &lt;- dat_test_LR2 %&gt;% select(all_of(c(response, predictors)))\n\n# Define training control\ntrained_control &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 3,\n  savePredictions = \"final\",\n  returnResamp = \"final\",\n  verboseIter = FALSE,\n  index = index # Ensure indices_LR2 is defined\n)\n\n\nTrain modelSummarize individual modelsPredictive performancesEnsemble modelSummarize predictor importances\n\n\n\ntictoc::tic(\"LR2\")\n# Train all models at once using caretList / using bestFit hyperparameters from 03_HyperparameterTuning.qmd\nset.seed(42)\nmodelsList_LR2 &lt;- caretList(\n  as.formula(paste(response, \"~ .\")),\n  data = dd,\n  trControl = trained_control,\n  tuneList = list(\n    ranger = caretModelSpec(\n      method = \"ranger\",\n      tuneGrid = expand.grid(\n        mtry = 52,\n        splitrule = \"extratrees\",\n        min.node.size = 5\n      ),\n      importance = \"permutation\",\n      num.trees = 5000\n    ),\n    gbm = caretModelSpec(\n      method = \"gbm\",\n      tuneGrid = expand.grid(\n        n.trees = 50,\n        interaction.depth = 7,\n        shrinkage = 0.1,\n        n.minobsinnode = 10\n      ),\n      verbose = FALSE\n    ),\n    xgbTree = caretModelSpec(\n      method = \"xgbTree\",\n      tuneGrid = expand.grid(\n        nrounds = 1000,\n        eta = 0.1,\n        max_depth = 5,\n        gamma = 0.1,\n        colsample_bytree = 0.6,\n        min_child_weight = 1,\n        subsample = 1\n      )\n    )\n  )\n)\n\n\n\n\n## Summarzize across individual models ========================================#\n\n# Create resamples from the list of models\nresamps_LR2 &lt;- resamples(modelsList_LR2)\n\n# Plot the resampled error rates for each model\ndotplot_resamps_LR2 &lt;- dotplot(resamps_LR2)\n\n# Summarize the resamples\nsummary_resamps_LR2 &lt;- summary(resamps_LR2)\n\n# Combine everything into a list\nresamples_all_LR2 &lt;- list(\n  Jaccard1 = list(\n    resamps_LR2 = resamps_LR2,\n    Dotplot = dotplot_resamps_LR2, # store the dotplot object\n    Summary = summary_resamps_LR2  # store the summary object\n  )\n)\n\nresamples_all_LR2\n\n$Jaccard1\n$Jaccard1$resamps_LR2\n\nCall:\nresamples.default(x = modelsList_LR2)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \nPerformance metrics: MAE, RMSE, Rsquared \nTime estimates for: everything, final model fit \n\n$Jaccard1$Dotplot\n\n\n\n\n\n\n\n\n\n\n$Jaccard1$Summary\n\nCall:\nsummary.resamples(object = resamps_LR2)\n\nModels: ranger, gbm, xgbTree \nNumber of resamples: 10 \n\nMAE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.3004566 0.3102128 0.3328967 0.3301751 0.3451095 0.3652087    0\ngbm     0.3135943 0.3219454 0.3451893 0.3444557 0.3621308 0.3862579    0\nxgbTree 0.3087705 0.3220933 0.3501550 0.3430031 0.3573145 0.3742934    0\n\nRMSE \n             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.4730713 0.5097311 0.5262349 0.5340082 0.5596536 0.6167102    0\ngbm     0.4889392 0.5145147 0.5353704 0.5449522 0.5817995 0.6148332    0\nxgbTree 0.4861044 0.5169500 0.5383450 0.5427940 0.5663961 0.6092266    0\n\nRsquared \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nranger  0.06949563 0.1513409 0.2025256 0.1840501 0.2313987 0.2711515    0\ngbm     0.05656313 0.1190845 0.1314444 0.1555374 0.1773010 0.3341799    0\nxgbTree 0.06441885 0.1349925 0.1462681 0.1673689 0.1964141 0.3395451    0\n\n\n\n\n\n## Predictive Performance analysis ================================================\np_LR2 &lt;- as.data.frame(\n  predict(modelsList_LR2, newdata = dd_test)) %&gt;% \n  cbind(dd_test$log_R2_1) %&gt;%\n  mutate(\n    error_ranger = dd_test$log_R2_1-ranger,\n    error_gbm = dd_test$log_R2_1-gbm,\n    error_xgb = dd_test$log_R2_1-xgbTree) \n\np_LR2 %&gt;%\n  summarise(mean_ranger = mean(error_ranger),\n            mean_gbm = mean(error_gbm),\n            mean_xgb = mean(error_xgb)) %&gt;% \n  kableExtra::kable() # ranger performs best\n\n\n\n\nmean_ranger\nmean_gbm\nmean_xgb\n\n\n\n\n0.0190872\n0.0256455\n0.0106742\n\n\n\n\n\n\n\nThe ensemble model is not better than the ranger model alone. In fact, it’s a bit worse. We will discard the ensembleModel approach therefore.\n\n# Create the ensemble model ================================================\n\n## Are they correlated? \n# yes. not the best foundation for ensembleModels...\nmodelCor(resamples(modelsList_LR2))\n\n           ranger       gbm   xgbTree\nranger  1.0000000 0.9432978 0.8818711\ngbm     0.9432978 1.0000000 0.8783226\nxgbTree 0.8818711 0.8783226 1.0000000\n\nensembleModel_LR2 &lt;- caretEnsemble(\n    modelsList_LR2,\n    metric = \"Rsquared\",\n    trControl = trained_control)\nsummary(ensembleModel_LR2)\n\nThe following models were ensembled: ranger, gbm, xgbTree \nThey were weighted: \n-0.0032 0.638 0.1287 0.2721\nThe resulting Rsquared is: 0.133\nThe fit for each individual model on the Rsquared is: \n  method  Rsquared RsquaredSD\n  ranger 0.1840501 0.06937784\n     gbm 0.1555374 0.07997992\n xgbTree 0.1673689 0.07606861\n\n\n\n\n\n# Summarize predictor importances ============================================\n\nvarImp(ensembleModel_LR2) %&gt;% arrange(desc(overall))\n\n                                       overall      ranger        gbm\nAlphaSR_sp                         10.11612443 13.45387191  3.5872870\nD_AOO_a                             7.06423408  8.03006401  6.3709632\nMass                                5.91125118  4.39197715  7.2125691\nrel_occ_Ncells                      5.28562183  3.22690854 13.2052189\nmoran                               5.00331582  5.62424144  2.8997142\nsp_centr_lon                        4.02991924  3.96897188  4.0128941\nrel_lin                             3.90108847  3.96228598  4.9163448\nWesternness                         3.40195764  3.77229024  3.7513181\nSouthernness                        3.34686900  3.76580936  1.3683501\nrel_relCirc                         3.30619278  3.38336812  4.0674227\nrel_elonRatio                       3.08612960  2.83408947  5.1146154\ndatasetBirds_atlas_EBBA             2.99755240  4.82616959  0.0000000\nwidthMinRect                        2.64957109  1.84166274  1.8380608\nrel_maxDist                         2.58844044  2.93431106  2.5373628\nbearing                             2.56053181  2.05404239  1.3756330\nBetaSR_sp                           2.39102205  1.63847017  3.8209994\nx_intercept                         2.30105690  2.69313081  1.9823862\nminDist_toBorder_centr              2.23511502  2.11218934  1.3225806\nAOO                                 2.09620916  1.38227673  3.3300574\nHand.Wing.Index                     1.89980004  1.00235087  2.5155682\nelonMinRect                         1.83402244  1.35836400  2.6217947\nbearingMinRect                      1.80386776  1.42145124  3.8604522\nGlobRangeSize_m2                    1.75300372  0.80935280  3.3411735\nsp_centr_lat                        1.72544295  1.56041066  2.0281839\nHabitatWetland                      1.66402311  2.55889300  0.4809024\nsd_PC1                              1.61308232  0.88912136  1.7297101\nFP                                  1.60759743  0.26447157  3.3844611\ncirc                                1.48092238  0.97277223  3.0885051\nsd_PC2                              1.43763481  0.77443396  1.7672393\nDist_centroid_to_COG                1.23525714  1.28641647  0.5443269\ndatasetBirds_Atlas_New_York         1.22349209  1.99200586  0.0000000\nTrophic.NicheOmnivore               1.02904114  1.54241367  0.0000000\natlas_bearingMinRect                0.99347834  1.41773538  0.6469355\ndatasetBirds_atlas_Japan            0.76252634  1.24149306  0.0000000\nIUCNLC                              0.75452607  0.73905538  1.0666336\nTrophic.NicheHerbivore aquatic      0.64692302  1.03101451  0.0000000\nHabitatForest                       0.48606626  0.69272147  0.0000000\nIUCNNT                              0.40742195  0.60497066  0.0000000\nHabitatShrubland                    0.36438088  0.52361921  0.0000000\nTrophic.NicheInvertivore            0.24672857  0.31287260  0.2103360\nHabitatHuman Modified               0.22427470  0.29270708  0.0000000\nTrophic.NicheVertivore              0.08939105  0.13925314  0.0000000\nHabitatMarine                       0.07209556  0.11738104  0.0000000\nHabitatGrassland                    0.06687058  0.10887409  0.0000000\nHabitatWoodland                     0.06413768  0.08586770  0.0000000\nIUCNEN                              0.03521777  0.05733915  0.0000000\nTrophic.NicheScavenger              0.03301031  0.05374512  0.0000000\nHabitatDesert                       0.03061986  0.04985316  0.0000000\nTrophic.NicheNectarivore            0.03061986  0.04985316  0.0000000\nTrophic.NicheFrugivore              0.03051382  0.04678184  0.0000000\nIUCNVU                              0.02388750  0.03512442  0.0000000\nHabitatRiverine                     0.01897091  0.03088713  0.0000000\nTrophic.NicheHerbivore terrestrial  0.01421970  0.00000000  0.0000000\nTrophic.NicheGranivore              0.01308781  0.02130868  0.0000000\nHabitatRock                         0.01164118  0.01895338  0.0000000\n                                       xgbTree\nAlphaSR_sp                         5.377015354\nD_AOO_a                            5.127273752\nMass                               8.858445885\nrel_occ_Ncells                     6.367675417\nmoran                              4.542160649\nsp_centr_lon                       4.180891012\nrel_lin                            3.277417429\nWesternness                        2.368309039\nSouthernness                       3.300205388\nrel_relCirc                        2.765195816\nrel_elonRatio                      2.717785324\ndatasetBirds_atlas_EBBA            0.127190053\nwidthMinRect                       4.927895160\nrel_maxDist                        1.801541382\nbearing                            4.308631322\nBetaSR_sp                          3.479426951\nx_intercept                        1.532370430\nminDist_toBorder_centr             2.954954537\nAOO                                3.186811726\nHand.Wing.Index                    3.713064370\nelonMinRect                        2.576850948\nbearingMinRect                     1.727962972\nGlobRangeSize_m2                   3.214713143\nsp_centr_lat                       1.969257589\nHabitatWetland                     0.125136104\nsd_PC1                             3.255590054\nFP                                 3.916820977\ncirc                               1.912214237\nsd_PC2                             2.836934725\nDist_centroid_to_COG               1.442064965\ndatasetBirds_Atlas_New_York        0.000000000\nTrophic.NicheOmnivore              0.311882158\natlas_bearingMinRect               0.162505481\ndatasetBirds_atlas_Japan           0.000000000\nIUCNLC                             0.643193405\nTrophic.NicheHerbivore aquatic     0.052201733\nHabitatForest                      0.231350658\nIUCNNT                             0.136866060\nHabitatShrubland                   0.163305439\nTrophic.NicheInvertivore           0.108834604\nHabitatHuman Modified              0.169873359\nTrophic.NicheVertivore             0.014743358\nHabitatMarine                      0.000000000\nHabitatGrassland                   0.000000000\nHabitatWoodland                    0.043515297\nIUCNEN                             0.000000000\nTrophic.NicheScavenger             0.000000000\nHabitatDesert                      0.000000000\nTrophic.NicheNectarivore           0.000000000\nTrophic.NicheFrugivore             0.006797264\nIUCNVU                             0.008834825\nHabitatRiverine                    0.000000000\nTrophic.NicheHerbivore terrestrial 0.054289652\nTrophic.NicheGranivore             0.000000000\nHabitatRock                        0.000000000\n\n# ranger\nimp_ranger &lt;- varImp(modelsList_LR2[[1]])$importance %&gt;% \n  as.data.frame() %&gt;% \n  rename(\"imp_ranger\" = \"Overall\")\nimp_ranger$var &lt;- row.names(imp_ranger)\n\n# gbm\nimp_gbm &lt;- varImp(modelsList_LR2[[2]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_gbm\" = \"Overall\")\nimp_gbm$var &lt;- row.names(imp_gbm)\nimp_merged &lt;- merge(imp_ranger, imp_gbm)\n\n# xgb\nimp_xgb &lt;- varImp(modelsList_LR2[[3]])$importance %&gt;% \n  as.data.frame()%&gt;% \n  rename(\"imp_xgb\" = \"Overall\")\nimp_xgb$var &lt;- row.names(imp_xgb)\n\nimp_merged_all_LR2 &lt;- merge(imp_merged, imp_xgb) %&gt;% \n  arrange(desc(imp_ranger))\n\n# Print results\nimp_merged_all_LR2 %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\nvar\nimp_ranger\nimp_gbm\nimp_xgb\n\n\n\n\nAlphaSR_sp\n100.0000000\n27.165676\n60.6993080\n\n\nD_AOO_a\n59.6858961\n48.245798\n57.8800595\n\n\nmoran\n41.8038872\n21.958850\n51.2749156\n\n\ndatasetBirds_atlas_EBBA\n35.8719752\n0.000000\n1.4358055\n\n\nMass\n32.6447077\n54.619080\n100.0000000\n\n\nsp_centr_lon\n29.5005921\n30.388698\n47.1966648\n\n\nrel_lin\n29.4508971\n37.230317\n36.9976571\n\n\nWesternness\n28.0386960\n28.407845\n26.7350399\n\n\nSouthernness\n27.9905249\n10.362192\n37.2549026\n\n\nrel_relCirc\n25.1479139\n30.801630\n31.2153605\n\n\nrel_occ_Ncells\n23.9849804\n100.000000\n71.8825345\n\n\nrel_maxDist\n21.8101605\n19.214848\n20.3369915\n\n\nrel_elonRatio\n21.0652331\n38.731773\n30.6801595\n\n\nx_intercept\n20.0175149\n15.012142\n17.2984116\n\n\nHabitatWetland\n19.0197515\n3.641760\n1.4126192\n\n\nminDist_toBorder_centr\n15.6994905\n10.015590\n33.3574825\n\n\nbearing\n15.2672955\n10.417343\n48.6386820\n\n\ndatasetBirds_Atlas_New_York\n14.8061902\n0.000000\n0.0000000\n\n\nwidthMinRect\n13.6887191\n13.919199\n55.6293420\n\n\nBetaSR_sp\n12.1784285\n28.935525\n39.2780742\n\n\nsp_centr_lat\n11.5982274\n15.358957\n22.2302830\n\n\nTrophic.NicheOmnivore\n11.4644593\n0.000000\n3.5207322\n\n\nbearingMinRect\n10.5653692\n29.234292\n19.5063897\n\n\natlas_bearingMinRect\n10.5377500\n4.899089\n1.8344694\n\n\nAOO\n10.2741928\n25.217737\n35.9748399\n\n\nelonMinRect\n10.0964541\n19.854231\n29.0891990\n\n\nDist_centroid_to_COG\n9.5616822\n4.122059\n16.2789837\n\n\ndatasetBirds_atlas_Japan\n9.2277752\n0.000000\n0.0000000\n\n\nTrophic.NicheHerbivore aquatic\n7.6633293\n0.000000\n0.5892877\n\n\nHand.Wing.Index\n7.4502781\n19.049803\n41.9155280\n\n\ncirc\n7.2304258\n23.388519\n21.5863399\n\n\nsd_PC1\n6.6086653\n13.098685\n36.7512552\n\n\nGlobRangeSize_m2\n6.0157612\n25.301917\n36.2898096\n\n\nsd_PC2\n5.7562163\n13.382885\n32.0251968\n\n\nIUCNLC\n5.4932541\n8.077364\n7.2607928\n\n\nHabitatForest\n5.1488633\n0.000000\n2.6116393\n\n\nIUCNNT\n4.4966286\n0.000000\n1.5450347\n\n\nHabitatShrubland\n3.8919593\n0.000000\n1.8434999\n\n\nTrophic.NicheInvertivore\n2.3255209\n1.592825\n1.2285970\n\n\nHabitatHuman Modified\n2.1756345\n0.000000\n1.9176429\n\n\nFP\n1.9657655\n25.629724\n44.2156675\n\n\nTrophic.NicheVertivore\n1.0350414\n0.000000\n0.1664328\n\n\nHabitatMarine\n0.8724703\n0.000000\n0.0000000\n\n\nHabitatGrassland\n0.8092399\n0.000000\n0.0000000\n\n\nHabitatWoodland\n0.6382378\n0.000000\n0.4912295\n\n\nIUCNEN\n0.4261907\n0.000000\n0.0000000\n\n\nTrophic.NicheScavenger\n0.3994770\n0.000000\n0.0000000\n\n\nHabitatDesert\n0.3705488\n0.000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n0.3705488\n0.000000\n0.0000000\n\n\nTrophic.NicheFrugivore\n0.3477203\n0.000000\n0.0767320\n\n\nIUCNVU\n0.2610730\n0.000000\n0.0997333\n\n\nHabitatRiverine\n0.2295780\n0.000000\n0.0000000\n\n\nTrophic.NicheGranivore\n0.1583832\n0.000000\n0.0000000\n\n\nHabitatRock\n0.1408767\n0.000000\n0.0000000\n\n\nTrophic.NicheHerbivore terrestrial\n0.0000000\n0.000000\n0.6128575\n\n\n\n\nvarImp(ensembleModel_LR2) %&gt;% \n  arrange(desc(overall)) %&gt;% \n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\noverall\nranger\ngbm\nxgbTree\n\n\n\n\nAlphaSR_sp\n10.1161244\n13.4538719\n3.5872870\n5.3770154\n\n\nD_AOO_a\n7.0642341\n8.0300640\n6.3709632\n5.1272738\n\n\nMass\n5.9112512\n4.3919772\n7.2125691\n8.8584459\n\n\nrel_occ_Ncells\n5.2856218\n3.2269085\n13.2052189\n6.3676754\n\n\nmoran\n5.0033158\n5.6242414\n2.8997142\n4.5421606\n\n\nsp_centr_lon\n4.0299192\n3.9689719\n4.0128941\n4.1808910\n\n\nrel_lin\n3.9010885\n3.9622860\n4.9163448\n3.2774174\n\n\nWesternness\n3.4019576\n3.7722902\n3.7513181\n2.3683090\n\n\nSouthernness\n3.3468690\n3.7658094\n1.3683501\n3.3002054\n\n\nrel_relCirc\n3.3061928\n3.3833681\n4.0674227\n2.7651958\n\n\nrel_elonRatio\n3.0861296\n2.8340895\n5.1146154\n2.7177853\n\n\ndatasetBirds_atlas_EBBA\n2.9975524\n4.8261696\n0.0000000\n0.1271901\n\n\nwidthMinRect\n2.6495711\n1.8416627\n1.8380608\n4.9278952\n\n\nrel_maxDist\n2.5884404\n2.9343111\n2.5373628\n1.8015414\n\n\nbearing\n2.5605318\n2.0540424\n1.3756330\n4.3086313\n\n\nBetaSR_sp\n2.3910220\n1.6384702\n3.8209994\n3.4794270\n\n\nx_intercept\n2.3010569\n2.6931308\n1.9823862\n1.5323704\n\n\nminDist_toBorder_centr\n2.2351150\n2.1121893\n1.3225806\n2.9549545\n\n\nAOO\n2.0962092\n1.3822767\n3.3300574\n3.1868117\n\n\nHand.Wing.Index\n1.8998000\n1.0023509\n2.5155682\n3.7130644\n\n\nelonMinRect\n1.8340224\n1.3583640\n2.6217947\n2.5768509\n\n\nbearingMinRect\n1.8038678\n1.4214512\n3.8604522\n1.7279630\n\n\nGlobRangeSize_m2\n1.7530037\n0.8093528\n3.3411735\n3.2147131\n\n\nsp_centr_lat\n1.7254429\n1.5604107\n2.0281839\n1.9692576\n\n\nHabitatWetland\n1.6640231\n2.5588930\n0.4809024\n0.1251361\n\n\nsd_PC1\n1.6130823\n0.8891214\n1.7297101\n3.2555901\n\n\nFP\n1.6075974\n0.2644716\n3.3844611\n3.9168210\n\n\ncirc\n1.4809224\n0.9727722\n3.0885051\n1.9122142\n\n\nsd_PC2\n1.4376348\n0.7744340\n1.7672393\n2.8369347\n\n\nDist_centroid_to_COG\n1.2352571\n1.2864165\n0.5443269\n1.4420650\n\n\ndatasetBirds_Atlas_New_York\n1.2234921\n1.9920059\n0.0000000\n0.0000000\n\n\nTrophic.NicheOmnivore\n1.0290411\n1.5424137\n0.0000000\n0.3118822\n\n\natlas_bearingMinRect\n0.9934783\n1.4177354\n0.6469355\n0.1625055\n\n\ndatasetBirds_atlas_Japan\n0.7625263\n1.2414931\n0.0000000\n0.0000000\n\n\nIUCNLC\n0.7545261\n0.7390554\n1.0666336\n0.6431934\n\n\nTrophic.NicheHerbivore aquatic\n0.6469230\n1.0310145\n0.0000000\n0.0522017\n\n\nHabitatForest\n0.4860663\n0.6927215\n0.0000000\n0.2313507\n\n\nIUCNNT\n0.4074220\n0.6049707\n0.0000000\n0.1368661\n\n\nHabitatShrubland\n0.3643809\n0.5236192\n0.0000000\n0.1633054\n\n\nTrophic.NicheInvertivore\n0.2467286\n0.3128726\n0.2103360\n0.1088346\n\n\nHabitatHuman Modified\n0.2242747\n0.2927071\n0.0000000\n0.1698734\n\n\nTrophic.NicheVertivore\n0.0893910\n0.1392531\n0.0000000\n0.0147434\n\n\nHabitatMarine\n0.0720956\n0.1173810\n0.0000000\n0.0000000\n\n\nHabitatGrassland\n0.0668706\n0.1088741\n0.0000000\n0.0000000\n\n\nHabitatWoodland\n0.0641377\n0.0858677\n0.0000000\n0.0435153\n\n\nIUCNEN\n0.0352178\n0.0573392\n0.0000000\n0.0000000\n\n\nTrophic.NicheScavenger\n0.0330103\n0.0537451\n0.0000000\n0.0000000\n\n\nHabitatDesert\n0.0306199\n0.0498532\n0.0000000\n0.0000000\n\n\nTrophic.NicheNectarivore\n0.0306199\n0.0498532\n0.0000000\n0.0000000\n\n\nTrophic.NicheFrugivore\n0.0305138\n0.0467818\n0.0000000\n0.0067973\n\n\nIUCNVU\n0.0238875\n0.0351244\n0.0000000\n0.0088348\n\n\nHabitatRiverine\n0.0189709\n0.0308871\n0.0000000\n0.0000000\n\n\nTrophic.NicheHerbivore terrestrial\n0.0142197\n0.0000000\n0.0000000\n0.0542897\n\n\nTrophic.NicheGranivore\n0.0130878\n0.0213087\n0.0000000\n0.0000000\n\n\nHabitatRock\n0.0116412\n0.0189534\n0.0000000\n0.0000000\n\n\n\n\nrm(response, predictors, index, dd, dd_test, trained_control)\n\ntictoc::toc()\n\nLR2: 1.33 sec elapsed\n\n\n\n\n\n\n\n9 Save the models\n\n# save.image(\"data/RData/04_FinalModels.RData\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Script 4 - FinalModels</span>"
    ]
  },
  {
    "objectID": "04b_PartialDependencePlots.html",
    "href": "04b_PartialDependencePlots.html",
    "title": "5  Script 4b - Partial Plots",
    "section": "",
    "text": "Source custom functionsMachineLearning packagesLoad RData to reduce computing time\n\n\n\nrm(list = ls())\nsource(\"src/functions.R\")\n\n\n\n\npckgs &lt;- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"pdp\", \n           \"gridExtra\")\n\ninstall_and_load(pckgs)\n\n\n\n\n# Load final workspace to save computing time:\nload(\"data/RData/04b_pdp.RData\")\n\n\n\n\n\n6 Partial Dependence Plots\n\nJaccard 1Jaccard 2LogRatio 1LogRatio 2\n\n\n\npp_list_ranger_J1 &lt;- replicate(39, list())\nranger &lt;- all_models[[\"Jaccard1\"]]$ranger\n\npp_list_xgb_J1 &lt;- replicate(39, list())\nxgb &lt;- all_models[[\"Jaccard1\"]]$xgb\n\npp_list_gbm_J1 &lt;- replicate(39, list())\ngbm &lt;- all_models[[\"Jaccard1\"]]$gbm\n\n\n\ndd &lt;- dat_train_J1\npred_vars &lt;- names(ranger$trainingData)[-1]\n\n\n# Get all partial dependencies ==================\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_ranger_J1[[var]] &lt;- partial(\n        ranger,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_xgb_J1[[var]] &lt;- partial(\n        xgb,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_gbm_J1[[var]] &lt;- partial(\n        gbm,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\npp_list_J1 &lt;- list(pp_list_ranger_J1, pp_list_xgb_J1, pp_list_gbm_J1)\n\nsaveRDS(pp_list_J1, \"data/rds/pdp_list_J1.rds\")\n\n\n# Plotting\n\n# Define the predictor names\npredictors &lt;- pred_vars\n\n# Initialize an empty list to store the plots\nplots &lt;- list()\n\n# Function to determine if a column is categorical\nis_categorical &lt;- function(column) {\n  is.factor(column) || is.character(column)\n}\n\n# Loop through the indices and create plots\nfor (i in seq_along(1:length(predictors))) {\n  predictor &lt;- names(pp_list_J1[[1]][[i]])[1]\n  \n  if (is_categorical(dd[[predictor]])) {\n    # Create boxplot for categorical predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_boxplot(data = pp_list_J1[[1]][[i]], \n                   aes(x = .data[[predictor]], y = yhat)) +\n      geom_boxplot(data = pp_list_J1[[2]][[i]],\n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dashed\") +\n      geom_boxplot(data = pp_list_J1[[3]][[i]], \n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Boxplots\") +\n      theme_bw() +\n      ylim(0, 1)\n  } else {\n    # Create line plot for continuous predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_line(data = pp_list_J1[[1]][[i]], \n                aes(x = .data[[predictor]], y = yhat)) +\n      geom_line(data = pp_list_J1[[2]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dashed\") +\n      geom_line(data = pp_list_J1[[3]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Plots\") +\n      theme_bw() +\n      ylim(0, 1)\n  }\n}\n\n# Arrange the plots in a grid\ngridExtra::grid.arrange(grobs = plots, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\npp_list_ranger_J2 &lt;- replicate(15, list())\nranger &lt;- all_models[[\"Jaccard2\"]]$ranger\n\npp_list_xgb_J2 &lt;- replicate(15, list())\nxgb &lt;- all_models[[\"Jaccard2\"]]$xgb\n\npp_list_gbm_J2 &lt;- replicate(15, list())\ngbm &lt;- all_models[[\"Jaccard2\"]]$gbm\n\n\n\ndd &lt;- dat_train_J2\npred_vars &lt;- names(ranger$trainingData)[-1]\n\n\n# Get all partial dependencies ==================\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_ranger_J2[[var]] &lt;- partial(\n        ranger,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_xgb_J2[[var]] &lt;- partial(\n        xgb,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_gbm_J2[[var]] &lt;- partial(\n        gbm,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\npp_list_J2 &lt;- list(pp_list_ranger_J2, pp_list_xgb_J2, pp_list_gbm_J2)\n\nsaveRDS(pp_list_J2, \"data/rds/pdp_list_J2.rds\")\n\n\n# Plotting\n\n# Define the predictor names\npredictors &lt;- pred_vars\n\n# Initialize an empty list to store the plots\nplots &lt;- list()\n\n# Function to determine if a column is categorical\nis_categorical &lt;- function(column) {\n  is.factor(column) || is.character(column)\n}\n\n# Loop through the indices and create plots\nfor (i in seq_along(1:length(predictors))) {\n  predictor &lt;- names(pp_list_J2[[1]][[i]])[1]\n  \n  if (is_categorical(dd[[predictor]])) {\n    # Create boxplot for categorical predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_boxplot(data = pp_list_J2[[1]][[i]], \n                   aes(x = .data[[predictor]], y = yhat)) +\n      geom_boxplot(data = pp_list_J2[[2]][[i]],\n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dashed\") +\n      geom_boxplot(data = pp_list_J2[[3]][[i]], \n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Boxplots\") +\n      theme_bw() +\n      ylim(0, 1)\n  } else {\n    # Create line plot for continuous predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_line(data = pp_list_J2[[1]][[i]], \n                aes(x = .data[[predictor]], y = yhat)) +\n      geom_line(data = pp_list_J2[[2]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dashed\") +\n      geom_line(data = pp_list_J2[[3]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Plots\") +\n      theme_bw() +\n      ylim(0, 1)\n  }\n}\n\n# Arrange the plots in a grid\ngridExtra::grid.arrange(grobs = plots, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\npp_list_ranger_LR1 &lt;- replicate(38, list())\nranger &lt;- all_models[[\"LogRatio1\"]]$ranger\n\npp_list_xgb_LR1 &lt;- replicate(38, list())\nxgb &lt;- all_models[[\"LogRatio1\"]]$xgb\n\npp_list_gbm_LR1 &lt;- replicate(38, list())\ngbm &lt;- all_models[[\"LogRatio1\"]]$gbm\n\n\n\ndd &lt;- dat_train_LR1\npred_vars &lt;- names(ranger$trainingData)[-1]\n\n\n# Get all partial dependencies ==================\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_ranger_LR1[[var]] &lt;- partial(\n        ranger,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_xgb_LR1[[var]] &lt;- partial(\n        xgb,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_gbm_LR1[[var]] &lt;- partial(\n        gbm,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\npp_list_LR1 &lt;- list(pp_list_ranger_LR1, pp_list_xgb_LR1, pp_list_gbm_LR1)\n\nsaveRDS(pp_list_LR1, \"data/rds/pdp_list_LR1.rds\")\n\n\n# Plotting\n\n# Define the predictor names\npredictors &lt;- pred_vars\n\n# Initialize an empty list to store the plots\nplots &lt;- list()\n\n# Function to determine if a column is categorical\nis_categorical &lt;- function(column) {\n  is.factor(column) || is.character(column)\n}\n\n# Loop through the indices and create plots\nfor (i in seq_along(1:length(predictors))) {\n  predictor &lt;- names(pp_list_LR1[[1]][[i]])[1]\n  \n  if (is_categorical(dd[[predictor]])) {\n    # Create boxplot for categorical predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_boxplot(data = pp_list_LR1[[1]][[i]], \n                   aes(x = .data[[predictor]], y = yhat)) +\n      geom_boxplot(data = pp_list_LR1[[2]][[i]],\n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dashed\") +\n      geom_boxplot(data = pp_list_LR1[[3]][[i]], \n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Boxplots\") +\n      theme_bw() +\n      ylim(-2, 2)\n  } else {\n    # Create line plot for continuous predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_line(data = pp_list_LR1[[1]][[i]], \n                aes(x = .data[[predictor]], y = yhat)) +\n      geom_line(data = pp_list_LR1[[2]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dashed\") +\n      geom_line(data = pp_list_LR1[[3]][[i]], \n                aes(x = .data[[predictor]], y = yhat), \n                linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Plots\") +\n      theme_bw() +\n      ylim(-2, 2)\n  }\n}\n\n# Arrange the plots in a grid\ngridExtra::grid.arrange(grobs = plots, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\npp_list_ranger_LR2 &lt;- replicate(33, list())\nranger &lt;- all_models[[\"LogRatio2\"]]$ranger\n\npp_list_xgb_LR2 &lt;- replicate(33, list())\nxgb &lt;- all_models[[\"LogRatio2\"]]$xgb\n\npp_list_gbm_LR2 &lt;- replicate(33, list())\ngbm &lt;- all_models[[\"LogRatio2\"]]$gbm\n\n\n\ndd &lt;- dat_train_LR2\npred_vars &lt;- names(ranger$trainingData)[-1]\n\n\n# Partial dependence plots\n# Get all partial dependencies ==================\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_ranger_LR2[[var]] &lt;- partial(\n        ranger,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_xgb_LR2[[var]] &lt;- partial(\n        xgb,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\n\nfor (var in seq_along(1:length(pred_vars))){\n    pred_var &lt;- pred_vars[var]\n    pp_list_gbm_LR2[[var]] &lt;- partial(\n        gbm,\n        pred.var = pred_var,\n        plot = FALSE)\n}\n\npp_list_LR2 &lt;- list(pp_list_ranger_LR2, pp_list_xgb_LR2, pp_list_gbm_LR2)\n\nsaveRDS(pp_list_LR2, \"data/rds/pdp_list_LR2.rds\")\n\n\n# Plotting\n\n# Define the predictor names\npredictors &lt;- pred_vars\n\n# Initialize an empty list to store the plots\nplots &lt;- list()\n\n# Function to determine if a column is categorical\nis_categorical &lt;- function(column) {\n  is.factor(column) || is.character(column)\n}\n\n# Loop through the indices and create plots\nfor (i in seq_along(1:length(predictors))) {\n  predictor &lt;- names(pp_list_LR2[[1]][[i]])[1]\n  \n  if (is_categorical(dd[[predictor]])) {\n    # Create boxplot for categorical predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_boxplot(data = pp_list_LR2[[1]][[i]], \n                   aes(x = .data[[predictor]], y = yhat)) +\n      geom_boxplot(data = pp_list_LR2[[2]][[i]],\n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dashed\") +\n      geom_boxplot(data = pp_list_LR2[[3]][[i]], \n                   aes(x = .data[[predictor]], y = yhat), \n                   linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Boxplots\") +\n      theme_bw() +\n      ylim(-2, 2)\n  } else {\n    # Create line plot for continuous predictors\n    plots[[i]] &lt;- ggplot() +\n      geom_line(data = pp_list_LR2[[1]][[i]], \n                aes(x = .data[[predictor]], y = yhat)) +\n      geom_line(data = pp_list_LR2[[2]][[i]], \n                aes(x = .data[[predictor]], y = yhat), linetype = \"dashed\") +\n      geom_line(data = pp_list_LR2[[3]][[i]], \n                aes(x = .data[[predictor]], y = yhat), linetype = \"dotted\") +\n      labs(x = paste(predictor), \n           y = \"Partial Dependence\", \n           title = \"Partial Dependence Plots\") +\n      theme_bw() +\n      ylim(-2, 2)\n  }\n}\n\n# Arrange the plots in a grid\ngridExtra::grid.arrange(grobs = plots, ncol = 4)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Script 4b - Partial Plots</span>"
    ]
  },
  {
    "objectID": "05_VarPart_v2_loop.html",
    "href": "05_VarPart_v2_loop.html",
    "title": "6  Script 5 - Variation Partitioning",
    "section": "",
    "text": "Source: Custom FunctionsMachine Learning PackagesLoad RData to reduce computing time\n\n\n\nrm(list = ls())\n#setwd(\"StaticPatterNN/\")\nsource(\"src/functions.R\")\n\n\n\n\npckgs &lt;- c(\"dplyr\", \"ggplot2\", \"reshape2\", \n           \"ggcorrplot\", \n           \"caret\",  \"recipes\",   \"caretEnsemble\", \n           \"randomForest\", \"ranger\", \"gbm\", \"xgboost\", \n           \"vegan\", \"pdp\", \n           \"gridExtra\", \"kableExtra\")\n\ninstall_and_load(pckgs)\n\n\n\n\n# Load workspace to save computing time:\n## it has: varPart from ranger models\n## recursive feature selection results\n\n# load(\"data/varPart_rfe.RData\")\n# load(\"data/models.RData\")\n# load(\"data/RData/01_Data_prep.RData\")\nload(\"data/RData/05_VarPart_loop.RData\")\n\n\n\n\n\n6.0.1 Variation Partitioning between Hypotheses\nNow we will look which of the four hypotheses explains most variation in the response.\n\nVariation Partitioning - ranger loops (Jaccard & log Ratio)\n\n\n\nindex_list &lt;- list(indices_J1, indices_J2, indices_LR1, indices_LR2)\ndat_train_list &lt;- list(dat_train_J1, dat_train_J2, dat_train_LR1, dat_train_LR2)\nresponse_list &lt;- c(\"Jaccard\", \"Jaccard\", \"log_R2_1\", \"log_R2_1\")\n\nvarPart_list &lt;- replicate(4, list())\n\nfor(i in seq_along(1:4)){\n  \n  dat_train &lt;- dat_train_list[[i]]\n  index &lt;- index_list[[i]]\n  response &lt;- response_list[[i]]\n\n  trainControl &lt;- trainControl(\n    method = \"repeatedcv\",\n    number = 10,\n    repeats = 3,\n    savePredictions = \"final\",\n    returnResamp = \"final\",\n    verboseIter = FALSE,\n    index = index)\n\n  tictoc::tic(\"ranger full model\")\n  set.seed(42)\n  full_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train,\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\ntictoc::toc()\n\n\n# Train ranger model\ntictoc::tic(\"ranger H1\")\nset.seed(42)\nH1_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(H1_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\ntictoc::toc()\n\n\ntictoc::tic(\"ranger H2\")\nset.seed(42)\nH2_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(H2_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\ntictoc::toc()\n\n\ntictoc::tic(\"ranger H3\")\nset.seed(42)\nH3_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(H3_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\ntictoc::toc()\n\ntictoc::tic(\"ranger H4\")\nset.seed(42)\nH4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(H4_vars)),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\ntictoc::toc()\n\n\n### combinations of 2 hypotheses:\n\nset.seed(42)\nH1H2_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H2_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH1H3_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H3_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH1H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H4_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\n\nset.seed(42)\nH2H3_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H2_vars, H3_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\n\nset.seed(42)\nH2H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H2_vars, H4_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH3H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H3_vars, H4_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\n\n### combinations of 3 hypotheses together =====\n\nset.seed(42)\nH1H2H3_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H2_vars, H3_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH1H2H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H2_vars, H4_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH1H3H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H3_vars, H4_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nset.seed(42)\nH2H3H4_model &lt;- train(\n    as.formula(paste0(response, \"~ .\")),\n    data = dat_train %&gt;% select(response, any_of(c(H1_vars, H2_vars, H3_vars))),\n    method = \"ranger\",\n    trControl = trainControl,\n    importance = \"permutation\",\n    scale.permutation.importance = TRUE,\n    num.trees = 5000,\n    respect.unordered.factors = TRUE,\n    oob.error = TRUE,\n    tuneLength = 5)\n\nvarPart_list[[i]] &lt;- list(\n    full_model, H1_model, H2_model, H3_model, H4_model, \n    H1H2_model, H1H3_model, H1H4_model, \n    H2H3_model, H2H4_model, \n    H3H4_model, \n    H1H2H3_model, H1H2H4_model, H1H3H4_model, \n    H2H3H4_model)\n}\n\nsaveRDS(varPart_list, \"data/varPart_list_loop.rds\")\n\n\n\n\n\n# ===== Performance eval ======= #\n\nperformance &lt;- list()\nperformance_all &lt;- list()\nplots &lt;- list()\ndat_test_list &lt;- list(dat_test_J1, dat_test_J2, dat_test_LR1, dat_test_LR2)\nfor(i in seq_along(1:4)){\n  \n  dat_test &lt;- dat_test_list[[i]]\n  index &lt;- index_list[[i]]\n  response &lt;- response_list[[i]]\n  models &lt;- varPart_list[[i]]\n\n\n# Predict on your test data\npredictions_full &lt;- predict(models[[1]], newdata = dat_test)\n\npredictions_H1 &lt;- predict(models[[2]], newdata = dat_test)\npredictions_H2 &lt;- predict(models[[3]], newdata = dat_test)\npredictions_H3 &lt;- predict(models[[4]], newdata = dat_test)\npredictions_H4 &lt;- predict(models[[5]], newdata = dat_test)\n\npredictions_H1H2 &lt;- predict(models[[6]], newdata = dat_test)\npredictions_H1H3 &lt;- predict(models[[7]], newdata = dat_test)\npredictions_H1H4 &lt;- predict(models[[8]], newdata = dat_test)\npredictions_H2H3 &lt;- predict(models[[9]], newdata = dat_test)\npredictions_H2H4 &lt;- predict(models[[10]], newdata = dat_test)\npredictions_H3H4 &lt;- predict(models[[11]], newdata = dat_test)\n\npredictions_H1H2H3 &lt;- predict(models[[12]], newdata = dat_test)\npredictions_H1H2H4 &lt;- predict(models[[13]], newdata = dat_test)\npredictions_H1H3H4 &lt;- predict(models[[14]], newdata = dat_test)\npredictions_H2H3H4 &lt;- predict(models[[15]], newdata = dat_test)\n\n# Calculate the performance metrics\nperf &lt;- rbind(postResample(pred = predictions_full, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H1, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H2, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H3, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H4, obs = dat_test %&gt;% pull(response)),\n              \n              postResample(pred = predictions_H1H2, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H1H3, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H1H4, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H2H3, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H2H4, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H3H4, obs = dat_test %&gt;% pull(response)),\n              \n              postResample(pred = predictions_H1H2H3, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H1H2H4, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H1H3H4, obs = dat_test %&gt;% pull(response)),\n              postResample(pred = predictions_H2H3H4, obs = dat_test %&gt;% pull(response))) %&gt;% \n                as.data.frame() %&gt;% \n                round(4)\n\nmodel &lt;- c(\"full\", \n            seq(1:4), \n            \"H1H2\", \"H1H3\", \"H1H4\", \"H2H3\", \"H2H4\", \"H3H4\", \n            \"H1H2H3\", \"H1H2H4\", \"H1H3H4\", \"H2H3H4\")\nperf$model &lt;- model\nperf$subset &lt;- paste0(response,\"_\", i)\n\n# Print the performance metrics\n\nperf %&gt;% \n    kableExtra::kable()\nperf %&gt;% \n    kableExtra::kable() %&gt;% \n    write.csv(paste0(\"data/csv/performance_varExpl_rf_loop_\", response,\"_\",i, \".csv\"))\n\nperformance[[i]] &lt;- slice_min(perf, RMSE) %&gt;% \n    slice_max(Rsquared)\n\nperformance_all[[i]] &lt;- perf\n\n# Create a bar plot of variance explained\nplots[[i]] &lt;- ggplot(perf, aes(x = reorder(model, RMSE), y = Rsquared)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Model\", y = \"Variance Explained\") +\n  theme_minimal()\n}\n\ndo.call(rbind, performance) %&gt;% kableExtra::kable()\n\n\n\n\nRMSE\nRsquared\nMAE\nmodel\nsubset\n\n\n\n\n0.1165\n0.8203\n0.0742\nfull\nJaccard_1\n\n\n0.1247\n0.7974\n0.0807\nH1H2H3\nJaccard_2\n\n\n0.1247\n0.7974\n0.0807\nH2H3H4\nJaccard_2\n\n\n0.4538\n0.1899\n0.2953\nH1H3H4\nlog_R2_1_3\n\n\n0.5129\n0.0895\n0.2932\nfull\nlog_R2_1_4\n\n\n\n\ndo.call(rbind, performance_all) %&gt;% kableExtra::kable()\n\n\n\n\nRMSE\nRsquared\nMAE\nmodel\nsubset\n\n\n\n\n0.1165\n0.8203\n0.0742\nfull\nJaccard_1\n\n\n0.2593\n0.1080\n0.2185\n1\nJaccard_1\n\n\n0.1181\n0.8159\n0.0761\n2\nJaccard_1\n\n\n0.2365\n0.3063\n0.1855\n3\nJaccard_1\n\n\n0.2576\n0.1182\n0.2193\n4\nJaccard_1\n\n\n0.1188\n0.8137\n0.0749\nH1H2\nJaccard_1\n\n\n0.1948\n0.4960\n0.1524\nH1H3\nJaccard_1\n\n\n0.2282\n0.3058\n0.1851\nH1H4\nJaccard_1\n\n\n0.1166\n0.8205\n0.0749\nH2H3\nJaccard_1\n\n\n0.1170\n0.8192\n0.0755\nH2H4\nJaccard_1\n\n\n0.2224\n0.3581\n0.1779\nH3H4\nJaccard_1\n\n\n0.1171\n0.8183\n0.0743\nH1H2H3\nJaccard_1\n\n\n0.1185\n0.8146\n0.0747\nH1H2H4\nJaccard_1\n\n\n0.1936\n0.5022\n0.1508\nH1H3H4\nJaccard_1\n\n\n0.1171\n0.8183\n0.0743\nH2H3H4\nJaccard_1\n\n\n0.1248\n0.7970\n0.0806\nfull\nJaccard_2\n\n\n0.2595\n0.1364\n0.2206\n1\nJaccard_2\n\n\n0.1299\n0.7807\n0.0844\n2\nJaccard_2\n\n\n0.2229\n0.3726\n0.1748\n3\nJaccard_2\n\n\n0.2604\n0.1175\n0.2206\n4\nJaccard_2\n\n\n0.1259\n0.7932\n0.0819\nH1H2\nJaccard_2\n\n\n0.1929\n0.5210\n0.1594\nH1H3\nJaccard_2\n\n\n0.2310\n0.3052\n0.1903\nH1H4\nJaccard_2\n\n\n0.1280\n0.7869\n0.0820\nH2H3\nJaccard_2\n\n\n0.1300\n0.7802\n0.0841\nH2H4\nJaccard_2\n\n\n0.2112\n0.4193\n0.1717\nH3H4\nJaccard_2\n\n\n0.1247\n0.7974\n0.0807\nH1H2H3\nJaccard_2\n\n\n0.1257\n0.7938\n0.0819\nH1H2H4\nJaccard_2\n\n\n0.1898\n0.5354\n0.1549\nH1H3H4\nJaccard_2\n\n\n0.1247\n0.7974\n0.0807\nH2H3H4\nJaccard_2\n\n\n0.4614\n0.1608\n0.2793\nfull\nlog_R2_1_3\n\n\n0.4793\n0.1116\n0.3031\n1\nlog_R2_1_3\n\n\n0.5045\n0.0221\n0.2990\n2\nlog_R2_1_3\n\n\n0.5499\n0.0021\n0.3673\n3\nlog_R2_1_3\n\n\n0.5051\n0.0046\n0.3209\n4\nlog_R2_1_3\n\n\n0.4647\n0.1507\n0.2798\nH1H2\nlog_R2_1_3\n\n\n0.4542\n0.1887\n0.2964\nH1H3\nlog_R2_1_3\n\n\n0.4574\n0.1814\n0.2972\nH1H4\nlog_R2_1_3\n\n\n0.5031\n0.0339\n0.3006\nH2H3\nlog_R2_1_3\n\n\n0.5015\n0.0291\n0.2976\nH2H4\nlog_R2_1_3\n\n\n0.5040\n0.0124\n0.3207\nH3H4\nlog_R2_1_3\n\n\n0.4568\n0.1822\n0.2768\nH1H2H3\nlog_R2_1_3\n\n\n0.4645\n0.1485\n0.2804\nH1H2H4\nlog_R2_1_3\n\n\n0.4538\n0.1899\n0.2953\nH1H3H4\nlog_R2_1_3\n\n\n0.4568\n0.1822\n0.2768\nH2H3H4\nlog_R2_1_3\n\n\n0.5129\n0.0895\n0.2932\nfull\nlog_R2_1_4\n\n\n0.5206\n0.0535\n0.3033\n1\nlog_R2_1_4\n\n\n0.5373\n0.0350\n0.3153\n2\nlog_R2_1_4\n\n\n0.5661\n0.0127\n0.3546\n3\nlog_R2_1_4\n\n\n0.5387\n0.0011\n0.3314\n4\nlog_R2_1_4\n\n\n0.5132\n0.0884\n0.2916\nH1H2\nlog_R2_1_4\n\n\n0.5196\n0.0733\n0.3027\nH1H3\nlog_R2_1_4\n\n\n0.5198\n0.0742\n0.3053\nH1H4\nlog_R2_1_4\n\n\n0.5390\n0.0350\n0.3180\nH2H3\nlog_R2_1_4\n\n\n0.5363\n0.0346\n0.3146\nH2H4\nlog_R2_1_4\n\n\n0.5336\n0.0148\n0.3273\nH3H4\nlog_R2_1_4\n\n\n0.5131\n0.0873\n0.2919\nH1H2H3\nlog_R2_1_4\n\n\n0.5140\n0.0868\n0.2928\nH1H2H4\nlog_R2_1_4\n\n\n0.5224\n0.0654\n0.3082\nH1H3H4\nlog_R2_1_4\n\n\n0.5131\n0.0873\n0.2919\nH2H3H4\nlog_R2_1_4\n\n\n\n\nplots\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n:::\n\n# save.image(\"data/RData/05_VarPart_loop.RData\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Script 5 - Variation Partitioning</span>"
    ]
  },
  {
    "objectID": "06_Predictions.html",
    "href": "06_Predictions.html",
    "title": "7  Script 6 - Predictions",
    "section": "",
    "text": "rm(list=ls())\nsource(\"src/functions.R\")\n\nLade nötiges Paket: caret\n\n\nLade nötiges Paket: ggplot2\n\n\nLade nötiges Paket: lattice\n\n\nLade nötiges Paket: caretEnsemble\n\n\n\nAttache Paket: 'caretEnsemble'\n\n\nDas folgende Objekt ist maskiert 'package:ggplot2':\n\n    autoplot\n\n\nLade nötiges Paket: plyr\n\n\nLade nötiges Paket: dplyr\n\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLade nötiges Paket: randomForest\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttache Paket: 'randomForest'\n\n\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    combine\n\n\nDas folgende Objekt ist maskiert 'package:ggplot2':\n\n    margin\n\n\nLade nötiges Paket: recipes\n\n\n\nAttache Paket: 'recipes'\n\n\nDas folgende Objekt ist maskiert 'package:stats':\n\n    step\n\n\nLade nötiges Paket: kableExtra\n\n\n\nAttache Paket: 'kableExtra'\n\n\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    group_rows\n\nload(\"data/RData/01_Data_prep.RData\")\nall_models &lt;- readRDS(\"data/models/reducedModels/03_List_all_reduced_models.rds\")[c(\"Jaccard1\", \"LogRatio1\")]\n\n\ninstall_and_load(c(\"dplyr\", \"caret\", \"caretEnsemble\"))\n\n\n## Predict Jaccard\nfile_path &lt;- \"data/AllPredictors.rds\"\ntp &lt;- 2\nvars &lt;- c(H1_vars, H2_vars, H3_vars, H4_vars)\n\ndat &lt;- readRDS(file_path) %&gt;% \n  filter(cell_grouping == 1 & exclude == 0 & tp == tp_value) %&gt;% \n  select(all_of(c( \"verbatim_name\", vars))) %&gt;% \n  mutate(\n    D_AOO_a = case_when(\n      is.na(D_AOO_a) & rel_occ_Ncells &gt; 0.97 ~ 2,\n      TRUE ~ D_AOO_a),\n    mean_prob_cooccur = case_when(\n      is.na(mean_prob_cooccur) & rel_occ_Ncells &lt; 0.05 ~ 0, \n      TRUE ~ mean_prob_cooccur)) %&gt;%\n  filter(!is.na(moran)) %&gt;% \n  mutate_if(\n    is.character, as.factor) %&gt;%\n  mutate_at(\n    vars(c(Habitat.Density, Migration)), as.factor) %&gt;%\n  na.omit()\n\n\ndat$Jacc_pred &lt;- predict(all_models[\"Jaccard1\"]$Jaccard1$ranger, newdata = dat)\n\ndat$LogRatio_pred &lt;- predict(all_models[\"LogRatio1\"]$LogRatio1$ranger, newdata = dat)\n\n\nggplot(data = dat)+\n  geom_histogram(aes(Jacc_pred, fill = dataset))+\n  facet_wrap(~dataset)+\n  theme_classic()+\n    geom_vline(xintercept = c(1,0))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = dat)+\n  geom_histogram(aes(LogRatio_pred, fill = dataset))+\n  facet_wrap(~dataset)+\n  theme_classic()+\n  geom_vline(xintercept = c(0))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Script 6 - Predictions</span>"
    ]
  },
  {
    "objectID": "07_references.html",
    "href": "07_references.html",
    "title": "References",
    "section": "",
    "text": "BirdLife International. 2020. “BirdLife International\nDatazone.” http://datazone.birdlife.org/.\n\n\nCouncil, European Bird Census. 2022. “European Breeding Bird Atlas\n2 Website.” http://ebba2.info.\n\n\nHagemeijer, W. J. M., and M. J. Blair. 1997. The EBCC Atlas of\nEuropean Breeding Birds: Their Distribution and Abundance. London:\nT. & A.D. Poyser.\n\n\nHagemeyer, W., M. Blair, and W. Loos. 2016. “EBCC Atlas of\nEuropean Breeding Birds.” https://doi.org/10.15468/adtfvf.\n\n\nKarger, Dirk N., Olaf Conrad, Jürgen Böhner, Thomas Kawohl, Holger\nKreft, Rodrigo W. Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder,\nand Michael Kessler. 2017a. “Climatologies at High Resolution for\nthe Earth Land Surface Areas.” Scientific Data 4:\n170122. https://doi.org/10.1038/sdata.2017.122.\n\n\n———. 2017b. “Data from: Climatologies at High Resolution for the\nEarth’s Land Surface Areas.” Dryad Digital Repository. https://doi.org/10.5061/dryad.kd1d4.\n\n\nKeller, V., S. Herrando, P. Voříšek, M. Franch, M. Kipson, P. Milanesi,\nD. Martí, et al. 2020. European Breeding Bird Atlas 2: Distribution,\nAbundance and Change. Barcelona: European Bird Census Council &\nLynx Edicions.\n\n\nMinistry of the Environment, Natural Environment Bureau, Biodiversity\nCenter. 2004. Bird Breeding Distribution Survey Report.\nFujiyoshida City: Ministry of the Environment, Natural Environment\nBureau, Biodiversity Center.\n\n\nNew York State Department of Environmental Conservation. 1980-1985.\n“New York State Breeding Bird Atlas.” https://www.dec.ny.gov/animals/7312.html.\n\n\n———. 2000-2005. “New York State Breeding Bird Atlas 2000.”\nhttps://www.dec.ny.gov/animals/7312.html.\n\n\nŠťastný, Karel, Vladimír Bejček, and Karel Hudec. 1997. Atlas\nHnízdního Rozšíření Ptáků v České Republice 1985-1989. Jinočany:\nNakladatelství a vydavatelství H&H.\n\n\n———. 2006. Atlas Hnízdního Rozšíření Ptáků v České Republice:\n2001-2003. Praha: Aventinum.\n\n\nTobias, Joseph A., Catherine Sheard, Alex L. Pigot, Adam J. M. Devenish,\nJingyi Yang, Ferran Sayol, Montague H. C. Neate‐Clegg, et al. 2022.\n“AVONET: Morphological, Ecological and Geographical Data for All\nBirds.” Edited by T. Coulson. Ecology Letters 25 (3):\n581–97. https://doi.org/10.1111/ele.13898.\n\n\nUeda, Mutsuyuki, and Shingo Uemura. 2021. National Bird Breeding\nDistribution Survey Report Let’s Describe the Current Status of Japanese\nBirds 2016-2021. Fuchu City: Bird Breeding Distribution Research\nCommittee.",
    "crumbs": [
      "References"
    ]
  }
]